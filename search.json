[
  {
    "objectID": "release_notes.html",
    "href": "release_notes.html",
    "title": "2.1.0 (2022-01-12)",
    "section": "",
    "text": "Release Notes"
  },
  {
    "objectID": "release_notes.html#section",
    "href": "release_notes.html#section",
    "title": "2.1.0 (2022-01-12)",
    "section": "6.0.0 (2023-07-05)",
    "text": "6.0.0 (2023-07-05)\n\n⚠ BREAKING CHANGES\n\nimports: Use of ibis.udf as a module is removed. Use ibis.legacy.udf instead.\nThe minimum supported Python version is now Python 3.9\napi: group_by().count() no longer automatically names the count aggregation count. Use relabel to rename columns.\nbackends: Backend.ast_schema is removed. Use expr.as_table().schema() instead.\nsnowflake/postgres: Postgres UDFs now use the new @udf.scalar.python API. This should be a low-effort replacement for the existing API.\nir: ops.NullLiteral is removed\ndatatypes: dt.Interval has no longer a default unit, dt.interval is removed\ndeps: snowflake-connector-python’s lower bound was increased to 3.0.2, the minimum version needed to avoid a high-severity vulnerability. Please upgrade snowflake-connector-python to at least version 3.0.2.\napi: Table.difference(), Table.intersection(), and Table.union() now require at least one argument.\npostgres: Ibis no longer automatically defines first/last reductions on connection to the postgres backend. Use DDL shown in https://wiki.postgresql.org/wiki/First/last_(aggregate) or one of the pgxn implementations instead.\napi: ibis.examples.&lt;example-name&gt;.fetch no longer forwards arbitrary keyword arguments to read_csv/read_parquet.\ndatatypes: dt.Interval.value_type attribute is removed\napi: Table.count() is no longer automatically named \"count\". Use Table.count().name(\"count\") to achieve the previous behavior.\ntrino: The trino backend now requires at least version 0.321 of the trino Python package.\nbackends: removed AlchemyTable, AlchemyDatabase, DaskTable, DaskDatabase, PandasTable, PandasDatabase, PySparkDatabaseTable, use ops.DatabaseTable instead\ndtypes: temporal unit enums are now available under ibis.common.temporal instead of ibis.common.enums.\nclickhouse: external_tables can no longer be passed in ibis.clickhouse.connect. Pass external_tables directly in raw_sql/execute/to_pyarrow/to_pyarrow_batches().\ndatatypes: dt.Set is now an alias for dt.Array\nbigquery: Before this change, ibis timestamp is mapping to Bigquery TIMESTAMP type and no timezone supports. However, it’s not correct, BigQuery TIMESTAMP type should have UTC timezone, while DATETIME type is the no timezone version. Hence, this change is breaking the ibis timestamp mapping to BigQuery: If ibis timestamp has the UTC timezone, will map to BigQuery TIMESTAMP type. If ibis timestamp has no timezone, will map to BigQuery DATETIME type.\nimpala: Cursors are no longer returned from DDL operations to prevent resource leakage. Use raw_sql if you need specialized operations that return a cursor. Additionally, table-based DDL operations now return the table they’re operating on.\napi: Column.first()/Column.last() are now reductions by default. Code running these expressions in isolation will no longer be windowed over the entire table. Code using this function in select-based APIs should function unchanged.\nbigquery: when using the bigquery backend, casting float to int will no longer round floats to the nearest integer\nops.Hash: The hash method on table columns on longer accepts the how argument. The hashing functions available are highly backend-dependent and the intention of the hash operation is to provide a fast, consistent (on the same backend, only) integer value. If you have been passing in a value for how, you can remove it and you will get the same results as before, as there were no backends with multiple hash functions working.\nduckdb: Some CSV files may now have headers that did not have them previously. Set header=False to get the previous behavior.\ndeps: New environments will have a different default setting for compression in the ClickHouse backend due to removal of optional dependencies. Ibis is still capable of using the optional dependencies but doesn’t include them by default. Install clickhouse-cityhash and lz4 to preserve the previous behavior.\napi: Table.set_column() is removed; use Table.mutate(name=expr) instead\napi: the suffixes argument in all join methods has been removed in favor of lname/rname args. The default renaming scheme for duplicate columns has also changed. To get the exact same behavior as before, pass in lname=\"{name}_x\", rname=\"{name}_y\".\nir: IntervalType.unit is now an enum instead of a string\ntype-system: Inferred types of Python objects may be slightly different. Ibis now use pyarrow to infer the column types of pandas DataFrame and other types.\nbackends: path argument of Backend.connect() is removed, use the database argument instead\napi: removed Table.sort_by() and Table.groupby(), use .order_by() and .group_by() respectively\ndatatypes: DataType.scalar and column class attributes are now strings.\nbackends: Backend.load_data(), Backend.exists_database() and Backend.exists_table() are removed\nir: Value.summary() and NumericValue.summary() are removed\nschema: Schema.merge() is removed, use the union operator schema1 | schema2 instead\napi: ibis.sequence() is removed\ndrop support for Python 3.8 (747f4ca)\n\n\n\nFeatures\n\nadd dask windowing (9cb920a)\nadd easy type hints to GroupBy (da330b1)\nadd microsecond method to TimestampValue and TimeValue (e9df2da)\napi: add __dataframe__ implementation (b3d9619)\napi: add ALL_CAPS option to Table.relabel (c0b30e2)\napi: add first/last reduction APIs (8c01980)\napi: add zip operation and api (fecf695)\napi: allow passing multiple keyword arguments to ibis.interval (22ee854)\napi: better repr and pickle support for deferred expressions (2b1ec9c)\napi: exact median (c53031c)\napi: raise better error on column name collision in joins (e04c38c)\napi: replace suffixes in join with lname/rname (3caf3a1)\napi: support abstract type names in selectors.of_type (f6d2d56)\napi: support list of strings and single strings in the across selector (a6b60e7)\napi: use create_table to load example data (42e09a4)\nbigquery: add client and storage_client params to connect (4cf1354)\nbigquery: enable group_concat over windows (d6a1117)\ncast: add table-level try_cast (5e4d16b)\nclickhouse: add array zip impl (efba835)\nclickhouse: move to clickhouse supported Python client (012557a)\nclickhouse: set default engine to native file (29815fa)\nclickhouse: support pyarrow decimal types (7472dd5)\ncommon: add a pure python egraph implementation (aed2ed0)\ncommon: add pattern matchers (b515d5c)\ncommon: add support for start parameter in StringFind (31ce741)\ncommon: add Topmost and Innermost pattern matchers (90b48fc)\ncommon: implement copy protocol for Immutable base class (e61c66b)\ncreate_table: support pyarrow Table in table creation (9dbb25c)\ndatafusion: add string functions (66c0afb)\ndatafusion: add support for scalar pyarrow UDFs (45935b7)\ndatafusion: minimal decimal support (c550780)\ndatafusion: register tables and datasets in datafusion (cb2cc58)\ndatatypes: add support for decimal values with arrow-based APIs (b4ba6b9)\ndatatypes: support creating Timestamp from units (66f2ff0)\ndeps: load examples lazily (4ea0ddb)\nduckdb: add attach_sqlite method (bd32649)\nduckdb: add support for native and pyarrow UDFs (7e56fc4)\nduckdb: expand map support to .values() and map concatenation (ad49a09)\nduckdb: set header=True by default (e4b515d)\nduckdb: support 0.8.0 (ae9ae7d)\nduckdb: support array zip operation (2d14ccc)\nduckdb: support motherduck (053dc7e)\nduckdb: warn when querying an already consumed RecordBatchReader (5a013ff)\nflink: add initial flink SQL compiler (053a6d2)\nformats: support timestamps in delta output; default to micros for pyarrow conversion (d8d5710)\nimplement read_delta and to_delta for some backends (74fc863)\nimplement read_delta for datafusion (eb4602f)\nimplement try_cast for a few backends (f488f0e)\nio: add to_torch API (685c8fc)\nio: add az/gs prefixes to normalize_filename in utils (e9eebba)\nmysql: add re_extract (5ed40e1)\noracle: add oracle backend (c9b038b)\noracle: support temporary tables (6e64cd0)\npandas: add approx_median (6714b9f)\npandas: support passing memtables to create_table (3ea9a21)\npolars: add any and all reductions (0bd3c01)\npolars: add argmin and argmax (78562d3)\npolars: add correlation operation (05ff488)\npolars: add polars support for identical_to (aab3bae)\npolars: add support for offset, binary literals, and dropna(how='all') (d2298e9)\npolars: allow seamless connection for DataFrame as well as LazyFrame (a2a3e45)\npolars: implement .sql methods (86f2a34)\npolars: lower-latency column return for non-temporal results (b009563)\npolars: support pyarrow decimal types (7e6c365)\npolars: support SQL dialect translation (c87f695)\npolars: support table registration from multiple parquet files (9c0a8be)\npostgres: add ApproxMedian aggregation (887f572)\npyspark: add zip array impl (6c00cbc)\nsnowflake/postgres: scalar UDFs (dbf5b62)\nsnowflake: implement array zip (839e1f0)\nsnowflake: implement proper approx median (b15a6fe)\nsnowflake: support SSO and other forms of passwordless authentication (23ac53d)\nsnowflake: use the client python version as the UDF runtime where possible (69a9101)\nsql: allow any SQL dialect accepted by sqlgllot in Table.sql and Backend.sql (f38c447)\nsqlite: add argmin and argmax functions (c8af9d4)\nsqlite: add arithmetic mode aggregation (6fcac44)\nsqlite: add ops.DateSub, ops.DateAdd, ops.DateDiff (cfd65a0)\nstreamlit: add support for streamlit connection interface (05c9449)\ntrino: implement zip (cd11daa)\n\n\n\nBug Fixes\n\nadd issue write permission to assign.yml (9445cee)\nalchemy: close the cursor on error during dataframe construction (cc7dffb)\nbackends: fix capitalize to lowercase subsequent characters (49978f9)\nbackends: fix notall/notany translation (56b56b3)\nbigquery: add srid=4326 to the geography dtype mapping (57a825b)\nbigquery: allow passing both schema and obj in create_table (49cc2c4)\nbigquery: bigquery timestamp and datetime dtypes (067e8a5)\nbigquery: ensure that bigquery temporal ops work with the new timeunit/dateunit/intervalunit enums (0e00d86)\nbigquery: ensure that generated names are used when compiling columns and allow flexible column names (c7044fe)\nbigquery: fix table naming from count rename removal refactor (5b009d2)\nbigquery: raise OperationNotDefinedError for IntervalAdd and IntervalSubtract (501aaf7)\nbigquery: support capture group functionality (3f4f05b)\nbigquery: truncate when casting float to int (267d8e1)\nci: use mariadb-admin instead of mysqladmin in mariadb 11.x (d4ccd3d)\nclickhouse: avoid generating names for structs (5d11f48)\nclickhouse: clean up external tables per query to avoid leaking them across queries (6d32edd)\nclickhouse: close cursors more aggressively (478a40f)\nclickhouse: use correct functions for milli and micro extraction (49b3136)\nclickhouse: use named rather than positional group by (1f7e309)\nclickhouse: use the correct dialect to generate subquery string for Contains operation (f656bd5)\ncommon: fix bug in re_extract (6ebaeab), closes #6167\ncore: interval resolution should upcast to smallest unit (f7f844d), closes #6139\ndatafusion: fix incorrect order of predicate -&gt; select compilation (0092304)\ndeps: make pyarrow a required dependency (b217cde)\ndeps: prevent vulnerable snowflake-connector-python versions (6dedb45)\ndeps: support multipledispatch version 1 (805a7d7)\ndeps: update dependency atpublic to v4 (3a44755)\ndeps: update dependency datafusion to v22 (15d8d11)\ndeps: update dependency datafusion to v23 (e4d666d)\ndeps: update dependency datafusion to v24 (c158b78)\ndeps: update dependency datafusion to v25 (c3a6264)\ndeps: update dependency datafusion to v26 (7e84ffe)\ndeps: update dependency deltalake to &gt;=0.9.0,&lt;0.11.0 (9817a83)\ndeps: update dependency pyarrow to v12 (3cbc239)\ndeps: update dependency sqlglot to v12 (5504bd4)\ndeps: update dependency sqlglot to v13 (1485dd0)\ndeps: update dependency sqlglot to v14 (9c40c06)\ndeps: update dependency sqlglot to v15 (f149729)\ndeps: update dependency sqlglot to v16 (46601ef)\ndeps: update dependency sqlglot to v17 (9b50fb4)\ndocs: fix failing doctests (04b9f19)\ndocs: typo in code without selectors (b236893)\ndocs: typo in docstrings and comments (0d3ed86)\ndocs: typo in snowflake do_connect kwargs (671bc31)\nduckdb: better types for null literals (7b9d85e)\nduckdb: disable map values and map merge for columns (b5472b3)\nduckdb: ensure to_timestamp returns a UTC timestamp (0ce0b9f)\nduckdb: ensure connection lifetime is greater than or equal to record batch reader lifetime (6ed353e)\nduckdb: ensure that quoted struct field names work (47de1c3)\nduckdb: ensure that types are inferred correctly across duckdb_engine versions (9c3d173)\nduckdb: fix check for literal maps (b2b229b)\nduckdb: fix exporting pyarrow record batches by bumping duckdb to 0.8.1 (aca52ab)\nduckdb: fix read_csv problem with kwargs (6f71735), closes #6190\nexamples: move lockfile creation to data directory (b8f6e6b)\nexamples: use filelock to prevent pooch from clobbering files when fetching concurrently (e14662e)\nexpr: fix graphviz rendering (6d4a34f)\nimpala: do not cast ca_cert None value to string (bfdfb0e)\nimpala: expose hdfs_connect function as ibis.impala.hdfs_connect (27a0d12)\nimpala: more aggressively clean up cursors internally (bf5687e)\nimpala: replace time_mapping with TIME_MAPPING and backwards compatible check (4c3ca20)\nir: force an alias if projecting or aggregating columns (9fb1e88)\nir: raise Exception for group by with no keys (845f7ab), closes #6237\nmssql: dont yield from inside a cursor (4af0731)\nmysql: do not fail when we cannot set the session timezone (930f8ab)\nmysql: ensure enum string functions are coerced to the correct type (e499c7f)\nmysql: ensure that floats and double do not come back as Python Decimal objects (a3c329f)\nmysql: fix binary literals (e081252)\nmysql: handle the zero timestamp value (9ac86fd)\noperations: ensure that self refs have a distinct name from the table they are referencing (bd8eb88)\noracle: disable autoload when cleaning up temp tables (b824142)\noracle: disable statement cache (41d3857)\noracle: disable temp tables to get inserts working (f9985fe)\npandas, dask: allow overlapping non-predicate columns in asof join (09e26a0)\npandas: fix first and last over windows (9079bc4), closes #5417\npandas: fix string translate function (12b9569), closes #6157\npandas: grouped aggregation using a case statement (d4ac345)\npandas: preserve RHS values in asof join when column names collide (4514668)\npandas: solve problem with first and last window function (dfdede5), closes #4918\npolars: avoid implode deprecation warning (ce3bdad)\npolars: ensure that to_pyarrow is called from the backend (41bacf2)\npolars: make list column operations backwards compatible (35fc5f7)\npostgres: ensure that alias method overwrites view even if types are different (7d5845b)\npostgres: ensure that backend still works when create/drop first/last aggregates fails (eb5d534)\npyspark: enable joining on columns with different names as well as complex predicates (dcee821)\nsnowflake: always use pyarrow for memtables (da34d6f)\nsnowflake: ensure connection lifetime is greater than or equal to record batch reader lifetime (34a0c59)\nsnowflake: ensure that _pandas_converter attribute is resolved correctly (9058bbe)\nsnowflake: ensure that temp tables are only created once (43b8152)\nsnowflake: ensure unnest works for nested struct/object types (fc6ffc2)\nsnowflake: ensure use of the right timezone value (40426bf)\nsnowflake: fix tmpdir construction for python &lt;3.10 (a507ae2)\nsnowflake: fix incorrect arguments to snowflake regexp_substr (9261f70)\nsnowflake: fix invalid attribute access when using pyarrow (bfd90a8)\nsnowflake: handle broken upstream behavior when a table can’t be found (31a8366)\nsnowflake: resolve import error from interval datatype refactor (3092012)\nsnowflake: use convert_timezone for timezone conversion instead of invalid postgres AT TIME ZONE syntax (1595e7b)\nsqlalchemy: ensure that backends don’t clobber tables needed by inputs (76e38a3)\nsqlalchemy: ensure that union_all-generated memtables use the correct column names (a4f546b)\nsqlalchemy: prepend the table’s schema when querying metadata (d8818e2)\nsqlalchemy: quote struct field names (f5c91fc)\ntests: ensure that record batch readers are cleaned up (d230a8d)\ntrino: bump lower bound to avoid having to handle experimental_python_types (bf6eeab)\ntrino: ensure that nested array types are inferred correctly (030f76d)\ntrino: fix incorrect version computation (04d3a89)\ntrino: support trino 0.323 special tuple type for struct results (ea1529d)\ntype-system: infer in-memory object types using pyarrow (f7018ee)\ntypehint: update type hint for class instance (2e1e14f)\n\n\n\nDocumentation\n\nacross: add documentation for across (b8941d3)\nadd allowed input for memtable constructor (69cdee5)\nadd disclaimer on no row order guarantees (75dd8b0)\nadd examples to if_any and if_all (5015677)\nadd platform comment in conda env creation (e38eacb)\nadd read_delta and related to backends docs (90eaed2)\napi: ensure all top-level items have a description (c83d783)\napi: hide dunder methods in API docs (6724b7b)\napi: manually add inherited mixin methods to timey classes (7dbc96d)\napi: show source for classes to allow dunder method inspection (4cef0f8)\nbackends: fix typo in pip install command (6a7207c)\nbigquery: add connection explainer to bigquery backend docs (84caa5b)\nblog: add Ibis + PyTorch + DuckDB blog post (1ad946c)\nchange plural variable name cols to col (c33a3ed), closes #6115\nclarify map refers to Python Mapping container (f050a61)\ncss: enable code block copy button, don’t select prompt (3510abe)\nde-template remaining backends (except pandas, dask, impala) (82b7408)\ndescribe NULL differences with pandas (688b293)\ndev-env: remove python 3.8 from environment support matrix (4f89565)\ndrop docker-compose install for conda dev env setup (e19924d)\nduckdb: add quick explainer on connecting to motherduck (4ef710e)\nfile support: add badge and docstrings for read_* methods (0767b7c)\nfill out more docstrings (dc0289c)\nfix errors and add ‘table’ before ‘expression’ (096b568)\nfix some redirects (3a23c1f)\nfix typo in Table.relabel return description (05cc51e)\ngeneric: add docstring examples in types/generic (1d87292)\nguides: add brief installation instructions at top of notebooks (dc3e694)\nguides: update ibis-for-dplyr-users.ipynb with latest (1aa172e), closes #6125\nimprove docstrings for BooleanValue and BoleanColumn (30c1009)\nimprove docstrings to map types (72a49b0)\ninstall: add quotes to all bracketed installs for shell compatibility (bb5c075)\nintersphinx: add mapping to autolink pyarrow and pandas refs (cd92019)\nintro: create Ibis for dplyr users document (e02a6f2)\nintroguides: use DuckDB for intro pandas notebook, remove iris (a7e845a)\nlink to Ibis for dplyr users (6e7c6a2)\nmake pandas.md filename lowercase (4937d45)\nmore group_by() and NULL in pandas guide (486b696)\nmore spelling fixes (564abbe)\nmove API docs to top-level (dcc409f)\nnumeric: add examples to numeric methods (39b470f)\noracle: add basic backend documentation (c871790)\noracle: add oracle to matrix (89aecf2)\npython-versions: document how we decide to drop support for Python versions (3474dbc)\nredirect Pandas to pandas (4074284)\nremove trailing whitespace (63db643)\nreorder sections in pandas guide (3b66093)\nrestructure and consistency (351d424)\nsnowflake: add connection explainer to snowflake backend docs (a62bbcd)\nstreamlit: fix ibis-framework install (a8cf773)\nupdate copyright and some minor edits (b9aed44)\nupdate notany/notall docstrings with arg (a5ec986), closes #5993\nupdate structs and fix constructor docstrings (493437a)\nuse lowercase pandas (19b5d10)\nuse to_pandas instead of execute (882949e)\n\n\n\nRefactors\n\nalchemy: abstract out custom type mapping and fix sqlite (d712e2e)\napi: consolidate ibis.date(), ibis.time() and ibis.timestamp() functions (20f71bf)\napi: enforce at least one argument for Table set operations (57e948f)\napi: remove automatic count name from relations (2cb19ec)\napi: remove automatic group by count naming (15d9e50)\napi: remove deprecated ibis.sequence() function (de0bf69)\napi: remove deprecated Table.set_column() method (aa5ed94)\napi: remove deprecated Table.sort_by() and Table.groupby() methods (1316635)\nbackends: remove ast_schema method (51b5ef8)\nbackends: remove backend specific DatabaseTable operations (d1bab97)\nbackends: remove deprecated Backend.load_data(), .exists_database() and .exists_table() methods (755555f)\nbackends: remove deprecated path argument of Backend.connect() (6737ea8)\nbigquery: align datatype conversions with the new convention (70b8232)\nbigquery: support a broader range of interval units in temporal binary operations (f78ce73)\ncommon: add sanity checks for creating ENodes and Patterns (fc89cc3)\ncommon: cleanup unit conversions (73de24e)\ncommon: disallow unit conversions between days and hours (5619ce0)\ncommon: move ibis.collections.DisjointSet to ibis.common.egraph (07dde21)\ncommon: move tests for re_extract to general suite (acd1774)\ncommon: use an enum as a sentinel value instead of NoMatch class (6674353), closes #6049\ndask/pandas: align datatype conversions with the new convention (cecc24c)\ndatatypes: make pandas conversion backend specific if needed (544d27c)\ndatatypes: normalize interval values to integers (80a40ab)\ndatatypes: remove Set() in favor of Array() datatype (30a4f7e)\ndatatypes: remove value_type parametrization of the Interval datatype (463cdc3)\ndatatypes: remove direct ir dependency from datatypes (d7f0be0)\ndatatypes: use typehints instead of rules (704542e)\ndeps: remove optional dependency on clickhouse-cityhash and lz4 (736fe26)\ndtypes: add normalize_datetime() and normalize_timezone() common utilities (c00ab38)\ndtypes: turn dt.dtype() into lazily dispatched factory function (5261003)\nformats: consolidate the dataframe conversion logic (53ed88e)\nformats: encapsulate conversions to TypeMapper, SchemaMapper and DataMapper subclasses (ab35311)\nformats: introduce a standalone subpackage to deal with common in-memory formats (e8f45f5)\nimpala: rely on impyla cursor for _wait_synchronous (a1b8736)\nimports: move old UDF implementation to ibis.legacy module (cf93d5d)\nir: encapsulate temporal unit handling in enums (1b8fa7b)\nir: remove rlz.column_from, rlz.base_table_of and rlz.function_of rules (ed71d51)\nir: remove deprecated Value.summary() and NumericValue.summary() expression methods (6cd8050)\nir: remove redundant ops.NullLiteral() operation (a881703)\nir: simplify Expr._find_backends() implementation by using the ibis.common.graph utilities (91ff8d4)\nir: use dt.normalize() to construct literals (bf72f16)\nops.Hash: remove how from backend-specific hash operation (46a55fc)\npandas: solve and remove stale TODOs (92d979e)\npolars: align datatype conversion functions with the new convention (5d61159)\npostgres: fail at execute time for UDFs to avoid db connections in .compile() (e3a4d4d)\npyspark: align datatype conversion functions with the new convention (3437bb6)\npyspark: remove useless window branching in compiler (ad08da4)\nreplace custom _merge using pd.merge (fe74f76)\nschema: remove deprecated Schema.merge() method (d307722)\nschema: use type annotations instead of rules (98cd539)\nsnowflake: add flags to supplemental JavaScript UDFs (054add4)\nsql: align datatype conversions with the new convention (0ef145b)\nsqlite: remove roundtripping for DayOfWeekIndex and DayOfWeekName (b5a2bc5)\ntest: cleanup test data (7ae2b24)\nto-pyarrow-batches: ensure that batch readers are always closed and exhausted (35a391f)\ntrino: always clean up prepared statements created when accessing query metadata (4f3a4cd)\nutil: use base32 to compress uuid table names (ba039a3)\n\n\n\nPerformance\n\nimports: speed up checking for geospatial support (aa601af)\nsnowflake: use pyarrow for all transport (1fb89a1)\nsqlalchemy: lazily construct the inspector object (8db5624)\n\n\n\nDeprecations\n\napi: deprecate tuple syntax for order by keys (5ed5110)"
  },
  {
    "objectID": "release_notes.html#section-1",
    "href": "release_notes.html#section-1",
    "title": "2.1.0 (2022-01-12)",
    "section": "5.1.0 (2023-04-11)",
    "text": "5.1.0 (2023-04-11)\n\nFeatures\n\napi: expand distinct API for dropping duplicates based on column subsets (3720ea5)\napi: implement pyarrow memtables (9d4fbbd)\napi: support passing a format string to Table.relabel (0583959)\napi: thread kwargs around properly to support more complex connection arguments (7e0e15b)\nbackends: add more array functions (5208801)\nbigquery: make to_pyarrow_batches() smarter (42f5987)\nbigquery: support bignumeric type (d7c0f49)\ndefault repr to showing all columns in Jupyter notebooks (91a0811)\ndruid: add re_search support (946202b)\nduckdb: add map operations (a4c4e77)\nduckdb: support sqlalchemy 2 (679bb52)\nmssql: implement ops.StandardDev, ops.Variance (e322f1d)\npandas: support memtable in pandas backend (6e4d621), closes #5467\npolars: implement count distinct (aea4ccd)\npostgres: implement ops.Arbitrary (ee8dbab)\npyspark: pivot_longer (f600c90)\npyspark: add ArrayFilter operation (2b1301e)\npyspark: add ArrayMap operation (e2c159c)\npyspark: add DateDiff operation (bfd6109)\npyspark: add partial support for interval types (067120d)\npyspark: add read_csv, read_parquet, and register (7bd22af)\npyspark: implement count distinct (db29e10)\npyspark: support basic caching (ab0df7a)\nsnowflake: add optional ‘connect_args’ param (8bf2043)\nsnowflake: native pyarrow support (ce3d6a4)\nsqlalchemy: support unknown types (fde79fa)\nsqlite: implement ops.Arbitrary (9bcdf77)\nsql: use temp views where possible (5b9d8c0)\ntable: implement pivot_wider API (60e7731)\nux: move ibis.expr.selectors to ibis.selectors and deprecate for removal in 6.0 (0ae639d)\n\n\n\nBug Fixes\n\napi: disambiguate attribute errors from a missing resolve method (e12c4df)\napi: support filter on literal followed by aggregate (68d65c8)\nclickhouse: do not render aliases when compiling aggregate expression components (46caf3b)\nclickhouse: ensure that clickhouse depends on sqlalchemy for make_url usage (ea10a27)\nclickhouse: ensure that truncate works (1639914)\nclickhouse: fix create_table implementation (5a54489)\nclickhouse: workaround sqlglot issue with calling match (762f4d6)\ndeps: support pandas 2.0 (4f1d9fe)\nduckdb: branch to avoid unnecessary dataframe construction (9d5d943)\nduckdb: disable the progress bar by default (1a1892c)\nduckdb: drop use of experimental parallel csv reader (47d8b92)\nduckdb: generate SIMILAR TO instead of tilde to workaround sqlglot issue (434da27)\nimprove typing signature of .dropna() (e11de3f)\nmssql: improve aggregation on expressions (58aa78d)\nmssql: remove invalid aggregations (1ce3ef9)\npolars: backwards compatibility for the time_zone and time_unit properties (3a2c4df)\npostgres: allow inference of unknown types (343fb37)\npyspark: fail when aggregation contains a having filter (bd81a9f)\npyspark: raise proper error when trying to generate sql (51afc13)\nsnowflake: fix new array operations; remove ArrayRemove operation (772668b)\nsnowflake: make sure ephemeral tables following backend quoting rules (9a845df)\nsnowflake: make sure pyarrow is used when possible (01f5154)\nsql: ensure that set operations resolve to a single relation (3a02965)\nsql: generate consistent pivot_longer semantics in the presence of multiple unnests (6bc301a)\nsqlglot: work with newer versions (6f7302d)\ntrino,duckdb,postgres: make cumulative notany/notall aggregations work (c2e985f)\ntrino: only support how='first' with arbitrary reduction (315b5e7)\nux: use guaranteed length-1 characters for NULL values (8618789)\n\n\n\nRefactors\n\napi: remove explicit use of .projection in favor of the shorter .select (73df8df)\ncache: factor out ref counted cache (c816f00)\nduckdb: simplify to_pyarrow_batches implementation (d6235ee)\nduckdb: source loaded and installed extensions from duckdb (fb06262)\nduckdb: use native duckdb parquet reader unless auth required (e9f57eb)\ngenerate uuid-based names for temp tables (a1164df)\nmemtable: clean up dispatch code (9a19302)\nmemtable: dedup table proxy code (3bccec0)\nsqlalchemy: remove unused _meta instance attributes (523e198)\n\n\n\nDeprecations\n\napi: deprecate Table.set_column in favor of Table.mutate (954a6b7)\n\n\n\nDocumentation\n\nadd a getting started guide (8fd03ce)\nadd warning about comparisons to None (5cf186a)\nblog: add campaign finance blog post (383c708)\nblog: add campaign finance to SUMMARY.md (0bdd093)\nclean up agg argument descriptions and add join examples (93d3059)\ncomparison: add a “why ibis” page (011cc19)\nmove conda before nix in dev setup instructions (6b2cbaa)\nnth: improve docstring for nth() (fb7b34b)\npatch docs build to fix anchor links (51be459)\npenguins: add citation for palmer penguins data (679848d)\npenguins: change to flipper (eec3706)\nrefresh environment setup pages (b609571)\nselectors: make doctests more complete and actually run them (c8f2964)\nstyle and review fixes in getting started guide (3b0f8db)"
  },
  {
    "objectID": "release_notes.html#section-2",
    "href": "release_notes.html#section-2",
    "title": "2.1.0 (2022-01-12)",
    "section": "5.0.0 (2023-03-15)",
    "text": "5.0.0 (2023-03-15)\n\n⚠ BREAKING CHANGES\n\napi: Snowflake identifiers are now kept as is from the database. Many table names and column names may now be in SHOUTING CASE. Adjust code accordingly.\nbackend: Backends now raise ibis.common.exceptions.UnsupportedOperationError in more places during compilation. You may need to catch this error type instead of the previous type, which differed between backends.\nux: Table.info now returns an expression\nux: Passing a sequence of column names to Table.drop is removed. Replace drop(cols) with drop(*cols).\nThe spark plugin alias is removed. Use pyspark instead\nir: removed ibis.expr.scope and ibis.expr.timecontext modules, access them under ibis.backends.base.df.&lt;module&gt;\nsome methods have been removed from the top-level ibis.&lt;backend&gt; namespaces, access them on a connected backend instance instead.\ncommon: removed ibis.common.geospatial, import the functions from ibis.backends.base.sql.registry.geospatial\ndatatypes: JSON is no longer a subtype of String\ndatatype: Category, CategoryValue/Column/Scalar are removed. Use string types instead.\nux: The metric_name argument to value_counts is removed. Use Table.relabel to change the metric column’s name.\ndeps: the minimum version of parsy is now 2.0\nir/backends: removed the following symbols:\nibis.backends.duckdb.parse_type() function\nibis.backends.impala.Backend.set_database() method\nibis.backends.pyspark.Backend.set_database() method\nibis.backends.impala.ImpalaConnection.ping() method\nibis.expr.operations.DatabaseTable.change_name() method\nibis.expr.operations.ParseURL class\nibis.expr.operations.Value.to_projection() method\nibis.expr.types.Table.get_column() method\nibis.expr.types.Table.get_columns() method\nibis.expr.types.StringValue.parse_url() method\nschema: Schema.from_dict(), .delete() and .append() methods are removed\ndatatype: struct_type.pairs is removed, use struct_type.fields instead\ndatatype: Struct(names, types) is not supported anymore, pass a dictionary to Struct constructor instead\n\n\n\nFeatures\n\nadd max_columns option for table repr (a3aa236)\nadd examples API (b62356e)\napi: add map/array accessors for easy conversion of JSON to stronger-typed values (d1e9d11)\napi: add array to string join operation (74de349)\napi: add builtin support for relabeling columns to snake case (1157273)\napi: add support for passing a mapping to ibis.map (d365fd4)\napi: allow single argument set operations (bb0a6f0)\napi: implement to_pandas() API for ecosystem compatibility (cad316c)\napi: implement isin (ac31db2)\napi: make cache evaluate only once per session per expression (5a8ffe9)\napi: make create_table uniform (833c698)\napi: more selectors (5844304)\napi: upcast pandas DataFrames to memtables in rlz.table rule (8dcfb8d)\nbackends: implement ops.Time for sqlalchemy backends (713cd33)\nbigquery: add BIGNUMERIC type support (5c98ea4)\nbigquery: add UUID literal support (ac47c62)\nbigquery: enable subqueries in select statements (ef4dc86)\nbigquery: implement create and drop table method (5f3c22c)\nbigquery: implement create_view and drop_view method (a586473)\nbigquery: support creating tables from in-memory tables (c3a25f1)\nbigquery: support in-memory tables (37e3279)\nchange Rich repr of dtypes from blue to dim (008311f)\nclickhouse: implement ArrayFilter translation (f2144b6)\nclickhouse: implement ops.ArrayMap (45000e7)\nclickhouse: implement ops.MapLength (fc82eaa)\nclickhouse: implement ops.Capitalize (914c64c)\nclickhouse: implement ops.ExtractMillisecond (ee74e3a)\nclickhouse: implement ops.RandomScalar (104aeed)\nclickhouse: implement ops.StringAscii (a507d17)\nclickhouse: implement ops.TimestampFromYMDHMS, ops.DateFromYMD (05f5ae5)\nclickhouse: improve error message for invalid types in literal (e4d7799)\nclickhouse: support asof_join (7ed5143)\ncommon: add abstract mapping collection with support for set operations (7d4aa0f)\ncommon: add support for variadic positional and variadic keyword annotations (baea1fa)\ncommon: hold typehint in the annotation objects (b3601c6)\ncommon: support Callable arguments and return types in Validator.from_annotable() (ae57c36)\ncommon: support positional only and keyword only arguments in annotations (340dca1)\ndask/pandas: raise OperationNotDefinedError exc for not defined operations (2833685)\ndatafusion: implement ops.Degrees, ops.Radians (7e61391)\ndatafusion: implement ops.Exp (7cb3ade)\ndatafusion: implement ops.Pi, ops.E (5a74cb4)\ndatafusion: implement ops.RandomScalar (5d1cd0f)\ndatafusion: implement ops.StartsWith (8099014)\ndatafusion: implement ops.StringAscii (b1d7672)\ndatafusion: implement ops.StrRight (016a082)\ndatafusion: implement ops.Translate (2fe3fc4)\ndatafusion: support substr without end (a19fd87)\ndatatype/schema: support datatype and schema declaration using type annotated classes (6722c31)\ndatatype: enable inference of Decimal type (8761732)\ndatatype: implement Mapping abstract base class for StructType (5df2022)\ndeps: add Python 3.11 support and tests (6f3f759)\ndruid: add Apache Druid backend (c4cc2a6)\ndruid: implement bitwise operations (3ac7447)\ndruid: implement ops.Pi, ops.Modulus, ops.Power, ops.Log10 (090ff03)\ndruid: implement ops.Sign (35f52cc)\ndruid: implement ops.StringJoin (42cd9a3)\nduckdb: add support for reading tables from sqlite databases (9ba2211)\nduckdb: add UUID type support (5cd6d76)\nduckdb: implement ArrayFilter translation (5f35d5c)\nduckdb: implement ops.ArrayMap (063602d)\nduckdb: implement create_view and drop_view method (4f73953)\nduckdb: implement ops.Capitalize (b17116e)\nduckdb: implement ops.TimestampDiff, ops.IntervalAdd, ops.IntervalSubtract (a7fd8fb)\nduckdb: implement uuid result type (3150333)\nduckdb: support dt.MACADDR, dt.INET as string (c4739c7)\nduckdb: use read_json_auto when reading json (4193867)\nexamples: add imdb dataset examples (3d63203)\nexamples: add movielens small dataset (5f7c15c)\nexamples: add wowah_data data to examples (bf9a7cc)\nexamples: enable progressbar and faster hashing (4adfe29)\nimpala: implement ops.Clip (279fd78)\nimpala: implement ops.Radians, ops.Degrees (a794ace)\nimpala: implement ops.RandomScalar (874f2ff)\nio: add to_parquet, to_csv to backends (fecca42)\nir: add ArrayFilter operation (e719d60)\nir: add ArrayMap operation (49e5f7a)\nmysql: support in-memory tables (4dfabbd)\npandas/dask: implement bitwise operations (4994add)\npandas/dask: implement ops.Pi, ops.E (091be3c)\npandas: add basic unnest support (dd36b9d)\npandas: implement ops.StartsWith, ops.EndsWith (2725423)\npandas: support more pandas extension dtypes (54818ef)\npolars: implement ops.Union (17c6011)\npolars: implement ops.Pi, ops.E (6d8fc4a)\npostgres: allow connecting with an explicit schema (39c9ea8)\npostgres: fix interval literal (c0fa933)\npostgres: implement argmin/argmax (82668ec)\npostgres: parse tsvector columns as strings (fac8c47), closes #5402\npyspark: add support for ops.ArgMin and ops.ArgMax (a3fa57c)\npyspark: implement ops.Between (ed83465)\nreturn Table from create_table(), create_view() (e4ea597)\nschema: implement Mapping abstract base class for Schema (167d85a)\nselectors: support ranges (e10caf4)\nsnowflake: add support for alias in snowflake (b1b947a)\nsnowflake: add support for bulk upload for temp tables in snowflake (6cc174f)\nsnowflake: add UUID literal support (436c781)\nsnowflake: implement argmin/argmax (8b998a5)\nsnowflake: implement ops.BitwiseAnd, ops.BitwiseNot, ops.BitwiseOr, ops.BitwiseXor (1acd4b7)\nsnowflake: implement ops.GroupConcat (2219866)\nsnowflake: implement remaining map functions (c48c9a6)\nsnowflake: support binary variance reduction with filters (eeabdee)\nsnowflake: support cross-database table access (79cb445)\nsqlalchemy: generalize unnest to work on backends that don’t support it (5943ce7)\nsqlite: add sqlite type support (addd6a9)\nsqlite: support in-memory tables (1b24848)\nsql: support for creating temporary tables in sql based backends (466cf35)\ntables: cast table using schema (96ce109)\ntables: implement pivot_longer API (11c5736)\ntrino: enable MapLength operation (a7ad1db)\ntrino: implement ArrayFilter translation (50f6fcc)\ntrino: implement ops.ArrayMap (657bf61)\ntrino: implement ops.Between (d70b9c0)\ntrino: support sqlalchemy 2 (0d078c1)\nux: accept selectors in Table.drop (325140f)\nux: allow creating unbound tables using annotated class definitions (d7bf6a2)\nux: easy interactive setup (6850146)\nux: expose between, rows and range keyword arguments in value.over() (5763063)\n\n\n\nBug Fixes\n\nanalysis: extract Limit subqueries (62f6e14)\napi: add a name attribute to backend proxy modules (d6d8e7e)\napi: fix broken __radd__ array concat operation (121d9a0)\napi: only include valid python identifiers in struct tab completion (8f33775)\napi: only include valid python identifiers in table tab completion (031a48c)\nbackend: provide useful error if default backend is unavailable (1dbc682)\nbackends: fix capitalize implementations across all backends (d4f0275)\nbackends: fix null literal handling (7f46342)\nbigquery: ensure that memtables are translated correctly (d6e56c5)\nbigquery: fix decimal literals (4a04c9b)\nbigquery: regenerate negative string index sql snapshots (3f02c73)\nbigquery: regenerate sql for predicate pushdown fix (509806f)\ncache: remove bogus schema argument and validate database argument type (c4254f6)\nci: fix invalid test id (f70de1d)\nclickhouse: fix decimal literal (4dcd2cb)\nclickhouse: fix set ops with table operands (86bcf32)\nclickhouse: raise OperationNotDefinedError if operation is not supported (71e2570)\nclickhouse: register in-memory tables in pyarrow-related calls (09a045c)\nclickhouse: use a bool type supported by clickhouse_driver (ab8f064)\nclickhouse: workaround sqlglot’s insistence on uppercasing (6151f37)\ncompiler: generate aliases in a less clever way (04a4aa5)\ndatafusion: support sum aggregation on bool column (9421400)\ndeps: bump duckdb to 0.7.0 (38d2276)\ndeps: bump snowflake-connector-python upper bound (b368b04)\ndeps: ensure that pyspark depends on sqlalchemy (60c7382)\ndeps: update dependency pyarrow to v11 (2af5d8d)\ndeps: update dependency sqlglot to v11 (e581e2f)\ndon’t expose backend methods on ibis.&lt;backend&gt; directly (5a16431)\ndruid: remove invalid operations (19f214c)\nduckdb: add null to duckdb datatype parser (07d2a86)\nduckdb: ensure that temp_directory exists (00ba6cb)\nduckdb: explicitly set timezone to UTC on connection (6ae4a06)\nduckdb: fix blob type in literal (f66e8a1)\nduckdb: fix memtable to_pyarrow/to_pyarrow_batches (0e8b066)\nduckdb: in-memory objects registered with duckdb show up in list_tables (7772f79)\nduckdb: quote identifiers if necessary in struct_pack (6e598cc)\nduckdb: support casting to unsigned integer types (066c158)\nduckdb: treat g re_replace flag as literal text (aa3c31c)\nduckdb: workaround an ownership bug at the interaction of duckdb, pandas and pyarrow (2819cff)\nduckdb: workaround duckdb bug that prevents multiple substitutions (0e09220)\nimports: remove top-level import of sqlalchemy from base backend (b13cf25)\nio: add read_parquet and read_csv to base backend mixin (ce80d36), closes #5420\nir: incorrect predicate pushdown (9a9204f)\nir: make find_subqueries return in topological order (3587910)\nir: properly raise error if literal cannot be coerced to a datatype (e16b91f)\nir: reorder the right schema of set operations to align with the left schema (58e60ae)\nir: use rlz.map_to() rule instead of isin to normalize temporal units (a1c46a2)\nir: use static connection pooling to prevent dropping temporary state (6d2ae26)\nmssql: set sqlglot to tsql (1044573)\nmysql: remove invalid operations (8f34a2b)\npandas/dask: handle non numpy scalar results in wrap_case_result (a3b82f7)\npandas: don’t try to dispatch on arrow dtype if not available (d22ae7b)\npandas: handle casting to arrays with None elements (382b90f)\npandas: handle NAs in array conversion (06bd15d)\npolars: back compat for concat_str separator argument (ced5a61)\npolars: back compat for the reverse/descending argument (f067d81)\npolars: polars execute respect limit kwargs (d962faf)\npolars: properly infer polars categorical dtype (5a4707a)\npolars: use metric name in aggregate output to dedupe columns (234d8c1)\npyspark: fix incorrect ops.EndsWith translation rule (4c0a5a2)\npyspark: fix isnan and isinf to work on bool (8dc623a)\nsnowflake: allow loose casting of objects and arrays (1cf8df0)\nsnowflake: ensure that memtables are translated correctly (b361e07)\nsnowflake: ensure that null comparisons are correct (9b83699)\nsnowflake: ensure that quoting matches snowflake behavior, not sqlalchemy (b6b67f9)\nsnowflake: ensure that we do not try to use a None schema or database (03e0265)\nsnowflake: handle the case where pyarrow isn’t installed (b624fa3)\nsnowflake: make array_agg preserve nulls (24b95bf)\nsnowflake: quote column names on construction of sa.Column (af4db5c)\nsnowflake: remove broken pyarrow fetch support (c440adb)\nsnowflake: return NULL when trying to call map functions on non-object JSON (d85fb28)\nsnowflake: use _flatten to avoid overriding unrelated function in other backends (8c31594)\nsqlalchemy: ensure that isin contains full column expression (9018eb6)\nsqlalchemy: get builtin dialects working; mysql/mssql/postgres/sqlite (d2356bc)\nsqlalchemy: make strip family of functions behave like Python (dd0a04c)\nsqlalchemy: reflect most recent schema when view is replaced (62c8dea)\nsqlalchemy: use sa.true instead of Python literal (8423eba)\nsqlalchemy: use indexed group by key references everywhere possible (9f1ddd8)\nsql: ensure that set operations generate valid sql in the presence of additional constructs such as sort keys (3e2c364)\nsqlite: explicitly disallow array in literal (de73b37)\nsqlite: fix random scalar range (26d0dde)\nsupport negative string indices (f84a54d)\ntrino: workaround broken dialect (b502faf)\ntypes: fix argument types of Table.order_by() (6ed3a97)\nutil: make convert_unit work with python types (cb3a90c)\nux: give the value_counts aggregate column a better name (abab1d7)\nux: make string range selectors inclusive (7071669)\nux: make top level set operations work (f5976b2)\n\n\n\nPerformance\n\nduckdb: faster to_parquet/to_csv implementations (6071bb5)\nfix duckdb insert-from-dataframe performance (cd27b99)\ndeps: bump minimum required version of parsy (22020cb)\nremove spark alias to pyspark and associated cruft (4b286bd)\n\n\n\nRefactors\n\nanalysis: slightly simplify find_subqueries() (ab3712f)\nbackend: normalize exceptions (065b66d)\nclickhouse: clean up parsing rules (6731772)\ncommon: move frozendict and DotDict to ibis.common.collections (4451375)\ncommon: move the geospatial module to the base SQL backend (3e7bfa3)\ndask: remove unneeded create_table() (86885a6)\ndatatype: clean up parsing rules (c15fb5f)\ndatatype: remove Category type and related APIs (bb0ee78)\ndatatype: remove StructType.pairs property in favor of identical fields attribute (6668122)\ndatatypes: move sqlalchemy datatypes to specific backend (d7b49eb)\ndatatypes: remove String parent type from JSON type (34f3898)\ndatatype: use a dictionary to store StructType fields rather than names and types tuples (84455ac)\ndatatype: use lazy dispatch when inferring pandas Timedelta objects (e5280ea)\ndrop limit kwarg from to_parquet/to_csv (a54460c)\nduckdb: clean up parsing rules (30da8f9)\nduckdb: handle parsing timestamp scale (16c1443)\nduckdb: remove unused list&lt;...&gt; parsing rule (f040b86)\nduckdb: use a proper sqlalchemy construct for structs and reduce casting (8daa4a1)\nir/api: introduce window frame operation and revamp the window API (2bc5e5e)\nir/backends: remove various deprecated functions and methods (a8d3007)\nir: reorganize the scope and timecontext utilities (80bd494)\nir: update ArrayMap to use the new callable_with validation rule (560474e)\nmove pretty repr tests back to their own file (4a75988)\nnix: clean up marker argument construction (12eb916)\npostgres: clean up datatype parsing (1f61661)\npostgres: clean up literal arrays (21b122d)\npyspark: remove another private function (c5081cf)\nremove unnecessary top-level rich console (8083a6b)\nrules: remove unused non_negative_integer and pair rules (e00920a)\nschema: remove deprecated Schema.from_dict(), .delete() and .append() methods (8912b24)\nsnowflake: remove the need for parsy (c53403a)\nsqlalchemy: set session parameters once per connection (ed4b476)\nsqlalchemy: use backend-specific startswith/endswith implementations (6101de2)\ntest_sqlalchemy.py: move to snapshot testing (96998f0)\ntests: reorganize rules test file to the ibis.expr subpackage (47f0909)\ntests: reorganize schema test file to the ibis.expr subpackage (40033e1)\ntests: reorganize datatype test files to the datatypes subpackage (16199c6)\ntrino: clean up datatype parsing (84c0e35)\nux: return expression from Table.info (71cc0e0)\n\n\n\nDeprecations\n\napi: deprecate summary API (e449c07)\napi: mark ibis.sequence() for removal (3589f80)\n\n\n\nDocumentation\n\nadd a bunch of string expression examples (18d3112)\nadd Apache Druid to backend matrix (764d9c3)\nadd CNAME file to mkdocs source (6d19111)\nadd druid to the backends index docs page (ad0b6a3)\nadd missing DataFusion entry to the backends in the README (8ce025a)\nadd redirects for common old pages (c9087f2)\napi: document deferred API and its pitfalls (8493604)\napi: improve collect method API documentation (b4fcef1)\narray expression examples (6812c17)\nbackends: document default backend configuration (6d917d3)\nbackends: link to configuration from the backends list (144044d)\nblob: blog on ibis + substrait + duckdb (5dc7a0a)\nblog: adds examples sneak peek blog + assets folder (fcbb3d5)\nblog: adds to file sneak peek blog (128194f)\nblog: specify parsy 2.0 in substrait blog article (c264477)\nbump query engine count in README and use project-preferred names (11169f7)\ndon’t sort backends by coverage percentage by default (68f73b1)\ndrop docs versioning (d7140e7)\nduckdb: fix broken docstring examples (51084ad)\nenable light/dark mode toggle in docs (b9e812a)\nfill out table API with working examples (16fc8be)\nfix notebook logging example (04b75ef)\nhow-to: fix sessionize.md to use ibis.read_parquet (ff9cbf7)\nimprove Expr.substitute() docstring (b954edd)\nimprove/update pandas walkthrough (80b05d8)\nio: doc/ux improvements for read_parquet and friends (2541556), closes #5420\nio: update README.md to recommend installing duckdb as default backend (0a72ec0), closes #5423 #5420\nmove tutorial from docs to external ibis-examples repo (11b0237)\nparquet: add docstring examples for to_parquet incl. partitioning (8040164)\npoint to ibis-examples repo in the README (1205636)\nREADME.md: clean up readme, fix typos, alter the example (383a3d3)\nremove duplicate “or” (b6ef3cc)\nremove duplicate spark backend in install docs (5954618)\nrender __dunder__ method API documentation (b532c63)\nrerender ci-analysis notebook with new table header colors (50507b6)\nstreamlit: fix url for support matrix (594199b)\ntutorial: remove impala from sql tutorial (7627c13)\nuse teal for primary & accent colors (24be961)"
  },
  {
    "objectID": "release_notes.html#section-3",
    "href": "release_notes.html#section-3",
    "title": "2.1.0 (2022-01-12)",
    "section": "4.1.0 (2023-01-25)",
    "text": "4.1.0 (2023-01-25)\n\nFeatures\n\nadd ibis.get_backend function (2d27df8)\nadd py.typed to allow mypy to type check packages that use ibis (765d42e)\napi: add ibis.set_backend function (e7fabaf)\napi: add selectors for easier selection of columns (306bc88)\nbigquery: add JS UDF support (e74328b)\nbigquery: add SQL UDF support (db24173)\nbigquery: add to_pyarrow method (30157c5)\nbigquery: implement bitwise operations (55b69b1)\nbigquery: implement ops.Typeof (b219919)\nbigquery: implement ops.ZeroIfNull (f4c5607)\nbigquery: implement struct literal (c5f2a1d)\nclickhouse: properly support native boolean types (31cc7ba)\ncommon: add support for annotating with coercible types (ae4a415)\ncommon: make frozendict truly immutable (1c25213)\ncommon: support annotations with typing.Literal (6f89f0b)\ncommon: support generic mapping and sequence type annotations (ddc6603)\ndask: support connect() with no arguments (67eed42)\ndatatype: add optional timestamp scale parameter (a38115a)\ndatatypes: add as_struct method to convert schemas to structs (64be7b1)\nduckdb: add read_json function for consuming newline-delimited JSON files (65e65c1)\nmssql: add a bunch of missing types (c698d35)\nmssql: implement inference for DATETIME2 and DATETIMEOFFSET (aa9f151)\nnicer repr for Backend.tables (0d319ca)\npandas: support connect() with no arguments (78cbbdd)\npolars: allow ibis.polars.connect() to function without any arguments (d653a07)\npolars: handle casting to scaled timestamps (099d1ec)\npostgres: add Map(string, string) support via the built-in HSTORE extension (f968f8f)\npyarrow: support conversion to pyarrow map and struct types (54a4557)\nsnowflake: add more array operations (8d8bb70)\nsnowflake: add more map operations (7ae6e25)\nsnowflake: any/all/notany/notall reductions (ba1af5e)\nsnowflake: bitwise reductions (5aba997)\nsnowflake: date from ymd (035f856)\nsnowflake: fix array slicing (bd7af2a)\nsnowflake: implement ArrayCollect (c425f68)\nsnowflake: implement NthValue (0dca57c)\nsnowflake: implement ops.Arbitrary (45f4f05)\nsnowflake: implement ops.StructColumn (41698ed)\nsnowflake: implement StringSplit (e6acc09)\nsnowflake: implement StructField and struct literals (286a5c3)\nsnowflake: implement TimestampFromUNIX (314637d)\nsnowflake: implement TimestampFromYMDHMS (1eba8be)\nsnowflake: implement typeof operation (029499c)\nsnowflake: implement exists/not exists (7c8363b)\nsnowflake: implement extract millisecond (3292e91)\nsnowflake: make literal maps and params work (dd759d3)\nsnowflake: regex extract, search and replace (9c82179)\nsnowflake: string to timestamp (095ded6)\nsqlite: implement _get_schema_using_query in SQLite backend (7ff84c8)\ntrino: compile timestamp types with scale (67683d3)\ntrino: enable ops.ExistsSubquery and ops.NotExistsSubquery (9b9b315)\ntrino: map parameters (53bd910)\nux: improve error message when column is not found (b527506)\n\n\n\nBug Fixes\n\nbackend: read the default backend setting in _default_backend (11252af)\nbigquery: move connection logic to do_connect (42f2106)\nbigquery: remove invalid operations from registry (911a080)\nbigquery: resolve deprecation warnings for StructType and Schema (c9e7078)\nclickhouse: fix position call (702de5d)\ncorrectly visualize array type (26b0b3f)\ndeps: make sure pyarrow is not an implicit dependency (10373f4)\nduckdb: make read_csv on URLs work (9e61816)\nduckdb: only try to load extensions when necessary for csv (c77bde7)\nduckdb: remove invalid operations from registry (ba2ec59)\nfallback to default backend with to_pyarrow/to_pyarrow_batches (a1a6902)\nimpala: remove broken alias elision (32b120f)\nir: error for order_by on nonexistent column (57b1dd8)\nir: ops.Where output shape should consider all arguments (6f87064)\nmssql: infer bit as boolean everywhere (24f9d7c)\nmssql: pull nullability from column information (490f8b4)\nmysql: fix mysql query schema inference (12f6438)\npolars: remove non-working Binary and Decimal literal inference (0482d15)\npostgres: use permanent views to avoid connection pool defeat (49a4991)\npyspark: fix substring constant translation (40d2072)\nset ops: raise if no tables passed to set operations (bf4bdde)\nsnowflake: bring back bitwise operations (260facd)\nsnowflake: don’t always insert a cast (ee8817b)\nsnowflake: implement working TimestampNow (42d95b0)\nsnowflake: make sqlalchemy 2.0 compatible (8071255)\nsnowflake: re-enable ops.TableArrayView (a1ad2b7)\nsnowflake: remove invalid operations from registry (2831559)\nsql: add typeof test and bring back implementations (7dc5356)\nsqlalchemy: 2.0 compatibility (837a736)\nsqlalchemy: fix view creation with select stmts that have bind parameters (d760e69)\nsqlalchemy: handle correlated exists sanely (efa42bd)\nsqlalchemy: handle generic geography/geometry by name instead of geotype (23c35e1)\nsqlalchemy: use exec_driver_sql in view teardown (2599c9b)\nsqlalchemy: use the backend’s compiler instead of AlchemyCompiler (9f4ff54)\nsql: fix broken call to ibis.map (045edc7)\nsqlite: interpolate pathlib.Path correctly in attach (0415bd3)\ntrino: ensure connecting works with trino 0.321 (07cee38)\ntrino: remove invalid operations from registry (665265c)\nux: remove extra trailing newline in expression repr (ee6d58a)\n\n\n\nDocumentation\n\nadd BigQuery backend docs (09d8995)\nadd streamlit app for showing the backend operation matrix (3228f64)\nallow deselecting geospatial ops in backend support matrix (012da8c)\napi: document more public expression APIs (337018f)\nbackend-info: prevent app from trying install duckdb extensions (3d94082)\nclean up gen_matrix.py after adding streamlit app (deb80f2)\nduckdb: add to_pyarrow_batches documentation (ec1ffce)\nembed streamlit operation matrix app to docs (469a50d)\nmake firefox render the proper iframe height (ff1d4dc)\npublish raw data for operation matrix (62e68da)\nre-order when to download test data (8ce8c16)\nrelease: update breaking changes in the release notes for 4.0.0 (4e91401)\nremove trailing parenthesis (4294397)\nupdate ibis-version-4.0.0-release.md (f6701df)\nupdate links to contributing guides (da615e4)\n\n\n\nRefactors\n\nbigquery: explicitly disallow INT64 in JS UDF (fb33bf9)\ndatatype: add custom sqlalchemy nested types for backend differentiation (dec70f5)\ndatatype: introduce to_sqla_type dispatching on dialect (a8bbc00)\ndatatypes: remove Geography and Geometry types in favor of GeoSpatial (d44978c)\ndatatype: use a mapping to store StructType fields rather than names and types tuples (ff34c7b)\ndtypes: expose nbytes property for integer and floating point datatypes (ccf80fd)\nduckdb: remove .raw_sql call (abc939e)\nduckdb: use sqlalchemy-views to reduce string hacking (c162750)\nir: remove UnnamedMarker (dd352b1)\npostgres: use a bindparam for metadata queries (b6b4669)\nremove empty unused file (9d63fd6)\nschema: use a mapping to store Schema fields rather than names and types tuples (318179a)\nsimplify _find_backend implementation (60f1a1b)\nsnowflake: remove unnecessary parse_json call in ops.StructField impl (9e80231)\nsnowflake: remove unnecessary casting (271554c)\nsnowflake: use unary instead of fixed_arity(..., 1) (4a1c7c9)\nsqlalchemy: clean up quoting implementation (506ce01)\nsqlalchemy: generalize handling of failed type inference (b0f4e4c)\nsqlalchemy: move _get_schema_using_query to base class (296cd7d)\nsqlalchemy: remove the need for deferred columns (e4011aa)\nsqlalchemy: remove use of deprecated isnot (4ec53a4)\nsqlalchemy: use exec_driver_sql everywhere (e8f96b6)\nsql: finally remove _CorrelatedRefCheck (f49e429)\n\n\n\nDeprecations\n\napi: deprecate .to_projection in favor of .as_table (7706a86)\napi: deprecate get_column/s in favor of __getitem__/__getattr__ syntax (e6372e2)\nir: schedule DatabaseTable.change_name for removal (e4bae26)\nschema: schedule Schema.delete() and Schema.append() for removal (45ac9a9)"
  },
  {
    "objectID": "release_notes.html#section-4",
    "href": "release_notes.html#section-4",
    "title": "2.1.0 (2022-01-12)",
    "section": "4.0.0 (2023-01-09)",
    "text": "4.0.0 (2023-01-09)\n\n⚠ BREAKING CHANGES\n\nfunctions, methods and classes marked as deprecated are removed now\nir: replace HLLCardinality with ApproxCountDistinct and CMSMedian with ApproxMedian operations.\nbackends: the datatype of returned execution results now more closely matches that of the ibis expression’s type. Downstream code may need to be adjusted.\nir: the JSONB type is replaced by the JSON type.\ndev-deps: expression types have been removed from ibis.expr.api. Use import ibis.expr.types as ir to access these types.\ncommon: removed @immutable_property decorator, use @attribute.default instead\ntimestamps: the timezone argument to to_timestamp is gone. This was only supported in the BigQuery backend. Append %Z to the format string and the desired time zone to the input column if necessary.\ndeps: ibis now supports at minimum duckdb 0.3.3. Please upgrade your duckdb install as needed.\napi: previously ibis.connect would return a Table object when calling connect on a parquet/csv file. This now returns a backend containing a single table created from that file. When possible users may use ibis.read instead to read files into ibis tables.\napi: histogram()’s closed argument no longer exists because it never had any effect. Remove it from your histogram method calls.\npandas/dask: the pandas and Dask backends now interpret casting ints to/from timestamps as seconds since the unix epoch, matching other backends.\ndatafusion: register_csv and register_parquet are removed. Pass filename to register method instead.\nir: ops.NodeList and ir.List are removed. Use tuples to represent sequence of expressions instead.\napi: re_extract now follows re.match behavior. In particular, the 0th group is now the entire string if there’s a match, otherwise the groups are 1-based.\ndatatypes: enums are now strings. Likely no action needed since no functionality existed.\nir: Replace t[t.x.topk(...)] with t.semi_join(t.x.topk(...), \"x\").\nir: ir.Analytic.type() and ir.TopK.type() methods are removed.\napi: the default limit for table/column expressions is now None (meaning no limit).\nir: join changes: previously all column names that collided between left and right tables were renamed with an appended suffix. Now for the case of inner joins with only equality predicates, colliding columns that are known to be equal due to the join predicates aren’t renamed.\nimpala: kerberos support is no longer installed by default for the impala backend. To add support you’ll need to install the kerberos package separately.\nir: ops.DeferredSortKey is removed. Use ops.SortKey directly instead.\nir: ibis.common.grounds.Annotable is mutable by default now\nir: node.has_resolved_name() is removed, use isinstance(node, ops.Named) instead; node.resolve_name() is removed use node.name instead\nir: removed ops.Node.flat_args(), directly use node.args property instead\nir: removed ops.Node.inputs property, use the multipledispatched get_node_arguments() function in the pandas backend\nir: Node.blocks() method has been removed.\nir: HasSchema mixin class is no longer available, directly subclass ops.TableNode and implement schema property instead\nir: Removed Node.output_type property in favor of abstractmethod Node.to_expr() which now must be explicitly implemented\nir: Expr(Op(Expr(Op(Expr(Op))))) is now represented as Expr(Op(Op(Op))), so code using ibis internals must be migrated\npandas: Use timezone conversion functions to compute the original machine localized value\ncommon: use ibis.common.validators.{Parameter, Signature} instead\nir: ibis.expr.lineage.lineage() is now removed\nir: removed ir.DestructValue, ir.DestructScalar and ir.DestructColumn, use table.unpack() instead\nir: removed Node.root_tables() method, use ibis.expr.analysis.find_immediate_parent_tables() instead\nimpala: use other methods for pinging the database\n\n\n\nFeatures\n\nadd experimental decorator (791335f)\nadd to_pyarrow and to_pyarrow_batches (a059cf9)\nadd unbind method to expressions (4b91b0b), closes #4536\nadd way to specify sqlglot dialect on backend (f1c0608)\nalchemy: implement json getitem for sqlalchemy backends (7384087)\napi: add agg alias for aggregate (907583f)\napi: add agg alias to group_by (6b6367c)\napi: add ibis.read top level API function (e67132c)\napi: add JSON __getitem__ operation (3e2efb4)\napi: implement __array__ (1402347)\napi: make drop variadic (1d69702)\napi: return object from to_sql to support notebook syntax highlighting (87c9833)\napi: use rich for interactive __repr__ (04758b8)\nbackend: make ArrayCollect filterable (1e1a5cf)\nbackends/mssql: add backend support for Microsoft Sql Server (fc39323)\nbigquery: add ops.DateFromYMD, ops.TimeFromHMS, ops.TimestampFromYMDHMS (a4a7936)\nbigquery: add ops.ExtractDayOfYear (30c547a)\nbigquery: add support for correlation (4df9f8b)\nbigquery: implement argmin and argmax (40c5f0d)\nbigquery: implement pi and e (b91370a)\nbigquery: implement array repeat (09d1e2f)\nbigquery: implement JSON getitem functionality (9c0e775)\nbigquery: implement ops.ArraySlice (49414ef)\nbigquery: implement ops.Capitalize (5757bb0)\nbigquery: implement ops.Clip (5495d6d)\nbigquery: implement ops.Degrees, ops.Radians (5119b93)\nbigquery: implement ops.ExtractWeekOfYear (477d287)\nbigquery: implement ops.RandomScalar (5dc8482)\nbigquery: implement ops.StructColumn, ops.ArrayColumn (2bbf73c)\nbigquery: implement ops.Translate (77a4b3e)\nbigquery: implementt ops.NthValue (b43ba28)\nbigquery: move bigquery backend back into the main repo (cd5e881)\nclickhouse: handle more options in parse_url implementation (874c5c0)\nclickhouse: implement INTERSECT ALL/EXCEPT ALL (f65fbc3)\nclickhouse: implement quantile/multiquantile (96d7d1b)\ncommon: support function annotations with both typehints and rules (7e23f3e)\ndask: implement mode aggregation (017f07a)\ndask: implement json getitem (381d805)\ndatafusion: convert column expressions to pyarrow (0a888de)\ndatafusion: enable topk (d44903f)\ndatafusion: implement Limit (1ddc876)\ndatafusion: implement ops.StringConcat (6bb5b4f)\ndecompile: support rendering ibis expression as python code (7eebc67)\ndeps: support shapely 2.0 (68dff10)\ndisplay qualified named in deprecation warnings (a6e2a49)\ndocs: first draft of Ibis for pandas users (7f7c9b5)\nduckdb: enable registration of parquet files from s3 (fced465)\nduckdb: implement mode aggregation (36fd152)\nduckdb: implement to_timestamp (26ca1e4)\nduckdb: implement quantile/multiquantile (fac9705)\nduckdb: overwrite views when calling register (ae07438)\nduckdb: pass through kwargs to file loaders (14fa2aa)\nduckdb: support out of core execution for in-memory connections (a4d4ba2)\nduckdb: support registering external postgres tables with duckdb (8633e6b)\nexpr: split ParseURL operation into multiple URL extract operations (1f0fcea)\nimpala: implement strftime (d3ede8d)\nimpala: support date literals (cd334c4)\ninsert: add support for list+dict to sqlalchemy backends (15d399e)\nir/pandas/dask/clickhouse: revamp Map type support (62b6f2d)\nir: add is_* methods to DataTypes (79f5c2b)\nir: prototype for parsing SQL into an ibis expression (1301183)\nir: support python 3.10 pattern matching on Annotable nodes (eca93eb)\nmssql: add window function support (ef1be45)\nmssql: detect schema from SQL (ff79928)\nmssql: extract quarter (7d04266)\nmssql: implement ops.DayOfWeekIndex (4125593)\nmssql: implement ops.ExtractDayOfYear (ae026d5)\nmssql: implement ops.ExtractEpochSeconds (4f49b5b)\nmssql: implement ops.ExtractWeekOfYear (f1394bc)\nmssql: implement ops.Ln, ops.Log, ops.Log2, ops.Log10 (f8ee1d8)\nmssql: implement ops.RandomScalar (4149450)\nmssql: implement ops.TimestampTruncate, ops.DateTruncate (738e496)\nmssql: implementt ops.DateFromYMD, ops.TimestampFromYMDHMS, ops.TimeFromHMS (e84f2ce)\nopen *.db files with sqlite in ibis.connect (37baf05)\npandas: implement mode aggregation (fc023b5)\npandas: implement RegexReplace for str (23713cc)\npandas: implement json getitem (8fa1190)\npandas: implement quantile/multiquantile (cd4dcaa)\npandas: support histogram API (5bfc0fe)\npolars: enable topk (8bfb16a)\npolars: implement mode aggregation (7982ba2)\npolars: initial support for polars backend (afecb0a)\npostgres: implement mode aggregation (b2f1c2d)\npostgres: implement quantile and multiquantile (82ed4f5)\npostgres: prettify array literals (cdc60d5)\npyspark: add support for struct operations (ce05987)\npyspark: enable topk (0f748e0)\npyspark: implement pi and e (fea81c6)\npyspark: implement json getitem (9bfb748)\npyspark: implement quantile and multiquantile (743f411)\npyspark: support histogram API (8f4808c)\nsnowflake: enable day-of-week column expression (6fd9c33)\nsnowflake: handle date and timestamp literals (ec2392d)\nsnowflake: implement mode aggregation (f35915e)\nsnowflake: implement parse_url (a9746e3)\nsnowflake: implement rowid scalar (7e1425a)\nsnowflake: implement time literal (068fc50)\nsnowflake: implement scalar (cc07d91)\nsnowflake: initial commit for snowflake backend (a8687dd)\nsnowflake: support reductions in window functions via automatic ordering (0234e5c)\nsql: add ops.StringSQLILike (7dc4924)\nsqlalchemy: implement ops.Where using IF/IFF functions (4cc9c15)\nsqlalchemy: in-memory tables have name in generated SQL (01b4c60)\nsql: improve error message in fixed_arity helper (891a1ad)\nsqlite: add type_map arg to override type inference (1961bad)\nsqlite: fix impl for missing pi and e functions (24b6d2f)\nsqlite: support con.sql with explicit schema specified (7ca82f3)\nsqlite: support wider range of datetime formats (f65093a)\nsupport both postgresql:// and postgres:// in ibis.connect (2f7a7b4)\nsupport deferred predicates in join (b51a64b)\nsupport more operations with unsigned integers (9992953)\nsupport passing callable to relabel (0bceefd)\nsupport tab completion for getitem access of table columns (732dba4)\nsupport Table.fillna for SQL backends (26d4cac)\ntrino: add bit_xor aggregation (830acf4)\ntrino: add EXTRACT-based functionality (6549657)\ntrino: add millisecond scale to *_trunc function (3065248)\ntrino: add some basic aggregation ops (7ecf7ab)\ntrino: extract milliseconds (09517a5)\ntrino: implement approx_median (1cba8bd)\ntrino: implement parse_url (2bc87fc)\ntrino: implement round, cot, pi, and e (c0e8736)\ntrino: implement arbitrary first support (0c7d3b3)\ntrino: implement array collect support (dfeb600)\ntrino: implement array column support (dadf9a8)\ntrino: implement array concat (240c55d)\ntrino: implement array index (c5f3a96)\ntrino: implement array length support (2d7cc65)\ntrino: implement array literal support (2182177)\ntrino: implement array repeat (2ee3d10)\ntrino: implement array slicing (643792e)\ntrino: implement basic struct operations (cc3c937)\ntrino: implement bitwise agg support (5288b35)\ntrino: implement bitwise scalar/column ops (ac4876c)\ntrino: implement default precision and scale (37f8a47)\ntrino: implement group concat support (5c41439)\ntrino: implement json getitem support (7c41566)\ntrino: implement map operations (4efc5ce)\ntrino: implement more generic and numeric ops (63b45c8)\ntrino: implement ops.Capitalize (dff14fc)\ntrino: implement ops.DateFromYMD (edd2994)\ntrino: implement ops.DateTruncate, ops.TimestampTruncate (32f4862)\ntrino: implement ops.DayOfWeekIndex, ops.DayOfWeekName (a316d6d)\ntrino: implement ops.ExtractDayOfYear (b0a3465)\ntrino: implement ops.ExtractEpochSeconds (10b82f1)\ntrino: implement ops.ExtractWeekOfYear (cf719b8)\ntrino: implement ops.Repeat (e9f6851)\ntrino: implement ops.Strftime (a436823)\ntrino: implement ops.StringAscii (93fd32d)\ntrino: implement ops.StringContains (d5cb2ec)\ntrino: implement ops.StringSplit (62d79a6)\ntrino: implement ops.StringToTimestamp (b766f62)\ntrino: implement ops.StrRight (691b39c)\ntrino: implement ops.TimeFromHMS (e5cacc2)\ntrino: implement ops.TimestampFromUNIX (ce5d726)\ntrino: implement ops.TimestampFromYMDHMS (9fa7304)\ntrino: implement ops.TimestampNow (c832e4c)\ntrino: implement ops.Translate (410ae1e)\ntrino: implement quantile/multiquantile (bc7fdab)\ntrino: implement regex functions (9e493c5)\ntrino: implement window function support (5b6cc45)\ntrino: initial trino backend (c367865)\ntrino: support string date scalar parameter (9092530)\ntrino: use proper approx_distinct function (3766fff)\n\n\n\nBug Fixes\n\nibis.connect always returns a backend (2d5b155)\nallow inserting memtable with alchemy backends (c02fcc3)\nalways display at least one column in the table repr (5ea9e5a)\nanalysis: only lower sort keys that are in an agg’s output (6bb4f66)\napi: allow arbitrary sort keys (a980b34)\napi: allow boolean scalars in predicate APIs (2a2636b)\napi: allow deferred instances as input to ibis.desc and ibis.asc (6861347)\napi: ensure that window functions are propagated (4fb1106)\napi: make re_extract conform to semantics of Python’s re.match (5981227)\nauto-register csv and parquet with duckdb using ibis.connect (67c4f87)\navoid renaming known equal columns for inner joins with equality predicates (5d4b0ed)\nbackends: fix casting and execution result types in many backends (46c21dc)\nbigquery: don’t try to parse database when name is already fully qualified (ae3c113)\nbigquery: fix integer to timestamp casting (f5bacad)\nbigquery: normalize W frequency in *_trunc (893cd49)\ncatch TypeError instead of more specific error (6db19d8)\nchange default limit to None (8d1526a)\nclarify and normalize behavior of Table.rowid (92b03d6)\nclickhouse: ensure that correlated subqueries’ columns can be referenced (708d682)\nclickhouse: fix list_tables to use database name (edc3511)\nclickhouse: make any/all filterable and reduce code size (99b10e2)\nclickhouse: use clickhouse’s dbapi (bd0da12)\ncommon: support copying variadic annotable instances (ee0d9ad)\ndask: make filterable reductions work (0f759fc)\ndask: raise TypeError with informative message in ibis.dask.connect (4e67f7a)\ndefine to_pandas/to_pyarrow on DataType/Schema classes directly (22f3b4d)\ndeps: bound shapely to a version that doesn’t segfault (be5a779)\ndeps: update dependency datafusion to &gt;=0.6,&lt;0.8 (4c73870)\ndeps: update dependency geopandas to &gt;=0.6,&lt;0.13 (58a32dc)\ndeps: update dependency packaging to v22 (e0b6177)\ndeps: update dependency rich to v13 (4f313dd)\ndeps: update dependency sqlglot to v10 (db19d43)\ndeps: update dependency sqlglot to v9 (cf330ac)\ndocs: make sure data can be downloaded when building notebooks (fa7da17)\ndon’t fuse filters & selections that contain window functions (d757069)\ndrop snowflake support for RowID (dd378f1)\nduckdb: drop incorrect translate implementation (8690151)\nduckdb: fix bug in json getitem for duckdb (49ce739)\nduckdb: keep ibis.now() type semantics (eca4a2c)\nduckdb: make array repeat actually work (021f4de)\nduckdb: replace all in re_replace (c138f0f)\nduckdb: rereflect sqla table on re-registration (613b311), closes #4729\nduckdb: s3 priority (a2d03d1)\nduckdb: silence duckdb-engine warnings (359adc3)\nensure numpy ops dont accidentally cast ibis types (a7ca6c8)\nexclude geospatial ops from pandas/dask/polars has_operation (6f1d265)\nfix table.mutate with deferred named expressions (5877d0b)\nfix bug when disabling show_types in interactive repr (2402506)\nfix expression repr for table -&gt; value operations (dbf92f5)\nhandle dimensionality of empty outputs (3a88170)\nimprove rich repr support (522db9c)\nir: normalize date types (39056b5)\nir: normalize timestamps to datetime.datetime values (157efde)\nmake col.day_of_week not an expr (96e1580)\nmssql: fix integer to timestamp casting (9122eef)\nmssql: fix ops.TimeFromHMS (d2188e1)\nmssql: fix ops.TimestampFromUNIX (ec28add)\nmssql: fix round without argument (52a60ce)\nmssql: use double-dollar sign to prevent from interpolating a value (b82da5d)\nmysql: fix mysql startswith/endswith to be case sensitive (d7469cc)\nmysql: handle out of bounds timestamps and fix milliseconds calculation (1f7649a)\nmysql: upcast bool agg args (8c5f9a5)\npandas/dask now cast int&lt;-&gt;timestamp as seconds since epoch (bbfe998)\npandas: drop RowID implementation (05f5016)\npandas: make quantile/multiquantile with filter work (6b5abd6)\npandas: support substr with no length (b2c2922)\npandas: use localized UTC time for now operation (f6d7327)\npandas: use the correct context when aggregating over a window (e7fa5c0)\npolars: fix polars startswith to call the right method (9e6f397)\npolars: workaround passing pl.Null to the null type (fd9633b)\npostgres/duckdb: fix negative slicing by copying the trino impl (39e3962)\npostgres: fix array repeat to work with literals (3c46eb1)\npostgres: fix array_index operation (63ef892)\npostgres: make any/all translation rules use reduction helper (78bfd1d)\npyspark: handle datetime.datetime literals (4f94abe)\nremove kerberos extra for impala dialect (6ed3e5f)\nrepr: don’t repeat value in repr for literals (974eeb6)\nrepr: fix off by one in repr (322c8dc)\ns3: fix quoting and autonaming for s3 (ce09266)\nselect: raise error on attempt to select no columns in projection (94ac10e)\nsnowflake: fix extracting query parameter by (75af240)\nsnowflake: fix failing snowflake url extraction functions (2eee50b)\nsnowflake: fix snowflake list_databases (680cd24)\nsnowflake: handle schema when getting table (f6fff5b)\nsnowflake: snowflake now likes Tuesdays (1bf9d7c)\nsqlalchemy: allow passing pd.DataFrame to create (1a083f6)\nsqlalchemy: ensure that arbitrary expressions are valid sort keys (cb1a013)\nsql: avoid generating cartesian products yet again (fdc52a2)\nsqlite: fix sqlite startswith/endswith to be case sensitive (fd4a88d)\nstandardize list_tables signature everywhere (abafe1b), closes #2877\nsupport arbitrary with no arguments (45156f5)\nsupport dtype in __array__ methods (1294b76)\ntest: ensure that file-based url tests don’t expect data to exist (c2b635a)\ntrino: fix integer to timestamp casting (49321a6)\ntrino: make filterable any/all reductions work (992bd18)\ntruncate columns in repr for wide tables (aadcba1)\ntypo: in StringValue helpstr (b2e2093)\nux: improve error messages for rlz.comparable failures (5ca41d2)\nux: prevent infinite looping when formatting a floating column of all nans (b6afe98)\nvisualize(label_edges=True) works for NodeList ops (a91ceae)\nvisualize: dedup nodes and edges and add verbose argument for debugging (521e188)\nvisualize: handle join predicates in visualize (d63cb57)\nwindow: allow window range tuples in preceding or following (77172b3)\n\n\n\nDeprecations\n\ndeprecate Table.groupby alias in favor of Table.group_by (39cea3b)\ndeprecate Table.sort_by in favor of Table.order_by (7ac7103)\n\n\n\nPerformance\n\nadd benchmark for known-slow table expression (e9617f0)\nexpr: traverse nodes only once during compilation (69019ed)\nfix join performance by avoiding Projection construction (ed532bf)\nnode: give Nodes the default Python repr (eb26b11)\nux: remove pandas import overhead from import ibis (ea452fc)\ndeps: bump duckdb lower bound (4539683)\ndev-deps: replace flake8 et al with ruff and fix lints (9c1b282)\n\n\n\nRefactors\n\nadd lazy_singledispatch utility (180ecff)\nadd rlz.lazy_instance_of (4e30480)\nadd Temporal base class for temporal data types (694eec4)\napi: add deprecated Node.op() #4519 (2b0826b)\navoid roundtripping to expression for IFF (3068ae2)\nclean up cot implementations to have one less function call (0f304e5)\nclean up timezone support in ops.TimestampFromYMDHMS (2e183a9)\ncleanup str method docstrings (36bd36c)\nclickhouse: implement sqlglot-based compiler (5cc5d4b)\nclickhouse: simplify Quantile and MultiQuantile implementation (9e16e9e)\ncommon: allow traversal and substitution of tuple and dictionary arguments (60f4806)\ncommon: enforce slots definitions for Base subclasses (6c3df91)\ncommon: move Parameter and Signature to validators.py (da20537)\ncommon: reduce implementation complexity of annotations (27cee71)\ndatafusion: align register API across backends (08046aa)\ndatafusion: get name from expr (fea3e5b)\ndatatypes: remove Enum (145e706)\ndev-deps: remove unnecessary poetry2nix overrides (5ed95bc)\ndon’t sort new columns in mutate (72ec96a)\nduckdb: use lambda to define backend operations (5d14de6)\nimpala: move impala SQL tests to snapshots (927bf65)\nimpala: replace custom pooling with sqlalchemy QueuePool (626cdca)\nir: ops.List -&gt; ops.NodeList (6765bd2)\nir: better encapsulate graph traversal logic, schema and datatype objects are not traversable anymore (1a07725)\nir: generalize handling and traversal of node sequences (e8bcd0f)\nir: make all value operations ‘Named’ for more consistent naming semantics (f1eb4d2)\nir: move random() to api.py (e136f1b)\nir: remove ops.DeferredSortKey (e629633)\nir: remove ops.TopKNode and ir.TopK (d4dc544)\nir: remove Analytic expression’s unused type() method (1864bc1)\nir: remove DecimalValue.precision(), DecimalValue.scale() method (be975bc)\nir: remove DestructValue expressions (762d384)\nir: remove duplicated literal creation code (7dfb56f)\nir: remove intermediate expressions (c6fb0c0)\nir: remove lin.lineage() since it’s not used anywhere (120b1d7)\nir: remove node.blocks() in favor of more explicit type handling (37d8ce4)\nir: remove Node.inputs since it is an implementation detail of the pandas backend (6d2c49c)\nir: remove node.root_tables() and unify parent table handling (fbb07c1)\nir: remove ops.AggregateSelection in favor of an.simplify_aggregation (ecf6ed3)\nir: remove ops.NodeList and ir.List in favor of builtin tuples (a90ce35)\nir: remove pydantic dependency and make grounds more composable (9da0f41)\nir: remove sch.HasSchema and introduce ops.Projection base class for ops.Selection (c3b0139)\nir: remove unnecessary complexity introduced by variadic annotation (698314b)\nir: resolve circular imports so operations can be globally imported for types (d2a3919)\nir: simplify analysis.substitute_unbound() (a6c7406)\nir: simplify SortKey construction using rules (4d63280)\nir: simplify switch-case builders (9acf717)\nir: split datatypes package into multiple submodules (cce6535)\nir: split out table count into CountStar operation (e812e6e)\nir: support replacing nodes in the tree (6a0df5a)\nir: support variadic annotable arguments and add generic graph traversal routines (5d6a289)\nir: unify aggregation construction to use AggregateSelection (c7d6a6f)\nmake quantile, any, and all reductions filterable (1bafc9e)\nmake sure value_counts always has a projection (a70a302)\nmssql: use lambda to define backend operations (1437cfb)\nmysql: dedup extract code (d551944)\nmysql: use lambda to define backend operations (d10bff8)\npolars: match duckdb registration api (ac59dac)\npostgres: use lambda to define backend operations (4c85d7b)\nremove dead compat.py module (eda0fdb)\nremove deprecated approximate aggregation classes (53fc6cb)\nremove deprecated functions and classes (be1cdda)\nremove duplicate _random_identifier calls (26e7942)\nremove setup.py and related infrastructure (adfcce1)\nremove the JSONB type (c4fc0ec)\nrename some infer methods for consistency (a8f5579)\nreplace isinstance dtype checking with is_* methods (386adc2)\nrework registration / file loading (c60e30d)\nrules: generalize field referencing using rlz.ref() (0afb8b9)\nsimplify ops.ArrayColumn in postgres backend (f9677cc)\nsimplify histogram implementation by using window functions (41cbc29)\nsimplify ops.ArrayColumn in alchemy backend (28ff4a8)\nsnowflake: use lambda to define backend operations (cb33fce)\nsplit up custom nix code; remove unused derivations (57dff10)\nsqlite: use lambda to define backend operations (b937391)\ntest: make clickhouse tests use pytest-snapshot (413dbd2)\ntests: move sql output to golden dir (6a6a453)\ntest: sort regex test cases by name instead of posix-ness (0dfb0e7)\ntests: replace sqlgolden with pytest-snapshot (5700eb0)\ntimestamps: remove timezone argument to to_timestamp API (eb4762e)\ntrino: use lambda to define backend operations (dbd61a5)\nuncouple MultiQuantile class from Quantile (9c48f8c)\nuse rlz.lazy_instance_of to delay shapely import (d14badc)\nuse lazy dispatch for dt.infer (2e56540)\n\n\n\nDocumentation\n\nadd backend_sensitive decorator (836f237)\nadd pip install poetry dev env setup step (69940b1)\nadd bigquery ci data analysis notebook (2b1d4e5)\nadd how to sessionize guide (18989dd)\nadd issue templates (4480c18)\nadd missing argument descriptions (ea757fa)\nadd mssql backend page (63c0f19)\nadded 4.0 release blog post (bcc0eca)\nadded memtable howto guide (5dde9bd)\nbackends: add duckdb and mssql to the backend index page (7b13218)\nbring back git revision localized date plugin (e4fc2c9)\ncreated how to guide for deferred expressions (2a9f6ab)\ndev: python-duckdb now available for windows with conda (7f76b09)\ndocument how to create a table from a pandas dataframe using ibis.memtable (c6521ec)\nfix backends label in feature request issue form (cf852d3)\nfix broken docstrings; reduce docstring noise; workaround griffe (bd1c637)\nfix docs for building docs (23af567)\nfix feature-request issue template (6fb62f5)\nfix installation section for conda (7af6ac1)\nfix landing page links (1879362)\nfix links to make docs work locally and remotely (13c7810)\nfix pyarrow batches docstring (dba9594)\nfix single line docstring summaries (8028201)\nfix snowflake doc link in readme.md (9aff68e)\nfix the inline example for ibis.dask.do_connect (6a533f0)\nfix tutorial link on install page (b34811a)\nfix typo in first example of the homepage (9a8a25a)\nformatting and syntax highlighting fixes (50864da)\nfront page rework (24b795a)\nhow-to: use parquet data source for sessionization, fix typos, more deferred usage (974be37)\nimprove the docstring of the generic connect method (ee87802)\nissue template cleanups (fed37da)\nlist (e331247)\npolars: add backend docs page (e303b68)\nremove hrs (4c30de4)\nrenamed how to guides to be more consistent (1bdc5bd)\nsentence structure in the Notes section (ac20232)\nshow interactive prompt for python (5d7d913)\nsplit out geospatial operations in the support matrix docs (0075c28)\ntrino: add backend docs (2f262cd)\ntypo (6bac645)\ntypos headers and formatting (9566cbb)\nudf: examples in pandas have the incorrect import path (49028b8)\nupdate filename (658a296)\nupdate line (4edfce0)\nupdate readme (19a3f3c)\nuse buf/feat prefix only (2561a29)\nuse components instead of pieces (179ca1e)\nuse heading instead of bulleted bold (99b044e)\nuse library instead of project (fd2d915)\nuse present tense for use cases and “why” section (6cc7416)\nwww: fix frontpage example (7db39e8)"
  },
  {
    "objectID": "release_notes.html#section-5",
    "href": "release_notes.html#section-5",
    "title": "2.1.0 (2022-01-12)",
    "section": "3.2.0 (2022-09-15)",
    "text": "3.2.0 (2022-09-15)\n\nFeatures\n\nadd api to get backend entry points (0152f5e)\napi: add and_ and or_ helpers (94bd4df)\napi: add argmax and argmin column methods (b52216a)\napi: add distinct to Intersection and Difference operations (cd9a34c)\napi: add ibis.memtable API for constructing in-memory table expressions (0cc6948)\napi: add ibis.sql to easily get a formatted SQL string (d971cc3)\napi: add Table.unpack() and StructValue.lift() APIs for projecting struct fields (ced5f53)\napi: allow transmute-style select method (d5fc364)\napi: implement all bitwise operators (7fc5073)\napi: promote psql to a show_sql public API (877a05d)\nclickhouse: add dataframe external table support for memtables (bc86aa7)\nclickhouse: add enum, ipaddr, json, lowcardinality to type parser (8f0287f)\nclickhouse: enable support for working window functions (310a5a8)\nclickhouse: implement argmin and argmax (ee7c878)\nclickhouse: implement bitwise operations (348cd08)\nclickhouse: implement struct scalars (1f3efe9)\ndask: implement StringReplace execution (1389f4b)\ndask: implement ungrouped argmin and argmax (854aea7)\ndeps: support duckdb 0.5.0 (47165b2)\nduckdb: handle query parameters in ibis.connect (fbde95d)\nduckdb: implement argmin and argmax (abf03f1)\nduckdb: implement bitwise xor (ca3abed)\nduckdb: register tables from pandas/pyarrow objects (36e48cc)\nduckdb: support unsigned integer types (2e67918)\nimpala: implement bitwise operations (c5302ab)\nimplement dropna for SQL backends (8a747fb)\nlog: make BaseSQLBackend._log print by default (12de5bb)\nmysql: register BLOB types (1e4fb92)\npandas: implement argmin and argmax (bf9b948)\npandas: implement NotContains on grouped data (976dce7)\npandas: implement StringReplace execution (578795f)\npandas: implement Contains with a group by (c534848)\npostgres: implement bitwise xor (9b1ebf5)\npyspark: add option to treat nan as null in aggregations (bf47250)\npyspark: implement ibis.connect for pyspark (a191744)\npyspark: implement Intersection and Difference (9845a3c)\npyspark: implement bitwise operators (33cadb1)\nsqlalchemy: implement bitwise operator translation (bd9f64c)\nsqlalchemy: make ibis.connect with sqlalchemy backends (b6cefb9)\nsqlalchemy: properly implement Intersection and Difference (2bc0b69)\nsql: implement StringReplace translation (29daa32)\nsqlite: implement bitwise xor and bitwise not (58c42f9)\nsupport table.sort_by(ibis.random()) (693005d)\ntype-system: infer pandas’ string dtype (5f0eb5d)\nux: add duckdb as the default backend (8ccb81d)\nux: use rich to format Table.info() output (67234c3)\nux: use sqlglot for pretty printing SQL (a3c81c5)\nvariadic union, intersect, & difference functions (05aca5a)\n\n\n\nBug Fixes\n\napi: make sure column names that are already inferred are not overwritten (6f1cb16)\napi: support deferred objects in existing API functions (241ce6a)\nbackend: ensure that chained limits respect prior limits (02a04f5)\nbackends: ensure select after filter works (e58ca73)\nbackends: only recommend installing ibis-foo when foo is a known backend (ac6974a)\nbase-sql: fix String-generating backend string concat implementation (3cf78c1)\nclickhouse: add IPv4/IPv6 literal inference (0a2f315)\nclickhouse: cast repeat times argument to UInt64 (b643544)\nclickhouse: fix listing tables from databases with no tables (08900c3)\ncompilers: make sure memtable rows have names in the SQL string compilers (18e7f95)\ncompiler: use repr for SQL string VALUES data (75af658)\ndask: ensure predicates are computed before projections (5cd70e1)\ndask: implement timestamp-date binary comparisons (48d5058)\ndask: set dask upper bound due to large scale test breakage (796c645), closes #9221\ndecimal: add decimal type inference (3fe3fd8)\ndeps: update dependency duckdb-engine to &gt;=0.1.8,&lt;0.4.0 (113dc8f)\ndeps: update dependency duckdb-engine to &gt;=0.1.8,&lt;0.5.0 (ef97c9d)\ndeps: update dependency parsy to v2 (9a06131)\ndeps: update dependency shapely to &gt;=1.6,&lt;1.8.4 (0c787d2)\ndeps: update dependency shapely to &gt;=1.6,&lt;1.8.5 (d08c737)\ndeps: update dependency sqlglot to v5 (f210bb8)\ndeps: update dependency sqlglot to v6 (5ca4533)\nduckdb: add missing types (59bad07)\nduckdb: ensure that in-memory connections remain in their creating thread (39bc537)\nduckdb: use fetch_arrow_table() to be able to handle big timestamps (85a76eb)\nfix bug in pandas & dask difference implementation (88a78fa)\nfix dask where implementation (49f8845)\nimpala: add date column dtype to impala to ibis type dict (c59e94e), closes #4449\npandas where supports scalar for left (48f6c1e)\npandas: fix anti-joins (10a659d)\npandas: implement timestamp-date binary comparisons (4fc666d)\npandas: properly handle empty groups when aggregating with GroupConcat (6545f4d)\npyspark: fix broken StringReplace implementation (22cb297)\npyspark: make sure ibis.connect works with pyspark (a7ab107)\npyspark: translate predicates before projections (b3d1c80)\nsqlalchemy: fix float64 type mapping (8782773)\nsqlalchemy: handle reductions with multiple arguments (5b2039b)\nsqlalchemy: implement SQLQueryResult translation (786a50f)\nsql: fix sql compilation after making InMemoryTable a subclass of PhysicalTable (aac9524)\nsquash several bugs in sort_by asc/desc handling (222b2ba)\nsupport chained set operations in SQL backends (227aed3)\nsupport filters on InMemoryTable exprs (abfaf1f)\ntypo: in BaseSQLBackend.compile docstring (0561b13)\n\n\n\nDeprecations\n\nright kwarg in union/intersect/difference (719a5a1)\nduckdb: deprecate path argument in favor of database (fcacc20)\nsqlite: deprecate path argument in favor of database (0f85919)\n\n\n\nPerformance\n\npandas: remove reexecution of alias children (64efa53)\npyspark: ensure that pyspark DDL doesn’t use VALUES (422c98d)\nsqlalchemy: register DataFrames cheaply where possible (ee9f1be)\n\n\n\nDocumentation\n\nadd to_sql (e2821a5)\nadd back constraints for transitive doc dependencies and fix docs (350fd43)\nadd coc reporting information (c2355ba)\nadd community guidelines documentation (fd0893f)\nadd HeavyAI to the readme (4c5ca80)\nadd how-to bfill and ffill (ff84027)\nadd how-to for ibis+duckdb register (73a726e)\nadd how-to section to docs (33c4b93)\nduckdb: add installation note for duckdb &gt;= 0.5.0 (608b1fb)\nfix memtable docstrings (72bc0f5)\nfix flake8 line length issues (fb7af75)\nfix markdown (4ab6b95)\nfix relative links in tutorial (2bd075f), closes #4064 #4201\nmake attribution style uniform across the blog (05561e0)\nmove the blog out to the top level sidebar for visibility (417ba64)\nremove underspecified UDF doc page (0eb0ac0)"
  },
  {
    "objectID": "release_notes.html#section-6",
    "href": "release_notes.html#section-6",
    "title": "2.1.0 (2022-01-12)",
    "section": "3.1.0 (2022-07-26)",
    "text": "3.1.0 (2022-07-26)\n\nFeatures\n\nadd __getattr__ support to StructValue (75bded1)\nallow selection subclasses to define new node args (2a7dc41)\napi: accept Schema objects in public ibis.schema (0daac6c)\napi: add .tables accessor to BaseBackend (7ad27f0)\napi: add e function to public API (3a07e70)\napi: add ops.StructColumn operation (020bfdb)\napi: add cume_dist operation (6b6b185)\napi: add toplevel ibis.connect() (e13946b)\napi: handle literal timestamps with timezone embedded in string (1ae976b)\napi: ibis.connect() default to duckdb for parquet/csv extensions (ff2f088)\napi: make struct metadata more convenient to access (3fd9bd8)\napi: support tab completion for backends (eb75fc5)\napi: underscore convenience api (81716da)\napi: unnest (98ecb09)\nbackends: allow column expressions from non-foreign tables on the right side of isin/notin (e1374a4)\nbase-sql: implement trig and math functions (addb2c1)\nclickhouse: add ability to pass arbitrary kwargs to Clickhouse do_connect (583f599)\nclickhouse: implement ops.StructColumn operation (0063007)\nclickhouse: implement array collect (8b2577d)\nclickhouse: implement ArrayColumn (1301f18)\nclickhouse: implement bit aggs (f94a5d2)\nclickhouse: implement clip (12dfe50)\nclickhouse: implement covariance and correlation (a37c155)\nclickhouse: implement degrees (7946c0f)\nclickhouse: implement proper type serialization (80f4ab9)\nclickhouse: implement radians (c7b7f08)\nclickhouse: implement strftime (222f2b5)\nclickhouse: implement struct field access (fff69f3)\nclickhouse: implement trig and math functions (c56440a)\nclickhouse: support subsecond timestamp literals (e8698a6)\ncompiler: restore intersect_class and difference_class overrides in base SQL backend (2c46a15)\ndask: implement trig functions (e4086bb)\ndask: implement zeroifnull (38487db)\ndatafusion: implement negate (69dd64d)\ndatafusion: implement trig functions (16803e1)\nduckdb: add register method to duckdb backend to load parquet and csv files (4ccc6fc)\nduckdb: enable find_in_set test (377023d)\nduckdb: enable group_concat test (4b9ad6c)\nduckdb: implement ops.StructColumn operation (211bfab)\nduckdb: implement approx_count_distinct (03c89ad)\nduckdb: implement approx_median (894ce90)\nduckdb: implement arbitrary first and last aggregation (8a500bc)\nduckdb: implement NthValue (1bf2842)\nduckdb: implement strftime (aebc252)\nduckdb: return the ir.Table instance from DuckDB’s register API (0d05d41)\nmysql: implement FindInSet (e55bbbf)\nmysql: implement StringToTimestamp (169250f)\npandas: implement bitwise aggregations (37ff328)\npandas: implement degrees (25b4f69)\npandas: implement radians (6816b75)\npandas: implement trig functions (1fd52d2)\npandas: implement zeroifnull (48e8ed1)\npostgres/duckdb: implement covariance and correlation (464d3ef)\npostgres: implement ArrayColumn (7b0a506)\npyspark: implement approx_count_distinct (1fe1d75)\npyspark: implement approx_median (07571a9)\npyspark: implement covariance and correlation (ae818fb)\npyspark: implement degrees (f478c7c)\npyspark: implement nth_value (abb559d)\npyspark: implement nullifzero (640234b)\npyspark: implement radians (18843c0)\npyspark: implement trig functions (fd7621a)\npyspark: implement Where (32b9abb)\npyspark: implement xor (550b35b)\npyspark: implement zeroifnull (db13241)\npyspark: topk support (9344591)\nsqlalchemy: add degrees and radians (8b7415f)\nsqlalchemy: add xor translation rule (2921664)\nsqlalchemy: allow non-primitive arrays (4e02918)\nsqlalchemy: implement approx_count_distinct as count distinct (4e8bcab)\nsqlalchemy: implement clip (8c02639)\nsqlalchemy: implement trig functions (34c1514)\nsqlalchemy: implement Where (7424704)\nsqlalchemy: implement zeroifnull (4735e9a)\nsqlite: implement BitAnd, BitOr and BitXor (e478479)\nsqlite: implement cotangent (01e7ce7)\nsqlite: implement degrees and radians (2cf9c5e)\n\n\n\nBug Fixes\n\napi: bring back null datatype parsing (fc131a1)\napi: compute the type from both branches of Where expressions (b8f4120)\napi: ensure that Deferred objects work in aggregations (bbb376c)\napi: ensure that nulls can be cast to any type to allow caller promotion (fab4393)\napi: make ExistSubquery and NotExistsSubquery pure boolean operations (dd70024)\nbackends: make execution transactional where possible (d1ea269)\nclickhouse: cast empty result dataframe (27ae68a)\nclickhouse: handle empty IN and NOT IN expressions (2c892eb)\nclickhouse: return null instead of empty string for group_concat when values are filtered out (b826b40)\ncompiler: fix bool bool comparisons (1ac9a9e)\ndask/pandas: allow limit to be None (9f91d6b)\ndask: aggregation with multi-key groupby fails on dask backend (4f8bc70)\ndatafusion: handle predicates in aggregates (4725571)\ndeps: update dependency datafusion to &gt;=0.4,&lt;0.7 (f5b244e)\ndeps: update dependency duckdb to &gt;=0.3.2,&lt;0.5.0 (57ee818)\ndeps: update dependency duckdb-engine to &gt;=0.1.8,&lt;0.3.0 (3e379a0)\ndeps: update dependency geoalchemy2 to &gt;=0.6.3,&lt;0.13 (c04a533)\ndeps: update dependency geopandas to &gt;=0.6,&lt;0.12 (b899c37)\ndeps: update dependency Shapely to &gt;=1.6,&lt;1.8.3 (87a49ad)\ndeps: update dependency toolz to &gt;=0.11,&lt;0.13 (258a641)\ndon’t mask udf module in init.py (3e567ba)\nduckdb: ensure that paths with non-extension . chars are parsed correctly (9448fd3)\nduckdb: fix struct datatype parsing (5124763)\nduckdb: force string_agg separator to be a constant (21cdf2f)\nduckdb: handle multiple dotted extensions; quote names; consolidate implementations (1494246)\nduckdb: remove timezone function invocation (33d38fc)\ngeospatial: ensure that later versions of numpy are compatible with geospatial code (33f0afb)\nimpala: a delimited table explicitly declare stored as textfile (04086a4), closes #4260\nimpala: remove broken nth_value implementation (dbc9cc2)\nir: don’t attempt fusion when projections aren’t exactly equivalent (3482ba2)\nmysql: cast mysql timestamp literals to ensure correct return type (8116e04)\nmysql: implement integer to timestamp using from_unixtime (1b43004)\npandas/dask: look at pre_execute for has_operation reporting (cb44efc)\npandas: execute negate on bool as not (330ab4f)\npandas: fix struct inference from dict in the pandas backend (5886a9a)\npandas: force backend options registration on trace.enable() calls (8818fe6)\npandas: handle empty boolean column casting in Series conversion (f697e3e)\npandas: handle struct columns with NA elements (9a7c510)\npandas: handle the case of selection from a join when remapping overlapping column names (031c4c6)\npandas: perform correct equality comparison (d62e7b9)\npostgres/duckdb: cast after milliseconds computation instead of after extraction (bdd1d65)\npyspark: handle predicates in Aggregation (842c307)\npyspark: prevent spark from trying to convert timezone of naive timestamps (dfb4127)\npyspark: remove xpassing test for #2453 (c051e28)\npyspark: specialize implementation of has_operation (5082346)\npyspark: use empty check for collect_list in GroupConcat rule (df66acb)\nrepr: allow DestructValue selections to be formatted by fmt (4b45d87)\nrepr: when formatting DestructValue selections, use struct field names as column names (d01fe42)\nsqlalchemy: fix parsing and construction of nested array types (e20bcc0)\nsqlalchemy: remove unused second argument when creating temporary views (8766b40)\nsqlite: register conversion to isoformat for pandas.Timestamp (fe95dca)\nsqlite: test case with whitespace at the end of the line (7623ae9)\nsql: use isoformat for timestamp literals (70d0ba6)\ntype-system: infer null datatype for empty sequence of expressions (f67d5f9)\nuse bounded precision for decimal aggregations (596acfb)\n\n\n\nPerformance Improvements\n\nanalysis: add _projection as cached_property to avoid reconstruction of projections (98510c8)\nlineage: ensure that expressions are not traversed multiple times in most cases (ff9708c)\n\n\n\nReverts\n\nci: install sqlite3 on ubuntu (1f2705f)\n\n\n\n3.0.2 (2022-04-28)\n\n\nBug Fixes\n\ndocs: fix tempdir location for docs build (dcd1b22)\n\n\n\n3.0.1 (2022-04-28)\n\n\nBug Fixes\n\nbuild: replace version before exec plugin runs (573139c)"
  },
  {
    "objectID": "release_notes.html#section-9",
    "href": "release_notes.html#section-9",
    "title": "2.1.0 (2022-01-12)",
    "section": "3.0.0 (2022-04-25)",
    "text": "3.0.0 (2022-04-25)\n\n⚠ BREAKING CHANGES\n\nir: The following are breaking changes due to simplifying expression internals\n\nibis.expr.datatypes.DataType.scalar_type and DataType.column_type factory methods have been removed, DataType.scalar and DataType.column class fields can be used to directly construct a corresponding expression instance (though prefer to use operation.to_expr())\nibis.expr.types.ValueExpr._name and ValueExpr._dtype`` fields are not   accassible anymore. While these were not supposed to used directly nowValueExpr.has_name(),ValueExpr.get_name()andValueExpr.type()` methods are the only way to retrieve the expression’s name and datatype.\nibis.expr.operations.Node.output_type is a property now not a method, decorate those methods with @property\nibis.expr.operations.Value subclasses must define output_shape and output_dtype properties from now on (note the datatype abbreviation dtype in the property name)\nibis.expr.rules.cast(), scalar_like() and array_like() rules have been removed\n\napi: Replace t[\"a\"].distinct() with t[[\"a\"]].distinct().\ndeps: The sqlalchemy lower bound is now 1.4\nir: Schema.names and Schema.types attributes now have tuple type rather than list\nexpr: Columns that were added or used in an aggregation or mutation would be alphabetically sorted in compiled SQL outputs. This was a vestige from when Python dicts didn’t preserve insertion order. Now columns will appear in the order in which they were passed to aggregate or mutate\napi: dt.float is now dt.float64; use dt.float32 for the previous behavior.\nir: Relation-based execute_node dispatch rules must now accept tuples of expressions.\nir: removed ibis.expr.lineage.{roots,find_nodes} functions\nconfig: Use ibis.options.graphviz_repr = True to enable\nhdfs: Use fsspec instead of HDFS from ibis\nudf: Vectorized UDF coercion functions are no longer a public API.\nThe minimum supported Python version is now Python 3.8\nconfig: register_option is no longer supported, please submit option requests upstream\nbackends: Read tables with pandas.read_hdf and use the pandas backend\nThe CSV backend is removed. Use Datafusion for CSV execution.\nbackends: Use the datafusion backend to read parquet files\nExpr() -&gt; Expr.pipe()\ncoercion functions previously in expr/schema.py are now in udf/vectorized.py\napi: materialize is removed. Joins with overlapping columns now have suffixes.\nkudu: use impala instead: https://kudu.apache.org/docs/kudu_impala_integration.html\nAny code that was relying implicitly on string-y behavior from UUID datatypes will need to add an explicit cast first.\n\n\n\nFeatures\n\nadd repr_html for expressions to print as tables in ipython (cd6fa4e)\nadd duckdb backend (667f2d5)\nallow construction of decimal literals (3d9e865)\napi: add ibis.asc expression (efe177e), closes #1454\napi: add has_operation API to the backend (4fab014)\napi: implement type for SortExpr (ab19bd6)\nclickhouse: implement string concat for clickhouse (1767205)\nclickhouse: implement StrRight operation (67749a0)\nclickhouse: implement table union (e0008d7)\nclickhouse: implement trim, pad and string predicates (a5b7293)\ndatafusion: implement Count operation (4797a86)\ndatatypes: unbounded decimal type (f7e6f65)\ndate: add ibis.date(y,m,d) functionality (26892b6), closes #386\nduckdb/postgres/mysql/pyspark: implement .sql on tables for mixing sql and expressions (00e8087)\nduckdb: add functionality needed to pass integer to interval test (e2119e8)\nduckdb: implement _get_schema_using_query (93cd730)\nduckdb: implement now() function (6924f50)\nduckdb: implement regexp replace and extract (18d16a7)\nimplement force argument in sqlalchemy backend base class (9df7f1b)\nimplement coalesce for the pyspark backend (8183efe)\nimplement semi/anti join for the pandas backend (cb36fc5)\nimplement semi/anti join for the pyspark backend (3e1ba9c)\nimplement the remaining clickhouse joins (b3aa1f0)\nir: rewrite and speed up expression repr (45ce9b2)\nmysql: implement _get_schema_from_query (456cd44)\nmysql: move string join impl up to alchemy for mysql (77a8eb9)\npostgres: implement _get_schema_using_query (f2459eb)\npyspark: implement Distinct for pyspark (4306ad9)\npyspark: implement log base b for pyspark (527af3c)\npyspark: implement percent_rank and enable testing (c051617)\nrepr: add interval info to interval repr (df26231)\nsqlalchemy: implement ilike (43996c0)\nsqlite: implement date_truncate (3ce4f2a)\nsqlite: implement ISO week of year (714ff7b)\nsqlite: implement string join and concat (6f5f353)\nsupport of arrays and tuples for clickhouse (db512a8)\nver: dynamic version identifiers (408f862)\n\n\n\nBug Fixes\n\nadded wheel to pyproject toml for venv users (b0b8e5c)\nallow major version changes in CalVer dependencies (9c3fbe5)\nannotable: allow optional arguments at any position (778995f), closes #3730\napi: add ibis.map and .struct (327b342), closes #3118\napi: map string multiplication with integer to repeat method (b205922)\napi: thread suffixes parameter to individual join methods (31a9aff)\nchange TimestampType to Timestamp (e0750be)\nclickhouse: disconnect from clickhouse when computing version (11cbf08)\nclickhouse: use a context manager for execution (a471225)\ncombine windows during windowization (7fdd851)\nconform epoch_seconds impls to expression return type (18a70f1)\ncontext-adjustment: pass scope when calling adjust_context in pyspark backend (33aad7b), closes #3108\ndask: fix asof joins for newer version of dask (50711cc)\ndask: workaround dask bug (a0f3bd9)\ndeps: update dependency atpublic to v3 (3fe8f0d)\ndeps: update dependency datafusion to &gt;=0.4,&lt;0.6 (3fb2194)\ndeps: update dependency geoalchemy2 to &gt;=0.6.3,&lt;0.12 (dc3c361)\ndeps: update dependency graphviz to &gt;=0.16,&lt;0.21 (3014445)\nduckdb: add casts to literals to fix binding errors (1977a55), closes #3629\nduckdb: fix array column type discovery on leaf tables and add tests (15e5412)\nduckdb: fix log with base b impl (4920097)\nduckdb: support both 0.3.2 and 0.3.3 (a73ccce)\nenforce the schema’s column names in apply_to (b0f334d)\nexpose ops.IfNull for mysql backend (156c2bd)\nexpr: add more binary operators to char list and implement fallback (b88184c)\nexpr: fix formatting of table info using tabulate (b110636)\nfix float vs real data type detection in sqlalchemy (24e6774)\nfix list_schemas argument (69c1abf)\nfix postgres udfs and re-enable ci tests (7d480d2)\nfix tablecolumn execution for filter following join (064595b)\nformat: remove some newlines from formatted expr repr (ed4fa78)\nhistogram: cross_join needs onclause=True (5d36a58), closes #622\nibis.expr.signature.Parameter is not pickleable (828fd54)\nimplement coalesce properly in the pandas backend (aca5312)\nimplement count on tables for pyspark (7fe5573), closes #2879\ninfer coalesce types when a non-null expression occurs after the first argument (c5f2906)\nmutate: do not lift table column that results from mutate (ba4e5e5)\npandas: disable range windows with order by (e016664)\npandas: don’t reassign the same column to silence SettingWithCopyWarning warning (75dc616)\npandas: implement percent_rank correctly (d8b83e7)\nprevent unintentional cross joins in mutate + filter (83eef99)\npyspark: fix range windows (a6f2aa8)\nregression in Selection.sort_by with resolved_keys (c7a69cd)\nregression in sort_by with resolved_keys (63f1382), closes #3619\nremove broken csv pre_execute (93b662a)\nremove importorskip call for backend tests (2f0bcd8)\nremove incorrect fix for pandas regression (339f544)\nremove passing schema into register_parquet (bdcbb08)\nrepr: add ops.TimeAdd to repr binop lookup table (fd94275)\nrepr: allow ops.TableNode in fmt_value (6f57003)\nreverse the predicate pushdown substitution (f3cd358)\nsort_index to satisfy pandas 1.4.x (6bac0fc)\nsqlalchemy: ensure correlated subqueries FROM clauses are rendered (3175321)\nsqlalchemy: use corresponding_column to prevent spurious cross joins (fdada21)\nsqlalchemy: use replace selectables to prevent semi/anti join cross join (e8a1a71)\nsql: retain column names for named ColumnExprs (f1b4b6e), closes #3754\nsql: walk right join trees and substitute joins with right-side joins with views (0231592)\nstore schema on the pandas backend to allow correct inference (35070be)\n\n\n\nPerformance Improvements\n\ndatatypes: speed up str and hash (262d3d7)\nfast path for simple column selection (d178498)\nir: global equality cache (13c2bb2)\nir: introduce CachedEqMixin to speed up equality checks (b633925)\nrepr: remove full tree repr from rule validator error message (65885ab)\nspeed up attribute access (89d1c05)\nuse assign instead of concat in projections when possible (985c242)\n\n\n\nMiscellaneous Chores\n\ndeps: increase sqlalchemy lower bound to 1.4 (560854a)\ndrop support for Python 3.7 (0afd138)\n\n\n\nCode Refactoring\n\napi: make primitive types more cohesive (71da8f7)\napi: remove distinct ColumnExpr API (3f48cb8)\napi: remove materialize (24285c1)\nbackends: remove the hdf5 backend (ff34f3e)\nbackends: remove the parquet backend (b510473)\nconfig: disable graphviz-repr-in-notebook by default (214ad4e)\nconfig: remove old config code and port to pydantic (4bb96d1)\ndt.UUID inherits from DataType, not String (2ba540d)\nexpr: preserve column ordering in aggregations/mutations (668be0f)\nhdfs: replace HDFS with fsspec (cc6eddb)\nir: make Annotable immutable (1f2b3fa)\nir: make schema annotable (b980903)\nir: remove unused lineage roots and find_nodes functions (d630a77)\nir: simplify expressions by not storing dtype and name (e929f85)\nkudu: remove support for use of kudu through kudu-python (36bd97f)\nmove coercion functions from schema.py to udf (58eea56), closes #3033\nremove blanket call for Expr (3a71116), closes #2258\nremove the csv backend (0e3e02e)\nudf: make coerce functions in ibis.udf.vectorized private (9ba4392)"
  },
  {
    "objectID": "release_notes.html#section-10",
    "href": "release_notes.html#section-10",
    "title": "2.1.0 (2022-01-12)",
    "section": "2.1.1 (2022-01-12)",
    "text": "2.1.1 (2022-01-12)\n\nBug Fixes\n\nsetup.py: set the correct version number for 2.1.0 (f3d267b)"
  },
  {
    "objectID": "release_notes.html#features-9",
    "href": "release_notes.html#features-9",
    "title": "2.1.0 (2022-01-12)",
    "section": "Features",
    "text": "Features\n\nSerialization-deserialization of Node via pickle is now byte compatible between different processes (#2938)\nSupport joining on different columns in ClickHouse backend (#2916)\nSupport summarization of empty data in pandas backend (#2908)\nUnify implementation of fillna and isna in Pyspark backend (#2882)\nSupport binary operation with Timedelta in Pyspark backend (#2873)\nAdd group_concat operation for Clickhouse backend (#2839)\nSupport comparison of ColumnExpr to timestamp literal (#2808)\nMake op schema a cached property (#2805)\nImplement .insert() for SQLAlchemy backends (#2613, #2613)\nInfer categorical and decimal Series to more specific Ibis types in pandas backend (#2792)\nAdd startswith and endswith operations (#2790)\nAllow more flexible return type for UDFs (#2776, #2797)\nImplement Clip in the Pyspark backend (#2779)\nUse ndarray as array representation in pandas backend (#2753)\nSupport Spark filter with window operation (#2687)\nSupport context adjustment for udfs for pandas backend (#2646)\nAdd auth_local_webserver, auth_external_data, and auth_cache parameters to BigQuery connect method. Set auth_local_webserver to use a local server instead of copy-pasting an authorization code. Set auth_external_data to true to request additional scopes required to query Google Drive and Sheets. Set auth_cache to reauth or none to force reauthentication. (#2655)\nAdd bit_and, bit_or, and bit_xor integer column aggregates (BigQuery and MySQL backends) (#2641)\nBackends are defined as entry points (#2379)\nAdd ibis.array for creating array expressions (#2615)\nImplement Not operation in PySpark backend (#2607)\nAdded support for case/when in PySpark backend (#2610)\nAdd support for np.array as literals for backends that already support lists as literals (#2603)"
  },
  {
    "objectID": "release_notes.html#bugs",
    "href": "release_notes.html#bugs",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bugs",
    "text": "Bugs\n\nFix data races in impala connection pool accounting (#2991)\nFix null literal compilation in the Clickhouse backend (#2985)\nFix order of limit and offset parameters in the Clickhouse backend (#2984)\nReplace equals operation for geospatial datatype to geo_equals (#2956)\nFix .drop(fields). The argument can now be either a list of strings or a string. (#2829)\nFix projection on differences and intersections for SQL backends (#2845)\nBackends are loaded in a lazy way, so third-party backends can import Ibis without circular imports (#2827)\nDisable aggregation optimization due to N squared performance (#2830)\nFix .cast() to array outputting list instead of np.array in pandas backend (#2821)\nFix aggregation with mixed reduction datatypes (array + scalar) on Dask backend (#2820)\nFix error when using reduction UDF that returns np.array in a grouped aggregation (#2770)\nFix time context trimming error for multi column udfs in pandas backend (#2712)\nFix error during compilation of range_window in base_sql backends (:issue:2608) (#2710)\nFix wrong row indexing in the result for ‘window after filter’ for timecontext adjustment (#2696)\nFix aggregate exploding the output of Reduction ops that return a list/ndarray (#2702)\nFix issues with context adjustment for filter with PySpark backend (#2693)\nAdd temporary struct col in pyspark backend to ensure that UDFs are executed only once (#2657)\nFix BigQuery connect bug that ignored project ID parameter (#2588)\nFix overwrite logic to account for DestructColumn inside mutate API (#2636)\nFix fusion optimization bug that incorrectly changes operation order (#2635)\nFixes a NPE issue with substr in PySpark backend (#2610)\nFixes binary data type translation into BigQuery bytes data type (#2354)\nMake StructValue picklable (#2577)"
  },
  {
    "objectID": "release_notes.html#support",
    "href": "release_notes.html#support",
    "title": "2.1.0 (2022-01-12)",
    "section": "Support",
    "text": "Support\n\nImprovement of the backend API. The former Client subclasses have been replaced by a Backend class that must subclass ibis.backends.base.BaseBackend. The BaseBackend class contains abstract methods for the minimum subset of methods that backends must implement, and their signatures have been standardized across backends. The Ibis compiler has been refactored, and backends don’t need to implement all compiler classes anymore if the default works for them. Only a subclass of ibis.backends.base.sql.compiler.Compiler is now required. Backends now need to register themselves as entry points. (#2678)\nDeprecate exists_table(table) in favor of table in list_tables() (#2905)\nRemove handwritten type parser; parsing errors that were previously IbisTypeError are now parsy.ParseError. parsy is now a hard requirement. (#2977)\nMethods current_database and list_databases raise an exception for backends that do not support databases (#2962)\nMethod set_database has been deprecated, in favor of creating a new connection to a different database (#2913)\nRemoved log method of clients, in favor of verbose_log option (#2914)\nOutput of Client.version returned as a string, instead of a setuptools Version (#2883)\nDeprecated list_schemas in SQLAlchemy backends in favor of list_databases (#2862)\nDeprecated ibis.&lt;backend&gt;.verify() in favor of capturing exception in ibis.&lt;backend&gt;.compile() (#2865)\nSimplification of data fetching. Backends don’t need to implement Query anymore (#2789)\nMove BigQuery backend to a separate repository &lt;https://github.com/ibis-project/ibis-bigquery&gt;_. The backend will be released separately, use pip install ibis-bigquery or conda install ibis-bigquery to install it, and then use as before. (#2665)\nSupporting SQLAlchemy 1.4, and requiring minimum 1.3 (#2689)\nNamespace time_col config, fix type check for trim_with_timecontext for pandas window execution (#2680)\nRemove deprecated ibis.HDFS, ibis.WebHDFS and ibis.hdfs_connect (#2505)"
  },
  {
    "objectID": "release_notes.html#features-10",
    "href": "release_notes.html#features-10",
    "title": "2.1.0 (2022-01-12)",
    "section": "Features",
    "text": "Features\n\nAdd Struct.from_dict (#2514)\nAdd hash and hashbytes support for BigQuery backend (#2310)\nSupport reduction UDF without groupby to return multiple columns for pandas backend (#2511)\nSupport analytic and reduction UDF to return multiple columns for pandas backend (#2487)\nSupport elementwise UDF to return multiple columns for pandas and PySpark backend (#2473)\nFEAT: Support Ibis interval for window in pyspark backend (#2409)\nUse Scope class for scope in pyspark backend (#2402)\nAdd PySpark support for ReductionVectorizedUDF (#2366)\nAdd time context in scope in execution for pandas backend (#2306)\nAdd start_point and end_point to PostGIS backend. (#2081)\nAdd set difference to general ibis api (#2347)\nAdd rowid expression, supported by SQLite and OmniSciDB (#2251)\nAdd intersection to general ibis api (#2230)\nAdd application_name argument to ibis.bigquery.connect to allow attributing Google API requests to projects that use Ibis. (#2303)\nAdd support for casting category dtype in pandas backend (#2285)\nAdd support for Union in the PySpark backend (#2270)\nAdd support for implementign custom window object for pandas backend (#2260)\nImplement two level dispatcher for execute_node (#2246)\nAdd ibis.pandas.trace module to log time and call stack information. (#2233)\nValidate that the output type of a UDF is a single element (#2198)\nZeroIfNull and NullIfZero implementation for OmniSciDB (#2186)\nIsNan implementation for OmniSciDB (#2093)\n[OmnisciDB] Support add_columns and drop_columns for OmnisciDB table (#2094)\nCreate ExtractQuarter operation and add its support to Clickhouse, CSV, Impala, MySQL, OmniSciDB, pandas, Parquet, PostgreSQL, PySpark, SQLite and Spark (#2175)\nAdd translation rules for isnull() and notnull() for pyspark backend (#2126)\nAdd window operations support to SQLite (#2232)\nImplement read_csv for omniscidb backend (#2062)\n[OmniSciDB] Add support to week extraction (#2171)\nDate, DateDiff and TimestampDiff implementations for OmniSciDB (#2097)\nCreate ExtractWeekOfYear operation and add its support to Clickhouse, CSV, MySQL, pandas, Parquet, PostgreSQL, PySpark and Spark (#2177)\nAdd initial support for ibis.random function (#2060)\nAdded epoch_seconds extraction operation to Clickhouse, CSV, Impala, MySQL, OmniSciDB, pandas, Parquet, PostgreSQL, PySpark, SQLite, Spark and BigQuery :issue:2273 (#2178)\n[OmniSciDB] Add “method” parameter to load_data (#2165)\nAdd non-nullable info to schema output (#2117)\nfillna and nullif implementations for OmnisciDB (#2083)\nAdd load_data to sqlalchemy’s backends and fix database parameter for load/create/drop when database parameter is the same than the current database (#1981)\n[OmniSciDB] Add support for within, d_fully_within and point (#2125)\nOmniSciDB - Refactor DDL and Client; Add temporary parameter to create_table and “force” parameter to drop_view (#2086)\nCreate ExtractDayOfYear operation and add its support to Clickhouse, CSV, MySQL, OmniSciDB, pandas, Parquet, PostgreSQL, PySpark, SQLite and Spark (#2173)\nImplementations of Log Log2 Log10 for OmniSciDB backend (#2095)"
  },
  {
    "objectID": "release_notes.html#bugs-1",
    "href": "release_notes.html#bugs-1",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bugs",
    "text": "Bugs\n\nTable expressions do not recognize inet datatype (Postgres backend) (#2462)\nTable expressions do not recognize macaddr datatype (Postgres backend) (#2461)\nFix aggcontext.Summarize not always producing scalar (pandas backend) (#2410)\nFix same window op with different window size on table lead to incorrect results for pyspark backend (#2414)\nFix same column with multiple aliases not showing properly in repr (#2229)\nFix reduction UDFs over ungrouped, bounded windows on pandas backend (#2395)\nFEAT: Support rolling window UDF with non numeric inputs for pandas backend. (#2386)\nFix scope get to use hashmap lookup instead of list lookup (#2386)\nFix equality behavior for Literal ops (#2387)\nFix analytic ops over ungrouped and unordered windows on pandas backend (#2376)\nFix the covariance operator in the BigQuery backend. (#2367)\nUpdate impala kerberos dependencies (#2342)\nAdded verbose logging to SQL backends (#1320)\nFix issue with sql_validate call to OmnisciDB. (#2256)\nAdd missing float types to pandas backend (#2237)\nAllow group_by and order_by as window operation input in pandas backend (#2252)\nFix PySpark compiler error when elementwise UDF output_type is Decimal or Timestamp (#2223)\nFix interactive mode returning a expression instead of the value when used in Jupyter (#2157)\nFix PySpark error when doing alias after selection (#2127)\nFix millisecond issue for OmniSciDB :issue:2167, MySQL :issue:2169, PostgreSQL :issue:2166, pandas :issue:2168, BigQuery :issue:2273 backends (#2170)\n[OmniSciDB] Fix TopK when used as filter (#2134)"
  },
  {
    "objectID": "release_notes.html#support-1",
    "href": "release_notes.html#support-1",
    "title": "2.1.0 (2022-01-12)",
    "section": "Support",
    "text": "Support\n\nMove ibis.HDFS, ibis.WebHDFS and ibis.hdfs_connect to ibis.impala.* (#2497)\nDrop support to Python 3.6 (#2288)\nSimplifying tests directories structure (#2351)\nUpdate google-cloud-bigquery dependency minimum version to 1.12.0 (#2304)\nRemove “experimental” mentions for OmniSciDB and pandas backends (#2234)\nUse an OmniSciDB image stable on CI (#2244)\nAdded fragment_size to table creation for OmniSciDB (#2107)\nAdded round() support for OmniSciDB (#2096)\nEnabled cumulative ops support for OmniSciDB (#2113)"
  },
  {
    "objectID": "release_notes.html#features-11",
    "href": "release_notes.html#features-11",
    "title": "2.1.0 (2022-01-12)",
    "section": "Features",
    "text": "Features\n\nImprove many arguments UDF performance in pandas backend. (#2071)\nAdd DenseRank, RowNumber, MinRank, Count, PercentRank/CumeDist window operations to OmniSciDB (#1976)\nIntroduce a top level vectorized UDF module (experimental). Implement element-wise UDF for pandas and PySpark backend. (#2047)\nAdd support for multi arguments window UDAF for the pandas backend (#2035)\nClean up window translation logic in pyspark backend (#2004)\nAdd docstring check to CI for an initial subset files (#1996)\nPyspark backend bounded windows (#2001)\nAdd more POSTGIS operations (#1987)\nSQLAlchemy Default precision and scale to decimal types for PostgreSQL and MySQL (#1969)\nAdd support for array operations in PySpark backend (#1983)\nImplement sort, if_null, null_if and notin for PySpark backend (#1978)\nAdd support for date/time operations in PySpark backend (#1974)\nAdd support for params, query_schema, and sql in PySpark backend (#1973)\nImplement join for PySpark backend (#1967)\nValidate AsOfJoin tolerance and attempt interval unit conversion (#1952)\nfilter for PySpark backend (#1943)\nwindow operations for pyspark backend (#1945)\nImplement IntervalSub for pandas backend (#1951)\nPySpark backend string and column ops (#1942)\nPySpark backend (#1913)\nDDL support for Spark backend (#1908)\nSupport timezone aware arrow timestamps (#1923)\nAdd shapely geometries as input for literals (#1860)\nAdd geopandas as output for omniscidb (#1858)\nSpark UDFs (#1885)\nAdd support for Postgres UDFs (#1871)\nSpark tests (#1830)\nSpark client (#1807)\nUse pandas rolling apply to implement rows_with_max_lookback (#1868)"
  },
  {
    "objectID": "release_notes.html#bugs-2",
    "href": "release_notes.html#bugs-2",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bugs",
    "text": "Bugs\n\nPin “clickhouse-driver” to “&gt;=0.1.3” (#2089)\nFix load data stage for Linux CI (#2069)\nFix datamgr.py fail if IBIS_TEST_OMNISCIDB_DATABASE=omnisci (#2057)\nChange pymapd connection parameter from “session_id” to “sessionid” (#2041)\nFix pandas backend to treat trailing_window preceding arg as window bound rather than window size (e.g. preceding=0 now indicates current row rather than window size 0) (#2009)\nFix handling of Array types in Postgres UDF (#2015)\nFix pydocstyle config (#2010)\nPinning clickhouse-driver&lt;0.1.2 (#2006)\nFix CI log for database (#1984)\nFixes explain operation (#1933)\nFix incorrect assumptions about attached SQLite databases (#1937)\nUpgrade to JDK11 (#1938)\nsql method doesn’t work when the query uses LIMIT clause (#1903)\nFix union implementation (#1910)\nFix failing com imports on master (#1912)\nOmniSci/MapD - Fix reduction for bool (#1901)\nPass scope to grouping execution in the pandas backend (#1899)\nFix various Spark backend issues (#1888)\nMake Nodes enforce the proper signature (#1891)\nFix according to bug in pd.to_datetime when passing the unit flag (#1893)\nFix small formatting buglet in PR merge tool (#1883)\nFix the case where we do not have an index when using preceding with intervals (#1876)\nFixed issues with geo data (#1872)\nRemove -x from pytest call in linux CI (#1869)\nFix return type of Struct.from_tuples (#1867)"
  },
  {
    "objectID": "release_notes.html#support-2",
    "href": "release_notes.html#support-2",
    "title": "2.1.0 (2022-01-12)",
    "section": "Support",
    "text": "Support\n\nAdd support to Python 3.8 (#2066)\nPin back version of isort (#2079)\nUse user-defined port variables for Omnisci and PostgreSQL tests (#2082)\nChange omniscidb image tag from v5.0.0 to v5.1.0 on docker-compose recipe (#2077)\n[Omnisci] The same SRIDs for test_geo_spatial_binops (#2051)\nUnpin rtree version (#2078)\nLink pandas issues with xfail tests in pandas/tests/test_udf.py (#2074)\nDisable Postgres tests on Windows CI. (#2075)\nuse conda for installation black and isort tools (#2068)\nCI: Fix CI builds related to new pandas 1.0 compatibility (#2061)\nFix data map for int8 on OmniSciDB backend (#2056)\nAdd possibility to run tests for separate backend via make test BACKENDS=[YOUR BACKEND] (#2052)\nFix “cudf” import on OmniSciDB backend (#2055)\nCI: Drop table only if it exists (OmniSciDB) (#2050)\nAdd initial documentation for OmniSciDB, MySQL, PySpark and SparkSQL backends, add initial documentation for geospatial methods and add links to Ibis wiki page (#2034)\nImplement covariance for bigquery backend (#2044)\nAdd Spark to supported backends list (#2046)\nPing dependency of rtree to fix CI failure (#2043)\nDrop support for Python 3.5 (#2037)\nHTML escape column names and types in png repr. (#2023)\nAdd geospatial tutorial notebook (#1991)\nChange omniscidb image tag from v4.7.0 to v5.0.0 on docker-compose recipe (#2031)\nPin “semantic_version” to “&lt;2.7” in the docs build CI, fix “builddoc” and “doc” section inside “Makefile” and skip mysql tzinfo on CI to allow to run MySQL using docker container on a hard disk drive. (#2030)\nFixed impala start up issues (#2012)\ncache all ops in translate() (#1999)\nAdd black step to CI (#1988)\nJson UUID any (#1962)\nAdd log for database services (#1982)\nFix BigQuery backend fixture so batting and awards_players fixture re… (#1972)\nDisable BigQuery explicitly in all/test_join.py (#1971)\nRe-formatting all files using pre-commit hook (#1963)\nDisable codecov report upload during CI builds (#1961)\nDeveloper doc enhancements (#1960)\nMissing geospatial ops for OmniSciDB (#1958)\nRemove pandas deprecation warnings (#1950)\nAdd developer docs to get docker setup (#1948)\nMore informative IntegrityError on duplicate columns (#1949)\nImprove geospatial literals and smoke tests (#1928)\nPostGIS enhancements (#1925)\nRename mapd to omniscidb backend (#1866)\nFix failing BigQuery tests (#1926)\nAdded missing null literal op (#1917)\nUpdate link to Presto website (#1895)\nRemoving linting from windows (#1896)\nFix link to NUMFOCUS CoC (#1884)\nAdded CoC section (#1882)\nRemove pandas exception for rows_with_max_lookback (#1859)\nMove CI pipelines to Azure (#1856)"
  },
  {
    "objectID": "release_notes.html#features-12",
    "href": "release_notes.html#features-12",
    "title": "2.1.0 (2022-01-12)",
    "section": "Features",
    "text": "Features\n\nAdd new geospatial functions to OmniSciDB backend (#1836)\nallow pandas timedelta in rows_with_max_lookback (#1838)\nAccept rows-with-max-lookback as preceding parameter (#1825)\nPostGIS support (#1787)"
  },
  {
    "objectID": "release_notes.html#bugs-3",
    "href": "release_notes.html#bugs-3",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bugs",
    "text": "Bugs\n\nFix call to psql causing failing CI (#1855)\nFix nested array literal repr (#1851)\nFix repr of empty schema (#1850)\nAdd max_lookback to window replace and combine functions (#1843)\nPartially revert #1758 (#1837)"
  },
  {
    "objectID": "release_notes.html#support-3",
    "href": "release_notes.html#support-3",
    "title": "2.1.0 (2022-01-12)",
    "section": "Support",
    "text": "Support\n\nSkip SQLAlchemy backend tests in connect method in backends.py (#1847)\nValidate order_by when using rows_with_max_lookback window (#1848)\nGenerate release notes from commits (#1845)\nRaise exception on backends where rows_with_max_lookback can’t be implemented (#1844)\nTighter version spec for pytest (#1840)\nAllow passing a branch to ci/feedstock.py (#1826)"
  },
  {
    "objectID": "release_notes.html#features-13",
    "href": "release_notes.html#features-13",
    "title": "2.1.0 (2022-01-12)",
    "section": "Features",
    "text": "Features\n\nConslidate trailing window functions (#1809)\nCall to_interval when casting integers to intervals (#1766)\nAdd session feature to mapd client API (#1796)\nAdd min periods parameter to Window (#1792)\nAllow strings for types in pandas UDFs (#1785)\nAdd missing date operations and struct field operation for the pandas backend (#1790)\nAdd window operations to the OmniSci backend (#1771)\nReimplement the pandas backend using topological sort (#1758)\nAdd marker for xfailing specific backends (#1778)\nEnable window function tests where possible (#1777)\nis_computable_arg dispatcher (#1743)\nAdded float32 and geospatial types for create table from schema (#1753)"
  },
  {
    "objectID": "release_notes.html#bugs-4",
    "href": "release_notes.html#bugs-4",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bugs",
    "text": "Bugs\n\nFix group_concat test and implementations (#1819)\nFix failing strftime tests on Python 3.7 (#1818)\nRemove unnecessary (and erroneous in some cases) frame clauses (#1757)\nChained mutate operations are buggy (#1799)\nAllow projections from joins to attempt fusion (#1783)\nFix Python 3.5 dependency versions (#1798)\nFix compatibility and bugs associated with pandas toposort reimplementation (#1789)\nFix outer_join generating LEFT join instead of FULL OUTER (#1772)\nNullIf should enforce that its arguments are castable to a common type (#1782)\nFix conda create command in documentation (#1775)\nFix preceding and following with None (#1765)\nPostgreSQL interval type not recognized (#1661)"
  },
  {
    "objectID": "release_notes.html#support-4",
    "href": "release_notes.html#support-4",
    "title": "2.1.0 (2022-01-12)",
    "section": "Support",
    "text": "Support\n\nRemove decorator hacks and add custom markers (#1820)\nAdd development deps to setup.py (#1814)\nFix design and developer docs (#1805)\nPin sphinx version to 2.0.1 (#1810)\nAdd pep8speaks integration (#1793)\nFix typo in UDF signature specification (#1821)\nClean up most xpassing tests (#1779)\nUpdate omnisci container version (#1781)\nConstrain PyMapD version to get passing builds (#1776)\nRemove warnings and clean up some docstrings (#1763)\nAdd StringToTimestamp as unsupported (#1638)\nAdd isort pre-commit hooks (#1759)\nAdd Python 3.5 testing back to CI (#1750)\nRe-enable CI for building step (#1700)\nUpdate README reference to MapD to say OmniSci (#1749)"
  },
  {
    "objectID": "release_notes.html#features-14",
    "href": "release_notes.html#features-14",
    "title": "2.1.0 (2022-01-12)",
    "section": "Features",
    "text": "Features\n\nAdd black as a pre-commit hook (#1735)\nAdd support for the arbitrary aggregate in the mapd backend (#1680)\nAdd SQL method for the MapD backend (#1731)\nClean up merge PR script and use the actual merge feature of GitHub (#1744)\nAdd cross join to the pandas backend (#1723)\nImplement default handler for multiple client pre_execute (#1727)\nImplement BigQuery auth using pydata_google_auth (#1728)\nTimestamp literal accepts a timezone parameter (#1712)\nRemove support for passing integers to ibis.timestamp (#1725)\nAdd find_nodes to lineage (#1704)\nRemove a bunch of deprecated APIs and clean up warnings (#1714)\nImplement table distinct for the pandas backend (#1716)\nImplement geospatial functions for MapD (#1678)\nImplement geospatial types for MapD (#1666)\nAdd pre commit hook (#1685)\nGetting started with mapd, mysql and pandas (#1686)\nSupport column names with special characters in mapd (#1675)\nAllow operations to hide arguments from display (#1669)\nRemove implicit ordering requirements in the PostgreSQL backend (#1636)\nAdd cross join operator to MapD (#1655)\nFix UDF bugs and add support for non-aggregate analytic functions (#1637)\nSupport string slicing with other expressions (#1627)\nPublish the ibis roadmap (#1618)\nImplement approx_median in BigQuery (#1604)\nMake ibis node instances hashable (#1611)\nAdd range_window and trailing_range_window to docs (#1608)"
  },
  {
    "objectID": "release_notes.html#bugs-5",
    "href": "release_notes.html#bugs-5",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bugs",
    "text": "Bugs\n\nMake dev/merge-pr.py script handle PR branches (#1745)\nFix NULLIF implementation for the pandas backend (#1742)\nFix casting to float in the MapD backend (#1737)\nFix testing for BigQuery after auth flow update (#1741)\nFix skipping for new BigQuery auth flow (#1738)\nFix bug in TableExpr.drop (#1732)\nFilter the raw warning from newer pandas to support older pandas (#1729)\nFix BigQuery credentials link (#1706)\nAdd Union as an unsuppoted operation for MapD (#1639)\nFix visualizing an ibis expression when showing a selection after a table join (#1705)\nFix MapD exception for toDateTime (#1659)\nUse == to compare strings (#1701)\nResolves joining with different column names (#1647)\nFix map get with compatible types (#1643)\nFixed where operator for MapD (#1653)\nRemove parameters from mapd (#1648)\nMake sure we cast when NULL is else in CASE expressions (#1651)\nFix equality (#1600)"
  },
  {
    "objectID": "release_notes.html#support-5",
    "href": "release_notes.html#support-5",
    "title": "2.1.0 (2022-01-12)",
    "section": "Support",
    "text": "Support\n\nDo not build universal wheels (#1748)\nRemove tag prefix from versioneer (#1747)\nUse releases to manage documentation (#1746)\nUse cudf instead of pygdf (#1694)\nFix multiple CI issues (#1696)\nUpdate mapd ci to v4.4.1 (#1681)\nEnabled mysql CI on azure pipelines (#1672)\nRemove support for Python 2 (#1670)\nFix flake8 and many other warnings (#1667)\nUpdate README.md for impala and kudu (#1664)\nRemove defaults as a channel from azure pipelines (#1660)\nFixes a very typo in the pandas/core.py docstring (#1658)\nUnpin clickhouse-driver version (#1657)\nAdd test for reduction returning lists (#1650)\nFix Azure VM image name (#1646)\nUpdated MapD server-CI (#1641)\nAdd TableExpr.drop to API documentation (#1645)\nFix Azure deployment step (#1642)\nSet up CI with Azure Pipelines (#1640)\nFix conda builds (#1609)"
  },
  {
    "objectID": "release_notes.html#new-features",
    "href": "release_notes.html#new-features",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nAllow keyword arguments in Node subclasses (#968)\nSplat args into Node subclasses instead of requiring a list (#969)\nAdd support for UNION in the BigQuery backend (#1408, #1409)\nSupport for writing UDFs in BigQuery (#1377). See the BigQuery UDF docs for more details.\nSupport for cross-project expressions in the BigQuery backend. (#1427, #1428)\nAdd strftime and to_timestamp support for BigQuery (#1422, #1410)\nRequire google-cloud-bigquery &gt;=1.0 (#1424)\nLimited support for interval arithmetic in the pandas backend (#1407)\nSupport for subclassing TableExpr (#1439)\nFill out pandas backend operations (#1423)\nAdd common DDL APIs to the pandas backend (#1464)\nImplement the sql method for BigQuery (#1463)\nAdd to_timestamp for BigQuery (#1455)\nAdd the mapd backend (#1419)\nImplement range windows (#1349)\nSupport for map types in the pandas backend (#1498)\nAdd mean and sum for boolean types in BigQuery (#1516)\nAll recent versions of SQLAlchemy are now supported (#1384)\nAdd support for NUMERIC types in the BigQuery backend (#1534)\nSpeed up grouped and rolling operations in the pandas backend (#1549)\nImplement TimestampNow for BigQuery and pandas (#1575)"
  },
  {
    "objectID": "release_notes.html#bug-fixes-12",
    "href": "release_notes.html#bug-fixes-12",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bug Fixes",
    "text": "Bug Fixes\n\nNullable property is now propagated through value types (#1289)\nImplicit casting between signed and unsigned integers checks boundaries\nFix precedence of case statement (#1412)\nFix handling of large timestamps (#1440)\nFix identical_to precedence (#1458)\npandas 0.23 compatibility (#1458)\nPreserve timezones in timestamp-typed literals (#1459)\nFix incorrect topological ordering of UNION expressions (#1501)\nFix projection fusion bug when attempting to fuse columns of the same name (#1496)\nFix output type for some decimal operations (#1541)"
  },
  {
    "objectID": "release_notes.html#api-changes",
    "href": "release_notes.html#api-changes",
    "title": "2.1.0 (2022-01-12)",
    "section": "API Changes",
    "text": "API Changes\n\nThe previous, private rules API has been rewritten (#1366)\nDefining input arguments for operations happens in a more readable fashion instead of the previous input_type list.\nRemoved support for async query execution (only Impala supported)\nRemove support for Python 3.4 (#1326)\nBigQuery division defaults to using IEEE_DIVIDE (#1390)\nAdd tolerance parameter to asof_join (#1443)"
  },
  {
    "objectID": "release_notes.html#new-backends",
    "href": "release_notes.html#new-backends",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Backends",
    "text": "New Backends\n\nFile Support for CSV & HDF5 (#1165, #1194)\nFile Support for Parquet Format (#1175, #1194)\nExperimental support for MySQL thanks to @kszucs (#1224)"
  },
  {
    "objectID": "release_notes.html#new-features-1",
    "href": "release_notes.html#new-features-1",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nSupport for Unsigned Integer Types (#1194)\nSupport for Interval types and expressions with support for execution on the Impala and Clickhouse backends (#1243)\nIsnan, isinf operations for float and double values (#1261)\nSupport for an interval with a quarter period (#1259)\nibis.pandas.from_dataframe convenience function (#1155)\nRemove the restriction on ROW_NUMBER() requiring it to have an ORDER BY clause (#1371)\nAdd .get() operation on a Map type (#1376)\nAllow visualization of custom defined expressions\nAdd experimental support for pandas UDFs/UDAFs (#1277)\nFunctions can be used as groupby keys (#1214, #1215)\nGeneralize the use of the where parameter to reduction operations (#1220)\nSupport for interval operations thanks to @kszucs (#1243, #1260, #1249)\nSupport for the PARTITIONTIME column in the BigQuery backend (#1322)\nAdd arbitrary() method for selecting the first non null value in a column (#1230, #1309)\nWindowed MultiQuantile operation in the pandas backend thanks to @DiegoAlbertoTorres (#1343)\nRules for validating table expressions thanks to @DiegoAlbertoTorres (#1298)\nComplete end-to-end testing framework for all supported backends (#1256)\ncontains/not contains now supported in the pandas backend (#1210, #1211)\nCI builds are now reproducible locally thanks to @kszucs (#1121, #1237, #1255, #1311)\nisnan/isinf operations thanks to @kszucs (#1261)\nFramework for generalized dtype and schema inference, and implicit casting thanks to @kszucs (#1221, #1269)\nGeneric utilities for expression traversal thanks to @kszucs (#1336)\nday_of_week API (#306, #1047)\nDesign documentation for ibis (#1351)"
  },
  {
    "objectID": "release_notes.html#bug-fixes-13",
    "href": "release_notes.html#bug-fixes-13",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bug Fixes",
    "text": "Bug Fixes\n\nUnbound parameters were failing in the simple case of a ibis.expr.types.TableExpr.mutate call with no operation (#1378)\nFix parameterized subqueries (#1300, #1331, #1303, #1378)\nFix subquery extraction, which wasn’t happening in topological order (#1342)\nFix parenthesization if isnull (#1307)\nCalling drop after mutate did not work (#1296, #1299)\nSQLAlchemy backends were missing an implementation of ibis.expr.operations.NotContains.\nSupport REGEX_EXTRACT in PostgreSQL 10 (#1276, #1278)"
  },
  {
    "objectID": "release_notes.html#api-changes-1",
    "href": "release_notes.html#api-changes-1",
    "title": "2.1.0 (2022-01-12)",
    "section": "API Changes",
    "text": "API Changes\n\nFixing #1378 required the removal of the name parameter to the ibis.param function. Use the ibis.expr.types.Expr.name method instead."
  },
  {
    "objectID": "release_notes.html#new-backends-1",
    "href": "release_notes.html#new-backends-1",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Backends",
    "text": "New Backends\n\nBigQuery backend (#1170), thanks to @tsdlovell.\nClickhouse backend (#1127), thanks to @kszucs."
  },
  {
    "objectID": "release_notes.html#new-features-2",
    "href": "release_notes.html#new-features-2",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nAdd support for Binary data type (#1183)\nAllow users of the BigQuery client to define their own API proxy classes (#1188)\nAdd support for HAVING in the pandas backend (#1182)\nAdd struct field tab completion (#1178)\nAdd expressions for Map/Struct types and columns (#1166)\nSupport Table.asof_join (#1162)\nAllow right side of arithmetic operations to take over (#1150)\nAdd a data_preload step in pandas backend (#1142)\nexpressions in join predicates in the pandas backend (#1138)\nScalar parameters (#1075)\nLimited window function support for pandas (#1083)\nImplement Time datatype (#1105)\nImplement array ops for pandas (#1100)\nsupport for passing multiple quantiles in .quantile() (#1094)\nsupport for clip and quantile ops on DoubleColumns (#1090)\nEnable unary math operations for pandas, sqlite (#1071)\nEnable casting from strings to temporal types (#1076)\nAllow selection of whole tables in pandas joins (#1072)\nImplement comparison for string vs date and timestamp types (#1065)\nImplement isnull and notnull for pandas (#1066)\nAllow like operation to accept a list of conditions to match (#1061)\nAdd a pre_execute step in pandas backend (#1189)"
  },
  {
    "objectID": "release_notes.html#bug-fixes-14",
    "href": "release_notes.html#bug-fixes-14",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bug Fixes",
    "text": "Bug Fixes\n\nRemove global expression caching to ensure repeatable code generation (#1179, #1181)\nFix ORDER BY generation without a GROUP BY (#1180, #1181)\nEnsure that ~ibis.expr.datatypes.DataType and subclasses hash properly (#1172)\nEnsure that the pandas backend can deal with unary operations in groupby\n(#1182)\nIncorrect impala code generated for NOT with complex argument (#1176)\nBUG/CLN: Fix predicates on Selections on Joins (#1149)\nDon't use SET LOCAL to allow redshift to work (#1163)\nAllow empty arrays as arguments (#1154)\nFix column renaming in groupby keys (#1151)\nEnsure that we only cast if timezone is not None (#1147)\nFix location of conftest.py (#1107)\nTST/Make sure we drop tables during postgres testing (#1101)\nFix misleading join error message (#1086)\nBUG/TST: Make hdfs an optional dependency (#1082)\nMemoization should include expression name where available (#1080)"
  },
  {
    "objectID": "release_notes.html#performance-enhancements",
    "href": "release_notes.html#performance-enhancements",
    "title": "2.1.0 (2022-01-12)",
    "section": "Performance Enhancements",
    "text": "Performance Enhancements\n\nSpeed up imports (#1074)\nFix execution perf of groupby and selection (#1073)\nUse normalize for casting to dates in pandas (#1070)\nSpeed up pandas groupby (#1067)"
  },
  {
    "objectID": "release_notes.html#contributors",
    "href": "release_notes.html#contributors",
    "title": "2.1.0 (2022-01-12)",
    "section": "Contributors",
    "text": "Contributors\nThe following people contributed to the 0.12.0 release :\n$ git shortlog -sn --no-merges v0.11.2..v0.12.0\n63  Phillip Cloud\n 8  Jeff Reback\n 2  Krisztián Szűcs\n 2  Tory Haavik\n 1  Anirudh\n 1  Szucs Krisztian\n 1  dlovell\n 1  kwangin"
  },
  {
    "objectID": "release_notes.html#new-features-3",
    "href": "release_notes.html#new-features-3",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nExperimental pandas backend to allow execution of ibis expression against pandas DataFrames\nGraphviz visualization of ibis expressions. Implements _repr_png_ for Jupyter Notebook functionality\nAbility to create a partitioned table from an ibis expression\nSupport for missing operations in the SQLite backend: sqrt, power, variance, and standard deviation, regular expression functions, and missing power support for PostgreSQL\nSupport for schemas inside databases with the PostgreSQL backend\nAppveyor testing on core ibis across all supported Python versions\nAdd year/month/day methods to date types\nAbility to sort, group by and project columns according to positional index rather than only by name\nAdded a type parameter to ibis.literal to allow user specification of literal types"
  },
  {
    "objectID": "release_notes.html#bug-fixes-15",
    "href": "release_notes.html#bug-fixes-15",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bug Fixes",
    "text": "Bug Fixes\n\nFix broken conda recipe\nFix incorrectly typed fillna operation\nFix postgres boolean summary operations\nFix kudu support to reflect client API Changes\nFix equality of nested types and construction of nested types when the value type is specified as a string"
  },
  {
    "objectID": "release_notes.html#api-changes-2",
    "href": "release_notes.html#api-changes-2",
    "title": "2.1.0 (2022-01-12)",
    "section": "API Changes",
    "text": "API Changes\n\nDeprecate passing integer values to the ibis.timestamp literal constructor, this will be removed in 0.12.0\nAdded the admin_timeout parameter to the kudu client connect function"
  },
  {
    "objectID": "release_notes.html#contributors-1",
    "href": "release_notes.html#contributors-1",
    "title": "2.1.0 (2022-01-12)",
    "section": "Contributors",
    "text": "Contributors\n$ git shortlog --summary --numbered v0.10.0..v0.11.0\n\n  58 Phillip Cloud\n   1 Greg Rahn\n   1 Marius van Niekerk\n   1 Tarun Gogineni\n   1 Wes McKinney"
  },
  {
    "objectID": "release_notes.html#new-features-4",
    "href": "release_notes.html#new-features-4",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nInitial PostgreSQL backend contributed by Phillip Cloud.\nAdd groupby as an alias for group_by to table expressions"
  },
  {
    "objectID": "release_notes.html#bug-fixes-16",
    "href": "release_notes.html#bug-fixes-16",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bug Fixes",
    "text": "Bug Fixes\n\nFix an expression error when filtering based on a new field\nFix Impala's SQL compilation of using OR with compound filters\nVarious fixes with the having(...) function in grouped table expressions\nFix CTE (WITH) extraction inside UNION ALL expressions.\nFix ImportError on Python 2 when mock library not installed"
  },
  {
    "objectID": "release_notes.html#api-changes-3",
    "href": "release_notes.html#api-changes-3",
    "title": "2.1.0 (2022-01-12)",
    "section": "API Changes",
    "text": "API Changes\n\nThe deprecated ibis.impala_connect and ibis.make_client APIs have been removed"
  },
  {
    "objectID": "release_notes.html#new-features-5",
    "href": "release_notes.html#new-features-5",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nApache Kudu (incubating) integration for Impala users. Will add some documentation here when possible.\nAdd use_https option to ibis.hdfs_connect for WebHDFS connections in secure (Kerberized) clusters without SSL enabled.\nCorrectly compile aggregate expressions involving multiple subqueries.\n\nTo explain this last point in more detail, suppose you had:\ntable = ibis.table([('flag', 'string'),\n                    ('value', 'double')],\n                   'tbl')\n\nflagged = table[table.flag == '1']\nunflagged = table[table.flag == '0']\n\nfv = flagged.value\nuv = unflagged.value\n\nexpr = (fv.mean() / fv.sum()) - (uv.mean() / uv.sum())\nThe last expression now generates the correct Impala or SQLite SQL:\nSELECT t0.`tmp` - t1.`tmp` AS `tmp`\nFROM (\n  SELECT avg(`value`) / sum(`value`) AS `tmp`\n  FROM tbl\n  WHERE `flag` = '1'\n) t0\n  CROSS JOIN (\n    SELECT avg(`value`) / sum(`value`) AS `tmp`\n    FROM tbl\n    WHERE `flag` = '0'\n  ) t1"
  },
  {
    "objectID": "release_notes.html#bug-fixes-17",
    "href": "release_notes.html#bug-fixes-17",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bug Fixes",
    "text": "Bug Fixes\n\nCHAR(n) and VARCHAR(n) Impala types now correctly map to Ibis string expressions\nFix inappropriate projection-join-filter expression rewrites resulting in incorrect generated SQL.\nImpalaClient.create_table correctly passes STORED AS PARQUET for format='parquet'.\nFixed several issues with Ibis dependencies (impyla, thriftpy, sasl, thrift_sasl), especially for secure clusters. Upgrading will pull in these new dependencies.\nDo not fail in ibis.impala.connect when trying to create the temporary Ibis database if no HDFS connection passed.\nFix join predicate evaluation bug when column names overlap with table attributes.\nFix handling of fully-materialized joins (aka select * joins) in SQLAlchemy / SQLite."
  },
  {
    "objectID": "release_notes.html#contributors-2",
    "href": "release_notes.html#contributors-2",
    "title": "2.1.0 (2022-01-12)",
    "section": "Contributors",
    "text": "Contributors\nThank you to all who contributed patches to this release.\n$ git log v0.6.0..v0.7.0 --pretty=format:%aN | sort | uniq -c | sort -rn\n    21 Wes McKinney\n     1 Uri Laserson\n     1 Kristopher Overholt"
  },
  {
    "objectID": "release_notes.html#new-features-6",
    "href": "release_notes.html#new-features-6",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nNew integrated Impala functionality. See Ibis for Impala Users for more details on these things.\n\nImproved Impala-pandas integration. Create tables or insert into existing tables from pandas DataFrame objects.\nPartitioned table metadata management API. Add, drop, alter, and insert into table partitions.\nAdd is_partitioned property to ImpalaTable.\nAdded support for LOAD DATA DDL using the load_data function, also supporting partitioned tables.\nModify table metadata (location, format, SerDe properties etc.) using ImpalaTable.alter\nInterrupting Impala expression execution with Control-C will attempt to cancel the running query with the server.\nSet the compression codec (e.g. snappy) used with ImpalaClient.set_compression_codec.\nGet and set query options for a client session with ImpalaClient.get_options and ImpalaClient.set_options.\nAdd ImpalaTable.metadata method that parses the output of the DESCRIBE FORMATTED DDL to simplify table metadata inspection.\nAdd ImpalaTable.stats and ImpalaTable.column_stats to see computed table and partition statistics.\nAdd CHAR and VARCHAR handling\nAdd refresh, invalidate_metadata DDL options and add incremental option to compute_stats for COMPUTE INCREMENTAL STATS.\n\nAdd substitute method for performing multiple value substitutions in an array or scalar expression.\nDivision is by default true division like Python 3 for all numeric data. This means for SQL systems that use C-style division semantics, the appropriate CAST will be automatically inserted in the generated SQL.\nEasier joins on tables with overlapping column names. See Ibis for SQL Programmers.\nExpressions like string_expr[:3] now work as expected.\nAdd coalesce instance method to all value expressions.\nPassing limit=None to the execute method on expressions disables any default row limits."
  },
  {
    "objectID": "release_notes.html#api-changes-4",
    "href": "release_notes.html#api-changes-4",
    "title": "2.1.0 (2022-01-12)",
    "section": "API Changes",
    "text": "API Changes\n\nImpalaTable.rename no longer mutates the calling table expression."
  },
  {
    "objectID": "release_notes.html#contributors-3",
    "href": "release_notes.html#contributors-3",
    "title": "2.1.0 (2022-01-12)",
    "section": "Contributors",
    "text": "Contributors\n$ git log v0.5.0..v0.6.0 --pretty=format:%aN | sort | uniq -c | sort -rn\n46 Wes McKinney\n 3 Uri Laserson\n 1 Phillip Cloud\n 1 mariusvniekerk\n 1 Kristopher Overholt"
  },
  {
    "objectID": "release_notes.html#new-features-7",
    "href": "release_notes.html#new-features-7",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nSQLite client and built-in function support\nIbis now supports Python 3.4 as well as 2.6 and 2.7\nIbis can utilize Impala user-defined aggregate (UDA) functions\nSQLAlchemy-based translation toolchain to enable more SQL engines having SQLAlchemy dialects to be supported\nMany window function usability improvements (nested analytic functions and deferred binding conveniences)\nMore convenient aggregation with keyword arguments in aggregate functions\nBuilt preliminary wrapper API for MADLib-on-Impala\nAdd var and std aggregation methods and support in Impala\nAdd nullifzero numeric method for all SQL engines\nAdd rename method to Impala tables (for renaming tables in the Hive metastore)\nAdd close method to ImpalaClient for session cleanup (#533)\nAdd relabel method to table expressions\nAdd insert method to Impala tables\nAdd compile and verify methods to all expressions to test compilation and ability to compile (since many operations are unavailable in SQLite, for example)"
  },
  {
    "objectID": "release_notes.html#api-changes-5",
    "href": "release_notes.html#api-changes-5",
    "title": "2.1.0 (2022-01-12)",
    "section": "API Changes",
    "text": "API Changes\n\nImpala Ibis client creation now uses only ibis.impala.connect, and ibis.make_client has been deprecated"
  },
  {
    "objectID": "release_notes.html#contributors-4",
    "href": "release_notes.html#contributors-4",
    "title": "2.1.0 (2022-01-12)",
    "section": "Contributors",
    "text": "Contributors\n$ git log v0.4.0..v0.5.0 --pretty=format:%aN | sort | uniq -c | sort -rn\n      55 Wes McKinney\n      9 Uri Laserson\n      1 Kristopher Overholt"
  },
  {
    "objectID": "release_notes.html#new-features-8",
    "href": "release_notes.html#new-features-8",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nAdd tooling to use Impala C++ scalar UDFs within Ibis (#262, #195)\nSupport and testing for Kerberos-enabled secure HDFS clusters\nMany table functions can now accept functions as parameters (invoked on the calling table) to enhance composability and emulate late-binding semantics of languages (like R) that have non-standard evaluation (#460)\nAdd any, all, notany, and notall reductions on boolean arrays, as well as cumany and cumall\nUsing topk now produces an analytic expression that is executable (as an aggregation) but can also be used as a filter as before (#392, #91)\nAdded experimental database object \"usability layer\", see ImpalaClient.database.\nAdd TableExpr.info\nAdd compute_stats API to table expressions referencing physical Impala tables\nAdd explain method to ImpalaClient to show query plan for an expression\nAdd chmod and chown APIs to HDFS interface for superusers\nAdd convert_base method to strings and integer types\nAdd option to ImpalaClient.create_table to create empty partitioned tables\nibis.cross_join can now join more than 2 tables at once\nAdd ImpalaClient.raw_sql method for running naked SQL queries\nImpalaClient.insert now validates schemas locally prior to sending query to cluster, for better usability.\nAdd conda installation recipes"
  },
  {
    "objectID": "release_notes.html#contributors-5",
    "href": "release_notes.html#contributors-5",
    "title": "2.1.0 (2022-01-12)",
    "section": "Contributors",
    "text": "Contributors\n$ git log v0.3.0..v0.4.0 --pretty=format:%aN | sort | uniq -c | sort -rn\n     38 Wes McKinney\n      9 Uri Laserson\n      2 Meghana Vuyyuru\n      2 Kristopher Overholt\n      1 Marius van Niekerk"
  },
  {
    "objectID": "release_notes.html#new-features-9",
    "href": "release_notes.html#new-features-9",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nImplement window / analytic function support\nEnable non-equijoins (join clauses with operations other than ==).\nAdd remaining string functions supported by Impala.\nAdd pipe method to tables (hat-tip to the pandas dev team).\nAdd mutate convenience method to tables.\nFleshed out WebHDFS implementations: get/put directories, move files, etc. See the full HDFS API.\nAdd truncate method for timestamp values\nImpalaClient can execute scalar expressions not involving any table.\nCan also create internal Impala tables with a specific HDFS path.\nMake Ibis's temporary Impala database and HDFS paths configurable (see ibis.options).\nAdd truncate_table function to client (if the user's Impala cluster supports it).\nPython 2.6 compatibility\nEnable Ibis to execute concurrent queries in multithreaded applications (earlier versions were not thread-safe).\nTest data load script in scripts/load_test_data.py\nAdd an internal operation type signature API to enhance developer productivity."
  },
  {
    "objectID": "release_notes.html#contributors-6",
    "href": "release_notes.html#contributors-6",
    "title": "2.1.0 (2022-01-12)",
    "section": "Contributors",
    "text": "Contributors\n$ git log v0.2.0..v0.3.0 --pretty=format:%aN | sort | uniq -c | sort -rn\n     59 Wes McKinney\n     29 Uri Laserson\n      4 Isaac Hodes\n      2 Meghana Vuyyuru"
  },
  {
    "objectID": "release_notes.html#new-features-10",
    "href": "release_notes.html#new-features-10",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\ninsert method on Ibis client for inserting data into existing tables.\nparquet_file, delimited_file, and avro_file client methods for querying datasets not yet available in Impala\nNew ibis.hdfs_connect method and HDFS client API for WebHDFS for writing files and directories to HDFS\nNew timedelta API and improved timestamp data support\nNew bucket and histogram methods on numeric expressions\nNew category logical datatype for handling bucketed data, among other things\nAdd summary API to numeric expressions\nAdd value_counts convenience API to array expressions\nNew string methods like, rlike, and contains for fuzzy and regex searching\nAdd options.verbose option and configurable options.verbose_log callback function for improved query logging and visibility\nSupport for new SQL built-in functions\n\nibis.coalesce\nibis.greatest and ibis.least\nibis.where for conditional logic (see also ibis.case and ibis.cases)\nnullif method on value expressions\nibis.now\n\nNew aggregate functions: approx_median, approx_nunique, and group_concat\nwhere argument in aggregate functions\nAdd having method to group_by intermediate object\nAdded group-by convenience table.group_by(exprs).COLUMN_NAME.agg_function()\nAdd default expression names to most aggregate functions\nNew Impala database client helper methods\n\ncreate_database\ndrop_database\nexists_database\nlist_databases\nset_database\n\nClient list_tables searching / listing method\nAdd add, sub, and other explicit arithmetic methods to value expressions"
  },
  {
    "objectID": "release_notes.html#api-changes-6",
    "href": "release_notes.html#api-changes-6",
    "title": "2.1.0 (2022-01-12)",
    "section": "API Changes",
    "text": "API Changes\n\nNew Ibis client and Impala connection workflow. Client now combined from an Impala connection and an optional HDFS connection"
  },
  {
    "objectID": "release_notes.html#bug-fixes-18",
    "href": "release_notes.html#bug-fixes-18",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bug Fixes",
    "text": "Bug Fixes\n\nNumerous expression API bug fixes and rough edges fixed"
  },
  {
    "objectID": "release_notes.html#contributors-7",
    "href": "release_notes.html#contributors-7",
    "title": "2.1.0 (2022-01-12)",
    "section": "Contributors",
    "text": "Contributors\n$ git log v0.1.0..v0.2.0 --pretty=format:%aN | sort | uniq -c | sort -rn\n     71 Wes McKinney\n      1 Juliet Hougland\n      1 Isaac Hodes"
  },
  {
    "objectID": "tutorial/ibis-for-sql-users.html",
    "href": "tutorial/ibis-for-sql-users.html",
    "title": "Install",
    "section": "",
    "text": "If you don’t have ibis installed, you can install it from:"
  },
  {
    "objectID": "tutorial/ibis-for-sql-users.html#projections-selectaddremove-columns",
    "href": "tutorial/ibis-for-sql-users.html#projections-selectaddremove-columns",
    "title": "Install",
    "section": "Projections: select/add/remove columns",
    "text": "Projections: select/add/remove columns\nAll tables in Ibis are immutable. To select a subset of a table’s columns, or to add new columns, you must produce a new table by means of a projection.\n\nt = ibis.table(\n    [('one', 'string'), ('two', 'float'), ('three', 'int32')], 'my_data'\n)\nt\n\nIn SQL, you might write something like:\nSELECT two, one\nFROM my_data\nIn Ibis, this is\n\nproj = t['two', 'one']\n\nor\n\nproj = t.select(['two', 'one'])\n\nThis generates the expected SQL:\n\nibis.show_sql(proj)\n\nWhat about adding new columns? To form a valid projection, all column expressions must be named. Let’s look at the SQL:\nSELECT two, one, three * 2 AS new_col\nFROM my_data\nThe last expression is written:\n\nnew_col = (t.three * 2).name('new_col')\n\nNow, we have:\n\nproj = t['two', 'one', new_col]\nibis.show_sql(proj)\n\n\nmutate: Add or modify columns easily\nSince adding new columns or modifying existing columns is so common, there is a convenience method mutate:\n\nmutated = t.mutate(new_col=t.three * 2)\n\nNotice that using the name was not necessary here because we’re using Python keywords to provide the name. Indeed:\n\nibis.show_sql(mutated)\n\nIf you modify an existing column with mutate it will list out all the other columns:\n\nmutated = t.mutate(two=t.two * 2)\nibis.show_sql(mutated)\n\n\n\nSELECT * equivalent\nEspecially in combination with relational joins, it’s convenient to be able to select all columns in a table using the SELECT * construct. To do this, use the table expression itself in a projection:\n\nproj = t[t]\nibis.show_sql(proj)\n\nThis is how mutate is implemented. The example above t.mutate(new_col=t.three * 2) can be written as a normal projection:\n\nproj = t[t, new_col]\nibis.show_sql(proj)\n\nLet’s consider a table we might wish to join with t:\n\nt2 = ibis.table([('key', 'string'), ('value', 'float')], 'dim_table')\n\nNow let’s take the SQL:\nSELECT t0.*, t0.two - t1.value AS diff\nFROM my_data t0\n  INNER JOIN dim_table t1\n    ON t0.one = t1.key\nTo write this with Ibis, it is:\n\ndiff = (t.two - t2.value).name('diff')\njoined = t.join(t2, t.one == t2.key)[t, diff]\n\nAnd verify the generated SQL:\n\nibis.show_sql(joined)\n\n\n\nUsing functions in projections\nIf you pass a function instead of a string or Ibis expression in any projection context, it will be invoked with the “parent” table as its argument. This can help significantly when [composing complex operations. Consider this SQL:\nSELECT one, avg(abs(the_sum)) AS mad\nFROM (\n  SELECT one, three, sum(two) AS the_sum\n  FROM my_data\n  GROUP BY 1, 2\n) t0\nGROUP BY 1\nThis can be written as one chained expression:\n\nexpr = (\n    t.group_by(['one', 'three'])\n    .aggregate(the_sum=t.two.sum())\n    .group_by('one')\n    .aggregate(mad=lambda x: x.the_sum.abs().mean())\n)\n\nIndeed:\n\nibis.show_sql(expr)"
  },
  {
    "objectID": "tutorial/ibis-for-sql-users.html#filtering-where",
    "href": "tutorial/ibis-for-sql-users.html#filtering-where",
    "title": "Install",
    "section": "Filtering / WHERE",
    "text": "Filtering / WHERE\nYou can add filter clauses to a table expression either by indexing with [] (like pandas) or use the filter method:\n\nfiltered = t[t.two &gt; 0]\nibis.show_sql(filtered)\n\nfilter can take a list of expressions, which must all be satisfied for a row to be included in the result:\n\nfiltered = t.filter([t.two &gt; 0, t.one.isin(['A', 'B'])])\nibis.show_sql(filtered)\n\nTo compose boolean expressions with AND or OR, use the respective & and | operators:\n\ncond = (t.two &lt; 0) | ((t.two &gt; 0) | t.one.isin(['A', 'B']))\nfiltered = t[cond]\nibis.show_sql(filtered)"
  },
  {
    "objectID": "tutorial/ibis-for-sql-users.html#aggregation-group-by",
    "href": "tutorial/ibis-for-sql-users.html#aggregation-group-by",
    "title": "Install",
    "section": "Aggregation / GROUP BY",
    "text": "Aggregation / GROUP BY\nTo aggregate a table, you need:\n\nZero or more grouping expressions (these can be column names)\nOne or more aggregation expressions\n\nLet’s look at the aggregate method on tables:\n\nstats = [t.two.sum().name('total_two'), t.three.mean().name('avg_three')]\nagged = t.aggregate(stats)\n\nIf you don’t use any group expressions, the result will have a single row with your statistics of interest:\n\nagged.schema()\n\n\nibis.show_sql(agged)\n\nTo add groupings, use either the by argument of aggregate or use the group_by construct:\n\nagged2 = t.aggregate(stats, by='one')\nagged3 = t.group_by('one').aggregate(stats)\nibis.show_sql(agged3)\n\n\nNon-trivial grouping keys\nYou can use any expression (or function, like in projections) deriving from the table you are aggregating. The only constraint is that the expressions must be named. Let’s look at an example:\n\nevents = ibis.table(\n    [('ts', 'timestamp'), ('event_type', 'int32'), ('session_id', 'int64')],\n    name='web_events',\n)\n\nSuppose we wanted to total up event types by year and month:\n\nkeys = [events.ts.year().name('year'), events.ts.month().name('month')]\n\nsessions = events.session_id.nunique()\nstats = events.group_by(keys).aggregate(\n    total=events.count(), sessions=sessions\n)\n\nNow we have:\n\nibis.show_sql(stats)\n\n\n\nAggregates considering table subsets\nIn analytics is it common to compare statistics from different subsets of a table. Let’s consider a dataset containing people’s name, age, gender, and nationality:\n\npop = ibis.table(\n    [\n        ('name', 'string'),\n        ('country', 'string'),\n        ('gender', 'string'),\n        ('age', 'int16'),\n    ],\n    name='population',\n)\n\nNow, suppose you wanted to know for each country:\n\nAverage overall age\nAverage male age\nAverage female age\nTotal number of persons\n\nIn SQL, you may write:\nSELECT country,\n       count(*) AS num_persons,\n       AVG(age) AS avg_age\n       AVG(CASE WHEN gender = 'M'\n             THEN age\n             ELSE NULL\n           END) AS avg_male,\n       AVG(CASE WHEN gender = 'F'\n             THEN age\n             ELSE NULL\n           END) AS avg_female,\nFROM population\nGROUP BY 1\nIbis makes this much simpler by giving you where option in aggregation functions:\n\nexpr = pop.group_by('country').aggregate(\n    num_persons=pop.count(),\n    avg_age=pop.age.mean(),\n    avg_male=pop.age.mean(where=pop.gender == 'M'),\n    avg_female=pop.age.mean(where=pop.gender == 'F'),\n)\n\nThis indeed generates the correct SQL. Note that SQL engines handle NULL values differently in aggregation functions, but Ibis will write the SQL expression that is correct for your query engine.\n\nibis.show_sql(expr)\n\n\n\ncount(*) convenience: size()\nComputing group frequencies is so common that, like pandas, we have a method size that is a shortcut for the count(*) idiom:\n\nfreqs = events.group_by(keys).size()\nibis.show_sql(freqs)\n\n\n\nFrequency table convenience: value_counts\nConsider the SQL idiom:\nSELECT some_column_expression, count(*)\nFROM table\nGROUP BY 1\nThis is so common that, like pandas, there is a generic array method value_counts which does this for us:\n\nexpr = events.ts.year().value_counts()\nibis.show_sql(expr)\n\n\n\nHAVING clause\nThe SQL HAVING clause enables you to filter the results of an aggregation based on some group-wise condition holding true. For example, suppose we wanted to limit our analysis to groups containing at least 1000 observations:\nSELECT one, sum(two) AS total\nFROM my_data\nGROUP BY 1\nHAVING count(*) &gt;= 1000\nWith Ibis, you can do:\n\nexpr = (\n    t.group_by('one')\n    .having(t.count() &gt;= 1000)\n    .aggregate(t.two.sum().name('total'))\n)\nibis.show_sql(expr)"
  },
  {
    "objectID": "tutorial/ibis-for-sql-users.html#sorting-order-by",
    "href": "tutorial/ibis-for-sql-users.html#sorting-order-by",
    "title": "Install",
    "section": "Sorting / ORDER BY",
    "text": "Sorting / ORDER BY\nTo sort a table, use the order_by method along with either column names or expressions that indicate the sorting keys:\n\nsorted = events.order_by([events.ts.year(), events.ts.month()])\n\nibis.show_sql(sorted)\n\nThe default for sorting is in ascending order. To reverse the sort direction of any key, either wrap it in ibis.desc or pass a tuple with False as the second value:\n\nsorted = events.order_by(\n    [ibis.desc('event_type'), (events.ts.month(), False)]\n).limit(100)\n\nibis.show_sql(sorted)"
  },
  {
    "objectID": "tutorial/ibis-for-sql-users.html#limit-and-offset",
    "href": "tutorial/ibis-for-sql-users.html#limit-and-offset",
    "title": "Install",
    "section": "LIMIT and OFFSET",
    "text": "LIMIT and OFFSET\nThis one is easy. The table limit function truncates a table to the indicates number of rows. So if you only want the first 1000 rows (which may not be deterministic depending on the SQL engine), you can do:\n\nlimited = t.limit(1000)\nibis.show_sql(limited)\n\nThe offset option in limit skips rows. So if you wanted rows 11 through 20, you could do:\n\nlimited = t.limit(10, offset=10)\nibis.show_sql(limited)"
  },
  {
    "objectID": "tutorial/ibis-for-sql-users.html#common-column-expressions",
    "href": "tutorial/ibis-for-sql-users.html#common-column-expressions",
    "title": "Install",
    "section": "Common column expressions",
    "text": "Common column expressions\nSee the full API documentation &lt;api&gt; for all of the available value methods and tools for creating value expressions. We mention a few common ones here as they relate to common SQL queries.\n\nType casts\nIbis’s type system is independent of any SQL system. You cast Ibis expressions from one Ibis type to another. For example:\n\nexpr = t.mutate(date=t.one.cast('timestamp'), four=t.three.cast('float32'))\n\nibis.show_sql(expr)\n\n\n\nCASE statements\nSQL dialects typically support one or more kind of CASE statements. The first is the simple case that compares against exact values of an expression.\nCASE expr\n  WHEN value_1 THEN result_1\n  WHEN value_2 THEN result_2\n  ELSE default\nEND\nValue expressions in Ibis have a case method that allows us to emulate these semantics:\n\ncase = (\n    t.one.cast('timestamp')\n    .year()\n    .case()\n    .when(2015, 'This year')\n    .when(2014, 'Last year')\n    .else_('Earlier')\n    .end()\n)\n\nexpr = t.mutate(year_group=case)\nibis.show_sql(expr)\n\nThe more general case is that of an arbitrary list of boolean expressions and result values:\nCASE\n  WHEN boolean_expr1 THEN result_1\n  WHEN boolean_expr2 THEN result_2\n  WHEN boolean_expr3 THEN result_3\n  ELSE default\nEND\nTo do this, use ibis.case:\n\ncase = (\n    ibis.case()\n    .when(t.two &lt; 0, t.three * 2)\n    .when(t.two &gt; 1, t.three)\n    .else_(t.two)\n    .end()\n)\n\nexpr = t.mutate(cond_value=case)\nibis.show_sql(expr)\n\nThere are several places where Ibis builds cases for you in a simplified way. One example is the ifelse function:\n\nswitch = (t.two &lt; 0).ifelse('Negative', 'Non-Negative')\nexpr = t.mutate(group=switch)\nibis.show_sql(expr)\n\n\n\nUsing NULL in expressions\nTo use NULL in an expression, either use the special ibis.NA value or ibis.null():\n\npos_two = (t.two &gt; 0).ifelse(t.two, ibis.NA)\nexpr = t.mutate(two_positive=pos_two)\nibis.show_sql(expr)\n\n\n\nSet membership: IN / NOT IN\nLet’s look again at the population dataset. Suppose you wanted to combine the United States and Canada data into a “North America” category. Here would be some SQL to do it:\nCASE\n  WHEN upper(country) IN ('UNITED STATES', 'CANADA')\n    THEN 'North America'\n  ELSE country\nEND AS refined_group\nThe Ibis equivalent of IN is the isin method. So we can write:\n\nrefined = (\n    pop.country.upper()\n    .isin(['UNITED STATES', 'CANADA'])\n    .ifelse('North America', pop.country)\n)\n\nexpr = pop.mutate(refined_group=refined)\nibis.show_sql(expr)\n\nThe opposite of isin is notin.\n\n\nConstant and literal expressions\nConsider a SQL expression like:\n'foo' IN (column1, column2)\nwhich is equivalent to\ncolumn1 = 'foo' OR column2 = 'foo'\nTo build expressions off constant values, you must first convert the value (whether a Python string or number) to an Ibis expression using ibis.literal:\n\nt3 = ibis.table(\n    [('column1', 'string'), ('column2', 'string'), ('column3', 'float')],\n    'data',\n)\n\nvalue = ibis.literal('foo')\n\nOnce you’ve done this, you can use the literal expression like any other array or scalar expression:\n\nhas_foo = value.isin([t3.column1, t3.column2])\n\nexpr = t3.mutate(has_foo=has_foo)\nibis.show_sql(expr)\n\nIn many other situations, you can use constants without having to use ibis.literal. For example, we could add a column containing nothing but the number 5 like so:\n\nexpr = t3.mutate(number5=5)\nibis.show_sql(expr)\n\n\n\nIS NULL and IS NOT NULL\nThese are simple: use the isnull and notnull functions respectively, which yield boolean arrays:\n\nindic = t.two.isnull().ifelse('valid', 'invalid')\nexpr = t.mutate(is_valid=indic)\nibis.show_sql(expr)\n\n\nagged = (\n    expr[expr.one.notnull()]\n    .group_by('is_valid')\n    .aggregate(three_count=lambda t: t.three.notnull().sum())\n)\n\nibis.show_sql(agged)\n\n\n\nBETWEEN\nThe between method on arrays and scalars compiles to the SQL BETWEEN keyword. The result of between is boolean and can be used with any other boolean expression:\n\nexpr = t[t.two.between(10, 50) & t.one.notnull()]\nibis.show_sql(expr)"
  },
  {
    "objectID": "tutorial/ibis-for-sql-users.html#joins",
    "href": "tutorial/ibis-for-sql-users.html#joins",
    "title": "Install",
    "section": "Joins",
    "text": "Joins\nIbis supports several kinds of joins between table expressions:\n\ninner_join: maps to SQL INNER JOIN\ncross_join: a cartesian product join with no keys. Equivalent to inner_join with no join predicates\nleft_join: maps to SQL LEFT OUTER JOIN\nouter_join: maps to SQL FULL OUTER JOIN\nsemi_join: maps to SQL LEFT SEMI JOIN. May or may not be an explicit join type in your query engine.\nanti_join: maps to SQL LEFT ANTI JOIN. May or may not be an explicit join type in your query engine.\n\nThe join table method is by default the same as inner_join.\nLet’s look at a couple example tables to see how joins work in Ibis:\n\nt1 = ibis.table(\n    [('value1', 'float'), ('key1', 'string'), ('key2', 'string')], 'table1'\n)\n\nt2 = ibis.table(\n    [('value2', 'float'), ('key3', 'string'), ('key4', 'string')], 'table2'\n)\n\nLet’s join on one key:\n\njoined = t1.left_join(t2, t1.key1 == t2.key3)\n\nThe immediate result of a join does not yet have a set schema. That is determined by the next action that you take. There’s several ways forward from here that address the spectrum of SQL use cases.\n\nJoin + projection\nConsider the SQL:\nSELECT t0.*, t1.value2\nFROM table1 t0\n  LEFT OUTER JOIN table2 t1\n    ON t0.key1 = t1.key3\nAfter one or more joins, you can reference any of the joined tables in a projection immediately after:\n\nexpr = joined[t1, t2.value2]\nibis.show_sql(expr)\n\nIf you need to compute an expression that involves both tables, you can do that also:\n\nexpr = joined[t1.key1, (t1.value1 - t2.value2).name('diff')]\nibis.show_sql(expr)\n\n\n\nJoin + aggregation\nYou can directly aggregate a join without need for projection, which also allows you to form statistics that reference any of the joined tables.\nConsider this SQL:\nSELECT t0.key1, avg(t0.value1 - t1.value2) AS avg_diff\nFROM table1 t0\n  LEFT OUTER JOIN table2 t1\n    ON t0.key1 = t1.key3\nGROUP BY 1\nAs you would hope, the code is as follows:\n\navg_diff = (t1.value1 - t2.value2).mean()\nexpr = (\n    t1.left_join(t2, t1.key1 == t2.key3)\n    .group_by(t1.key1)\n    .aggregate(avg_diff=avg_diff)\n)\nibis.show_sql(expr)\n\n\n\nJoin with SELECT *\nIf you try to compile or execute a join that has not been projected or aggregated, it will be fully materialized:\n\njoined = t1.left_join(t2, t1.key1 == t2.key3)\nibis.show_sql(joined)\n\n\n\nMultiple joins\nYou can join multiple tables together in succession without needing to address any of the above concerns.\n\nt3 = ibis.table([('value3', 'float'), ('key5', 'string')], 'table3')\n\ntotal = (t1.value1 + t2.value2 + t3.value3).sum()\nexpr = (\n    t1.join(t2, [t1.key1 == t2.key3, t1.key2 == t2.key4])\n    .join(t3, t1.key1 == t3.key5)\n    .group_by([t2.key4, t3.key5])\n    .aggregate(total=total)\n)\nibis.show_sql(expr)\n\n\n\nSelf joins\nWhat about when you need to join a table on itself? For example:\nSELECT t0.one, avg(t0.two - t1.three) AS metric\nFROM my_data t0\n  INNER JOIN my_data t1\n    ON t0.one = t1.one\nGROUP BY 1\nThe table view method enables you to form a self-reference that is referentially distinct in expressions. Now you can proceed normally:\n\nt_view = t.view()\n\nstat = (t.two - t_view.three).mean()\nexpr = (\n    t.join(t_view, t.three.cast('string') == t_view.one)\n    .group_by(t.one)\n    .aggregate(metric=stat)\n)\nibis.show_sql(expr)\n\n\n\nOverlapping join keys\nIn many cases the columns being joined between two tables or table expressions have the same name. Consider this example:\n\nt4 = ibis.table(\n    [\n        ('key1', 'string'),\n        ('key2', 'string'),\n        ('key3', 'string'),\n        ('value1', 'float'),\n    ],\n    'table4',\n)\n\nt5 = ibis.table(\n    [\n        ('key1', 'string'),\n        ('key2', 'string'),\n        ('key3', 'string'),\n        ('value2', 'float'),\n    ],\n    'table5',\n)\n\nIn these case, we can specify a list of common join keys:\n\njoined = t4.join(t5, ['key1', 'key2', 'key3'])\nexpr = joined[t4, t5.value2]\nibis.show_sql(expr)\n\nYou can mix the overlapping key names with other expressions:\n\njoined = t4.join(t5, ['key1', 'key2', t4.key3.left(4) == t4.key3.left(4)])\nexpr = joined[t4, t5.value2]\nibis.show_sql(expr)\n\n\n\nNon-equality join predicates\nYou can join tables with boolean clauses that are not equality. Some query engines support these efficiently, some inefficiently, or some not at all. In the latter case, these conditions get moved by Ibis into the WHERE part of the SELECT query.\n\nexpr = t1.join(t2, t1.value1 &lt; t2.value2).group_by(t1.key1).size()\nibis.show_sql(expr)\n\n\n\nOther ways to specify join keys\nYou can also pass a list of column names instead of forming boolean expressions:\n\njoined = t1.join(t2, [('key1', 'key3'), ('key2', 'key4')])"
  },
  {
    "objectID": "tutorial/ibis-for-sql-users.html#subqueries",
    "href": "tutorial/ibis-for-sql-users.html#subqueries",
    "title": "Install",
    "section": "Subqueries",
    "text": "Subqueries\nIbis creates inline views and nested subqueries automatically. This section concerns more complex subqueries involving foreign references and other advanced relational algebra.\n\nCorrelated EXISTS / NOT EXISTS filters\nThe SQL EXISTS and NOT EXISTS constructs are typically used for efficient filtering in large many-to-many relationships.\nLet’s consider a web dataset involving website session / usage data and purchases:\n\nevents = ibis.table(\n    [\n        ('session_id', 'int64'),\n        ('user_id', 'int64'),\n        ('event_type', 'int32'),\n        ('ts', 'timestamp'),\n    ],\n    'events',\n)\n\npurchases = ibis.table(\n    [\n        ('item_id', 'int64'),\n        ('user_id', 'int64'),\n        ('price', 'float'),\n        ('ts', 'timestamp'),\n    ],\n    'purchases',\n)\n\nNow, the key user_id appears with high frequency in both tables. But let’s say you want to limit your analysis of the events table to only sessions by users who have made a purchase.\nIn SQL, you can do this using the somewhat esoteric EXISTS construct:\nSELECT t0.*\nFROM events t0\nWHERE EXISTS (\n  SELECT 1\n  FROM purchases t1\n  WHERE t0.user_id = t1.user_id\n)\nTo describe this operation in Ibis, you compare the user_id columns and use the any reduction:\n\ncond = (events.user_id == purchases.user_id).any()\n\nThis can now be used to filter events:\n\nexpr = events[cond]\nibis.show_sql(expr)\n\nIf you negate the condition, it will instead give you only event data from user that have not made a purchase:\n\nexpr = events[-cond]\nibis.show_sql(expr)\n\n\n\nSubqueries with IN / NOT IN\nSubquery filters with IN (and NOT IN) are functionally similar to EXISTS subqueries. Let’s look at some SQL:\nSELECT *\nFROM events\nWHERE user_id IN (\n  SELECT user_id\n  FROM purchases\n)\nThis is almost semantically the same as the EXISTS example. Indeed, you can write with Ibis:\n\ncond = events.user_id.isin(purchases.user_id)\nexpr = events[cond]\nibis.show_sql(expr)\n\nDepending on the query engine, the query planner/optimizer will often rewrite IN or EXISTS subqueries into the same set of relational algebra operations.\n\n\nComparison with scalar aggregates\nSometime you want to compare a value with an unconditional aggregate value from a different table. Take the SQL:\nSELECT *\nFROM table1\nWHERE value1 &gt; (\n  SELECT max(value2)\n  FROM table2\n)\nWith Ibis, the code is simpler and more pandas-like:\n\nexpr = t1[t1.value1 &gt; t2.value2.max()]\nibis.show_sql(expr)\n\n\n\nConditional aggregates\nSuppose you want to compare a value with the aggregate value for some common group values between two tables. Here’s some SQL:\nSELECT *\nFROM table1 t0\nWHERE value1 &gt; (\n  SELECT avg(value2)\n  FROM table2 t1\n  WHERE t0.key1 = t1.key3\n)\nThis query computes the average for each distinct value of key3 and uses the corresponding average for the comparison, rather than the whole-table average as above.\nWith Ibis, the code is similar, but you add the correlated filter to the average statistic:\n\nstat = t2[t1.key1 == t2.key3].value2.mean()\nexpr = t1[t1.value1 &gt; stat]\nibis.show_sql(expr)"
  },
  {
    "objectID": "tutorial/ibis-for-sql-users.html#distinct-expressions",
    "href": "tutorial/ibis-for-sql-users.html#distinct-expressions",
    "title": "Install",
    "section": "DISTINCT expressions",
    "text": "DISTINCT expressions\nIn SQL, the DISTINCT keyword is used in a couple of ways:\n\nDeduplicating identical rows in some SELECT statement\nAggregating on the distinct values of some column expression\n\nIbis supports both use cases. So let’s have a look. The first case is the simplest: call distinct on a table expression. First, here’s the SQL:\nSELECT DISTINCT *\nFROM table1\nAnd the Ibis Python code:\n\nexpr = t1.distinct()\nibis.show_sql(expr)\n\nFor distinct aggregates, the most common case is COUNT(DISTINCT ...), which computes the number of unique values in an expression. So if we’re looking at the events table, let’s compute the number of distinct event_type values for each user_id. First, the SQL:\nSELECT user_id, COUNT(DISTINCT event_type) AS unique_events\nFROM events\nGROUP BY 1\nIn Ibis this is:\n\nmetric = events.event_type.nunique()\nexpr = events.group_by('user_id').aggregate(unique_events=metric)\nibis.show_sql(expr)"
  },
  {
    "objectID": "tutorial/ibis-for-sql-users.html#window-functions",
    "href": "tutorial/ibis-for-sql-users.html#window-functions",
    "title": "Install",
    "section": "Window functions",
    "text": "Window functions\nWindow functions in SQL allow you to write expressions that involve possibly-ordered groups of a table. Each window function involves one of the following:\n\nAn analytic function. Most aggregate functions are valid analytic functions, and there are additional ones such as LEAD, LAG, NTILE, and others.\nA PARTITION BY clause. This may be omitted.\nAn ORDER BY clause. This may be omitted for many functions.\nA window frame clause. The default is to use the entire partition.\n\nSo you may see SQL like:\nAVG(value) OVER (PARTITION BY key1)\nOr simply\nAVG(value) OVER ()\nIbis will automatically write window clauses when you use aggregate functions in a non-aggregate context. Suppose you wanted to subtract the mean of a column from itself:\n\nexpr = t.mutate(two_demean=t.two - t.two.mean())\nibis.show_sql(expr)\n\nIf you use mutate in conjunction with group_by, it will add a PARTITION BY to the OVER specification:\n\nexpr = t.group_by('one').mutate(two_demean=t.two - t.two.mean())\n\nibis.show_sql(expr)\n\nFor functions like LAG that require an ordering, we can add an order_by call:\n\nexpr = (\n    t.group_by('one')\n    .order_by(t.two)\n    .mutate(two_first_diff=t.two - t.two.lag())\n)\n\nibis.show_sql(expr)\n\nFor more precision, you can create a Window object that also includes a window frame clause:\n\nw = ibis.window(group_by='one', preceding=5, following=5)\nexpr = t.mutate(group_demeaned=t.two - t.two.mean().over(w))\nibis.show_sql(expr)"
  },
  {
    "objectID": "tutorial/ibis-for-sql-users.html#top-k-operations",
    "href": "tutorial/ibis-for-sql-users.html#top-k-operations",
    "title": "Install",
    "section": "Top-K operations",
    "text": "Top-K operations\nA common SQL idiom is the “top-K” or “top-N” operation: subsetting a dimension by aggregate statistics:\nSELECT key1, count(*) AS `count`\nFROM table1\nGROUP BY 1\nORDER BY `count` DESC\nLIMIT 10\nIbis has a special analytic expression topk:\n\nexpr = t1.key1.topk(10)\n\nThis can be evaluated directly, yielding the above query:\n\nibis.show_sql(expr)"
  },
  {
    "objectID": "tutorial/ibis-for-sql-users.html#date-time-data",
    "href": "tutorial/ibis-for-sql-users.html#date-time-data",
    "title": "Install",
    "section": "Date / time data",
    "text": "Date / time data\nSee Timestamp methods &lt;api.timestamp&gt; for a table of available date/time methods.\nFor example, we can do:\n\nexpr = events.mutate(year=events.ts.year(), month=events.ts.month())\n\nibis.show_sql(expr)\n\n\nCasting to date / time types\nIn many cases, you can convert string values to datetime / timestamp with strings.cast('timestamp'), but you may have to do some more reconnaissance into the data if this does not work.\n\n\nIntervals\nIbis has a set of interval APIs that allow you to do date/time arithmetic. For example:\n\nexpr = events[events.ts &gt; (ibis.now() - ibis.interval(years=1))]\nibis.show_sql(expr)\n\nThe implementation of each timedelta offset will depend on the query engine."
  },
  {
    "objectID": "tutorial/ibis-for-sql-users.html#buckets-and-histograms",
    "href": "tutorial/ibis-for-sql-users.html#buckets-and-histograms",
    "title": "Install",
    "section": "Buckets and histograms",
    "text": "Buckets and histograms\nTo appear."
  },
  {
    "objectID": "tutorial/ibis-for-sql-users.html#unions",
    "href": "tutorial/ibis-for-sql-users.html#unions",
    "title": "Install",
    "section": "Unions",
    "text": "Unions\nSQL dialects often support two kinds of UNION operations:\n\nUNION: the combination of distinct rows from each table.\nUNION ALL: the combination of all rows from each table, whether or not they are distinct.\n\nThe Ibis union function by distinct is a UNION ALL, and you can set distinct=True to get the normal UNION behavior:\n\nexpr1 = t1.limit(10)\nexpr2 = t1.limit(10, offset=10)\n\nexpr = expr1.union(expr2)\nibis.show_sql(expr)"
  },
  {
    "objectID": "tutorial/ibis-for-sql-users.html#esoterica",
    "href": "tutorial/ibis-for-sql-users.html#esoterica",
    "title": "Install",
    "section": "Esoterica",
    "text": "Esoterica\nThis area will be the spillover for miscellaneous SQL concepts and how queries featuring them can be ported to Ibis.\n\nCommon table expressions (CTEs)\nThe simplest SQL CTE is a SQL statement that is used multiple times in a SELECT query, which can be “factored” out using the WITH keyword:\nWITH t0 AS (\n   SELECT region, kind, sum(amount) AS total\n   FROM purchases\n   GROUP BY 1, 2\n)\nSELECT t0.region, t0.total - t1.total\nFROM t0\n  INNER JOIN t0 t1\n    ON t0.region = t1.region\nWHERE t0.kind = 'foo' AND t1.kind = 'bar'\nExplicit CTEs are not necessary with Ibis. Let’s look at an example involving joining an aggregated table on itself after filtering:\n\npurchases = ibis.table(\n    [\n        ('region', 'string'),\n        ('kind', 'string'),\n        ('user', 'int64'),\n        ('amount', 'float'),\n    ],\n    'purchases',\n)\n\nmetric = purchases.amount.sum().name('total')\nagged = purchases.group_by(['region', 'kind']).aggregate(metric)\n\nleft = agged[agged.kind == 'foo']\nright = agged[agged.kind == 'bar']\n\nresult = left.join(right, left.region == right.region)[\n    left.region, (left.total - right.total).name('diff')\n]\n\nIbis automatically creates a CTE for agged:\n\nibis.show_sql(result)"
  },
  {
    "objectID": "tutorial/ibis-for-dplyr-users.html",
    "href": "tutorial/ibis-for-dplyr-users.html",
    "title": "Ibis for dplyr users",
    "section": "",
    "text": "R users familiar with dplyr and other packages in the Tidyverse are likely to find Ibis familiar. In fact, some Ibis features were even inspired by similar features in the Tidyverse.\nHowever, due to differences between Python and R and the design and goals of Ibis itself, you may notice some big differences right away:\nUsing the same example data and similar operations as in Introduction to dplyr, below you will find some examples of the more common dplyr operations and their Ibis equivalents."
  },
  {
    "objectID": "tutorial/ibis-for-dplyr-users.html#loading-ibis",
    "href": "tutorial/ibis-for-dplyr-users.html#loading-ibis",
    "title": "Ibis for dplyr users",
    "section": "Loading Ibis",
    "text": "Loading Ibis\nTo start using dplyr in R we would run:\nlibrary(dplyr)\nTo load Ibis:\n\nimport ibis\n\nAnd then also load and alias some helpers to make our code more concise:\n\nimport ibis.selectors as s\nfrom ibis import _\n\nLast, as mentioned above, to get Ibis to automatically execute our queries and show the results in a nicely-formatted table, we run:\n\nibis.options.interactive = True"
  },
  {
    "objectID": "tutorial/ibis-for-dplyr-users.html#loading-example-data",
    "href": "tutorial/ibis-for-dplyr-users.html#loading-example-data",
    "title": "Ibis for dplyr users",
    "section": "Loading example data",
    "text": "Loading example data\nIn R, datasets are typically lazily loaded with packages. For instance, the starwars dataset is packaged with dplyr, but is not loaded in memory before you start using it. Ibis provides many datasets in the examples module. So to be able to use the starwars dataset, you can use:\n\nstarwars = ibis.examples.starwars.fetch()\n\nSimilar to dplyr, if we evaluate the name of a table, we get a nicely-formatted table:\n\nstarwars\n\nIn addition to printing a nicely-formatted table and automatically executing, setting ibis.options.interactive to True also causes our query to be limited to 10 rows. To get Ibis to give us all rows, we can directly call to_pandas and save the result as a pandas DataFrame:\n\nstarwars_df = starwars.to_pandas()\n\nWhich then gives us all of the data as a pandas DataFrame:\n\nstarwars_df\n\nDirectly calling to_pandas and saving the result to a variable is useful for passing the results of Ibis table expressions to other packages (e.g., matplotlib)."
  },
  {
    "objectID": "tutorial/ibis-for-dplyr-users.html#inspecting-the-dataset-with-head",
    "href": "tutorial/ibis-for-dplyr-users.html#inspecting-the-dataset-with-head",
    "title": "Ibis for dplyr users",
    "section": "Inspecting the dataset with head()",
    "text": "Inspecting the dataset with head()\nJust like in R, you can use head() to inspect the beginning of a dataset. You can also specify the number of rows you want to get back by using the parameter n (default n = 5).\nIn R:\nhead(starwars) # or starwars |&gt; head()\nWith Ibis:\n\nstarwars.head(6)\n\nThere is no tail() in Ibis because most databases do not support this operation.\nAnother method you can use to limit the number of rows returned by a query is limit() which also takes the n parameter.\n\nstarwars.limit(3)"
  },
  {
    "objectID": "tutorial/ibis-for-dplyr-users.html#filtering-rows-with-filter",
    "href": "tutorial/ibis-for-dplyr-users.html#filtering-rows-with-filter",
    "title": "Ibis for dplyr users",
    "section": "Filtering rows with filter()",
    "text": "Filtering rows with filter()\nIbis, like dplyr, has the filter method to select rows based on conditions.\nWith dplyr:\nstarwars |&gt;\n  filter(skin_color == \"light\")\nIn Ibis:\n\nstarwars.filter(_.skin_color == \"light\")\n\nIn dplyr, you can specify multiple conditions separated with , that are then combined with the & operator:\nstarwars |&gt;\n  filter(skin_color == \"light\", eye_color == \"brown\")\nIn Ibis, you can do the same by putting multiple conditions in a list:\n\nstarwars.filter([_.skin_color == \"light\", _.eye_color == \"brown\"])\n\nIn previous code, we used the _ helper we imported earlier. The _ is shorthand for the table returned by the previous step in the chained sequence of operations (in this case, starwars). We could have also written the more verbose form,\nstarwars.filter([starwars.skin_color == \"light\", starwars.eye_color == \"brown\"])\nIf you want to combine multiple conditions, in dplyr, you could do:\nstarwars |&gt;\n  filter(\n      (skin_color == \"light\" & eye_color == \"brown\") |\n       species == \"Droid\"\n  )\nIn Ibis, this would be:\n\nstarwars.filter(\n    ((_.skin_color == \"light\") & (_.eye_color == \"brown\")) |\n    (_.species == \"Droid\")\n)"
  },
  {
    "objectID": "tutorial/ibis-for-dplyr-users.html#sorting-your-data-with-order_by",
    "href": "tutorial/ibis-for-dplyr-users.html#sorting-your-data-with-order_by",
    "title": "Ibis for dplyr users",
    "section": "Sorting your data with order_by()",
    "text": "Sorting your data with order_by()\nTo sort a column, dplyr has the verb arrange. For instance, to sort the column height using dplyr:\nstarwars |&gt;\n   arrange(height)\nIn Ibis:\n\nstarwars.order_by(_.height)\n\nYou might notice that while dplyr puts missing values at the end, Ibis places them at the top. This behavior can actually vary from backend to backend and is something to be aware of when using Ibis.\nIf you want to order using multiple variables, you can pass them as a list:\n\nstarwars.order_by([_.height, _.mass]) # or starwars.order_by([\"height\", \"mass\"])\n\nTo order a column in descending order, there are two ways to do it. Note that missing values remain at the top.\n\nstarwars.order_by(_.height.desc()) # or: starwars.order_by(ibis.desc(\"height\"))"
  },
  {
    "objectID": "tutorial/ibis-for-dplyr-users.html#selecting-columns-with-select",
    "href": "tutorial/ibis-for-dplyr-users.html#selecting-columns-with-select",
    "title": "Ibis for dplyr users",
    "section": "Selecting columns with select()",
    "text": "Selecting columns with select()\nIbis, like dplyr, has a select method to include or exclude columns:\nWith dplyr:\nstarwars |&gt; \n    select(hair_color)\nIn Ibis:\n\nstarwars.select(_.hair_color)\n\nNote: A common pitfall to be aware of when referencing column names in Ibis is when column names collide with built-in methods on the Ibis Table object, such as count. In this situation, you will have to reference count like table[\"count\"] or _[\"count\"].\ndplyr also allows selecting more than one column at a time:\nstarwars |&gt;\n    select(hair_color, skin_color, eye_color)\nIn Ibis, we can either quote the names:\n\nstarwars.select(\"hair_color\", \"skin_color\", \"eye_color\")\n\nOr use the _ helper:\n\nstarwars.select(_.hair_color, _.skin_color, _.eye_color)\n\nTo select columns by name based on a condition, dplyr has helpers such as:\n\nstarts_with(): Starts with a prefix.\nends_with(): Ends with a suffix.\ncontains(): Contains a literal string.\n\nThese and many more selectors are available in Ibis as well, with slightly different names:\n\nstarwars.select(s.startswith(\"h\"))\n\n\nstarwars.select(s.endswith(\"color\"))\n\n\nstarwars.select(s.contains(\"world\"))\n\nSee the Ibis Column Selectors documentation for the full list of selectors in Ibis."
  },
  {
    "objectID": "tutorial/ibis-for-dplyr-users.html#renaming-columns-with-relabel",
    "href": "tutorial/ibis-for-dplyr-users.html#renaming-columns-with-relabel",
    "title": "Ibis for dplyr users",
    "section": "Renaming columns with relabel()",
    "text": "Renaming columns with relabel()\nIbis allows you to rename columns using relabel() which provides similar functionality to rename() in dplyr.\nIn dplyr:\nstarwars |&gt; \n    rename(\"homeworld\" = \"home_world\")\nIn Ibis, use relabel and pass a dict of name mappings:\n\nstarwars.relabel({\"homeworld\": \"home_world\"})"
  },
  {
    "objectID": "tutorial/ibis-for-dplyr-users.html#add-new-columns-with-mutate",
    "href": "tutorial/ibis-for-dplyr-users.html#add-new-columns-with-mutate",
    "title": "Ibis for dplyr users",
    "section": "Add new columns with mutate()",
    "text": "Add new columns with mutate()\nIbis, like dplyr, uses the mutate verb to add columns.\nIn dplyr,\nstarwars |&gt;\n    mutate(height_m = height / 100) |&gt;\n    select(name, height_m)\nIn Ibis:\n\n(\n    starwars\n        .mutate(height_m = _.height / 100)\n        .select(\"name\", \"height_m\")\n)\n\nA big difference between dplyr’s mutate and Ibis’ mutate is that, in Ibis, you have to chain separate mutate calls together when you reference newly-created columns in the same mutate whereas in dplyr, you can put them all in the same call. This makes Ibis’ mutate more similar to transform in base R.\nIn dplyr, we only need one mutate call:\nstarwars %&gt;%\n  mutate(\n    height_m = height / 100,\n    BMI = mass / (height_m^2)\n  ) %&gt;%\n  select(BMI, everything())\nIn Ibis, for BMI to reference height_m, it needs to be in a separate mutate call:\n\n(starwars\n    .mutate(\n        height_m = _.height / 100\n    )\n    .mutate(        \n        BMI = _.mass / (_.height_m**2)\n    )\n    .select(\"BMI\", ~s.matches(\"BMI\"))\n)"
  },
  {
    "objectID": "tutorial/ibis-for-dplyr-users.html#summarize-values-with-aggregate",
    "href": "tutorial/ibis-for-dplyr-users.html#summarize-values-with-aggregate",
    "title": "Ibis for dplyr users",
    "section": "Summarize values with aggregate()",
    "text": "Summarize values with aggregate()\nTo summarize tables, dplyr has the verbs summarise/summarize:\nIn dplyr:\nstarwars %&gt;% \n    summarise(height = mean(height, na.rm = TRUE))\nIn Ibis, the corresponding verb is aggregate:\n\nstarwars.aggregate(height = _.height.mean())"
  },
  {
    "objectID": "tutorial/ibis-for-dplyr-users.html#join-tables-with-left_join",
    "href": "tutorial/ibis-for-dplyr-users.html#join-tables-with-left_join",
    "title": "Ibis for dplyr users",
    "section": "Join tables with left_join()",
    "text": "Join tables with left_join()\nTo demonstrate how to do joins with Ibis, we’ll load two more example datasets that also come from the example datasets included in dplyr:\n\nband_members = ibis.examples.band_members.fetch()\nband_instruments = ibis.examples.band_instruments.fetch()\n\nIn dplyr, we can perform a left join of these two tables like:\nband_members |&gt; \n    left_join(band_instruments)\nIn Ibis:\n\nband_members.left_join(band_instruments, \"name\")\n\nThere are two main differences between Ibis and dplyr here:\n\nIbis requires us to explicitly specify our join key (“name”, in this example) whereas in dplyr, if the join key is missing, we get the natural join of the two tables which joins across all shared column names\nIbis keeps columns for join keys from each table whereas dplyr does not by default\n\nTo replicate the result we’d get by default in dplyr but using Ibis, we need to incorporate two other verbs we’ve already seen in this tutorial:\n\n(\n    band_members\n        .left_join(band_instruments, \"name\")\n        .select(~s.contains(\"_right\"))\n)"
  },
  {
    "objectID": "tutorial/ibis-for-dplyr-users.html#pivot-data-with-pivot_wider-and-pivot_longer",
    "href": "tutorial/ibis-for-dplyr-users.html#pivot-data-with-pivot_wider-and-pivot_longer",
    "title": "Ibis for dplyr users",
    "section": "Pivot data with pivot_wider() and pivot_longer()",
    "text": "Pivot data with pivot_wider() and pivot_longer()\ndplyr users are likely to be familiar with the pivot_wider and pivot_longer functions from the tidyr package which convert tables between wide and long formats, respectively.\nIn dplyr+tidyr:\nstarwars |&gt; \n    select(name, matches(\"color\")) |&gt; \n    pivot_longer(matches(\"color\"), names_to = \"attribute\", values_to = \"color\")\nIn Ibis:\n\nstarwars_colors = (\n    starwars\n        .select(\"name\", s.matches(\"color\"))\n        .pivot_longer(s.matches(\"color\"), names_to=\"attribute\", values_to=\"color\")\n)\n\nstarwars_colors\n\nAnd the reverse, in dplyr:\nstarwars_colors |&gt; \n    pivot_wider(names_from = \"attribute\", values_from = \"value\")\nIn Ibis:\n\n(\n    starwars_colors.\n        pivot_wider(names_from=\"attribute\", values_from=\"color\")\n)"
  },
  {
    "objectID": "tutorial/ibis-for-dplyr-users.html#next-steps",
    "href": "tutorial/ibis-for-dplyr-users.html#next-steps",
    "title": "Ibis for dplyr users",
    "section": "Next Steps",
    "text": "Next Steps\nNow that you’ve gotten an introudction to the common differences between dplyr and Ibis, head over to Getting Started with ibis for a full introduction. If you’re familiar with SQL, check out Ibis for SQL Programmers. If you’re familiar with pandas, take a look at Ibis for pandas Users"
  },
  {
    "objectID": "tutorial/index.html",
    "href": "tutorial/index.html",
    "title": "Ibis tutorials",
    "section": "",
    "text": "Ibis tutorials\nWelcome to the Ibis tutorials!\n\nLearning Ibis for the first time?: Check out the Ibis getting started tutorial!\nComing from SQL?: Take a look at Ibis for SQL users!\nComing from pandas?: Check out Ibis for pandas users!\nComing from R?: See Ibis for dplyr users!\nWant to see some more examples?: We’ve got a repository of examples for that!"
  },
  {
    "objectID": "tutorial/getting_started.html",
    "href": "tutorial/getting_started.html",
    "title": "Getting started with ibis",
    "section": "",
    "text": "This is a quick tour of some basic commands and usage patterns, just to get your flippers wet.\n\n\nThis quick-start guide uses the DuckDB backend. You can check out the Install page for information on how to install other backends.\nshell title=\"Install Ibis using pip\" $ pip install 'ibis-framework[duckdb]'\nshell title=\"Install Ibis using conda\" $ conda install ibis-framework\n\n\n\nIbis can work with several file types, but at its core, it connects to existing databases and interacts with the data there. We’ll use a local database (DuckDB) to get the hang of this.1\npython title=\"Download an example dataset\" &gt;&gt;&gt; import urllib.request &gt;&gt;&gt; urllib.request.urlretrieve(         \"https://storage.googleapis.com/ibis-tutorial-data/palmer_penguins.ddb\",         \"palmer_penguins.ddb\",     )\n\n\n\npython title=\"Connect to an existing database\" &gt;&gt;&gt; import ibis &gt;&gt;&gt; con = ibis.duckdb.connect(\"palmer_penguins.ddb\")\nWe’re connected! Let’s take a look at what tables are available.\n&gt;&gt;&gt; con.list_tables()\n['penguins']\nThere’s one table, called penguins. We can ask Ibis to give us an object that we can interact with.\n&gt;&gt;&gt; penguins = con.table(\"penguins\")\n&gt;&gt;&gt; penguins\nAlchemyTable: penguins\n  species           string\n  island            string\n  bill_length_mm    float64\n  bill_depth_mm     float64\n  flipper_length_mm int64\n  body_mass_g       int64\n  sex               string\n  year              int64\nIbis is lazily evaluated, so instead of seeing the data, we see the schema of the table, instead. To peek at the data, we can call head and then to_pandas to get the first few rows of the table as a pandas DataFrame.\n&gt;&gt;&gt; penguins.head().to_pandas()\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g     sex  year\n0  Adelie  Torgersen            39.1           18.7              181.0       3750.0    male  2007\n1  Adelie  Torgersen            39.5           17.4              186.0       3800.0  female  2007\n2  Adelie  Torgersen            40.3           18.0              195.0       3250.0  female  2007\n3  Adelie  Torgersen             NaN            NaN                NaN          NaN    None  2007\n4  Adelie  Torgersen            36.7           19.3              193.0       3450.0  female  2007\nto_pandas takes the existing lazy table expression and evaluates it. If we leave it off, you’ll see the Ibis representation of the table expression that to_pandas will evaluate (when you’re ready!).\n&gt;&gt;&gt; penguins.head()\nr0 := AlchemyTable: penguins\n  species           string\n  island            string\n  bill_length_mm    float64\n  bill_depth_mm     float64\n  flipper_length_mm int64\n  body_mass_g       int64\n  sex               string\n  year              int64\n\nLimit[r0, n=5]\n!!! note “Results in pandas DataFrame”\nIbis returns results as a pandas DataFrame using `to_pandas`, but isn't using pandas to\nperform any of the computation. The query is executed by the backend (DuckDB in\nthis case). Only when `to_pandas` is called does Ibis then pull back the results\nand convert them into a DataFrame.\n\n\n\nFor the rest of this intro, we’ll turn on interactive mode, which partially executes queries to give users a preview of the results. There is a small difference in the way the output is formatted, but otherwise this is the same as calling to_pandas on the table expression with a limit of 10 result rows returned.\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; penguins.head()\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nIbis has a collection of useful table methods to manipulate and query the data in a table (or tables).\n\n\nfilter allows you to select rows based on a condition or set of conditions.\nWe can filter so we only have penguins of the species Adelie:\n &gt;&gt;&gt; penguins.filter(penguins.species == \"Adelie\")\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\nOr filter for Adelie penguins that reside on the island of Torgersen:\n&gt;&gt;&gt; penguins.filter((penguins.island == \"Torgersen\") & (penguins.species == \"Adelie\"))\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\nYou can use any boolean comparison in a filter (although if you try to do something like use &lt; on a string, Ibis will yell at you).\n\n\n\nYour data analysis might not require all the columns present in a given table. select lets you pick out only those columns that you want to work with.\nTo select a column you can use the name of the column as a string:\n&gt;&gt;&gt; penguins.select(\"species\", \"island\", \"year\")\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ …       │ …         │     … │\n└─────────┴───────────┴───────┘\nOr you can use column objects directly (this can be convenient when paired with tab-completion):\n&gt;&gt;&gt; penguins.select(penguins.species, penguins.island, penguins.year)\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ …       │ …         │     … │\n└─────────┴───────────┴───────┘\nOr you can mix-and-match:\n\n&gt;&gt;&gt; penguins.select(\"species\", \"island\", penguins.year)\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ …       │ …         │     … │\n└─────────┴───────────┴───────┘\n\n\n\nmutate lets you add new columns to your table, derived from the values of existing columns.\n&gt;&gt;&gt; penguins.mutate(bill_length_cm=penguins.bill_length_mm / 10)\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ … │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │ … │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │ … │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │ … │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │        NULL │ NULL   │ … │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │ … │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │ … │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │ … │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │ … │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │ … │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │ … │\n│ …       │ …         │              … │             … │                 … │           … │ …      │ … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───┘\nNotice that the table is a little too wide to display all the columns now (depending on your screen-size). bill_length is now present in millimeters AND centimeters. Use a select to trim down the number of columns we’re looking at.\n&gt;&gt;&gt; penguins.mutate(bill_length_cm=penguins.bill_length_mm / 10).select(\n        \"species\",\n        \"island\",\n        \"bill_depth_mm\",\n        \"flipper_length_mm\",\n        \"body_mass_g\",\n        \"sex\",\n        \"year\",\n        \"bill_length_cm\",\n    )\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃ bill_length_cm ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ float64       │ int64             │ int64       │ string │ int64 │ float64        │\n├─────────┼───────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┼────────────────┤\n│ Adelie  │ Torgersen │          18.7 │               181 │        3750 │ male   │  2007 │           3.91 │\n│ Adelie  │ Torgersen │          17.4 │               186 │        3800 │ female │  2007 │           3.95 │\n│ Adelie  │ Torgersen │          18.0 │               195 │        3250 │ female │  2007 │           4.03 │\n│ Adelie  │ Torgersen │           nan │              NULL │        NULL │ NULL   │  2007 │            nan │\n│ Adelie  │ Torgersen │          19.3 │               193 │        3450 │ female │  2007 │           3.67 │\n│ Adelie  │ Torgersen │          20.6 │               190 │        3650 │ male   │  2007 │           3.93 │\n│ Adelie  │ Torgersen │          17.8 │               181 │        3625 │ female │  2007 │           3.89 │\n│ Adelie  │ Torgersen │          19.6 │               195 │        4675 │ male   │  2007 │           3.92 │\n│ Adelie  │ Torgersen │          18.1 │               193 │        3475 │ NULL   │  2007 │           3.41 │\n│ Adelie  │ Torgersen │          20.2 │               190 │        4250 │ NULL   │  2007 │           4.20 │\n│ …       │ …         │             … │                 … │           … │ …      │     … │              … │\n└─────────┴───────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┴────────────────┘\n\n\n\nTyping out ALL of the column names except one is a little annoying. Instead of doing that again, we can use a selector to quickly select or deselect groups of columns.\n&gt;&gt;&gt; from ibis import selectors as s\n\n&gt;&gt;&gt; penguins.mutate(bill_length_cm=penguins.bill_length_mm / 10).select(\n        ~s.matches(\"bill_length_mm\")\n        # match every column except `bill_length_mm`\n    )\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃ bill_length_cm ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ float64       │ int64             │ int64       │ string │ int64 │ float64        │\n├─────────┼───────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┼────────────────┤\n│ Adelie  │ Torgersen │          18.7 │               181 │        3750 │ male   │  2007 │           3.91 │\n│ Adelie  │ Torgersen │          17.4 │               186 │        3800 │ female │  2007 │           3.95 │\n│ Adelie  │ Torgersen │          18.0 │               195 │        3250 │ female │  2007 │           4.03 │\n│ Adelie  │ Torgersen │           nan │              NULL │        NULL │ NULL   │  2007 │            nan │\n│ Adelie  │ Torgersen │          19.3 │               193 │        3450 │ female │  2007 │           3.67 │\n│ Adelie  │ Torgersen │          20.6 │               190 │        3650 │ male   │  2007 │           3.93 │\n│ Adelie  │ Torgersen │          17.8 │               181 │        3625 │ female │  2007 │           3.89 │\n│ Adelie  │ Torgersen │          19.6 │               195 │        4675 │ male   │  2007 │           3.92 │\n│ Adelie  │ Torgersen │          18.1 │               193 │        3475 │ NULL   │  2007 │           3.41 │\n│ Adelie  │ Torgersen │          20.2 │               190 │        4250 │ NULL   │  2007 │           4.20 │\n│ …       │ …         │             … │                 … │           … │ …      │     … │              … │\n└─────────┴───────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┴────────────────┘\nYou can also use a selector alongside a column name.\n&gt;&gt;&gt; penguins.select(\"island\", s.numeric())\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━┓\n┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ year  ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━┩\n│ string    │ float64        │ float64       │ int64             │ int64       │ int64 │\n├───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼───────┤\n│ Torgersen │           39.1 │          18.7 │               181 │        3750 │  2007 │\n│ Torgersen │           39.5 │          17.4 │               186 │        3800 │  2007 │\n│ Torgersen │           40.3 │          18.0 │               195 │        3250 │  2007 │\n│ Torgersen │            nan │           nan │              NULL │        NULL │  2007 │\n│ Torgersen │           36.7 │          19.3 │               193 │        3450 │  2007 │\n│ Torgersen │           39.3 │          20.6 │               190 │        3650 │  2007 │\n│ Torgersen │           38.9 │          17.8 │               181 │        3625 │  2007 │\n│ Torgersen │           39.2 │          19.6 │               195 │        4675 │  2007 │\n│ Torgersen │           34.1 │          18.1 │               193 │        3475 │  2007 │\n│ Torgersen │           42.0 │          20.2 │               190 │        4250 │  2007 │\n│ …         │              … │             … │                 … │           … │     … │\n└───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴───────┘\nYou can read more about selectors in the docs!\n\n\n\norder_by arranges the values of one or more columns in ascending or descending order.\nBy default, ibis sorts in ascending order:\n&gt;&gt;&gt; penguins.order_by(penguins.flipper_length_mm).select(\n        \"species\", \"island\", \"flipper_length_mm\"\n    )\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ species   ┃ island    ┃ flipper_length_mm ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string    │ int64             │\n├───────────┼───────────┼───────────────────┤\n│ Adelie    │ Torgersen │              NULL │\n│ Gentoo    │ Biscoe    │              NULL │\n│ Adelie    │ Biscoe    │               172 │\n│ Adelie    │ Biscoe    │               174 │\n│ Adelie    │ Torgersen │               176 │\n│ Adelie    │ Dream     │               178 │\n│ Adelie    │ Dream     │               178 │\n│ Adelie    │ Dream     │               178 │\n│ Chinstrap │ Dream     │               178 │\n│ Adelie    │ Dream     │               179 │\n│ …         │ …         │                 … │\n└───────────┴───────────┴───────────────────┘\nYou can sort in descending order using the desc method of a column:\n&gt;&gt;&gt; penguins.order_by(penguins.flipper_length_mm.desc()).select(\n        \"species\", \"island\", \"flipper_length_mm\"\n    )\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ flipper_length_mm ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ int64             │\n├─────────┼───────────┼───────────────────┤\n│ Adelie  │ Torgersen │              NULL │\n│ Gentoo  │ Biscoe    │              NULL │\n│ Gentoo  │ Biscoe    │               231 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ …       │ …         │                 … │\n└─────────┴───────────┴───────────────────┘\nOr you can use ibis.desc\n&gt;&gt;&gt; penguins.order_by(ibis.desc(\"flipper_length_mm\")).select(\n        \"species\", \"island\", \"flipper_length_mm\"\n    )\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ flipper_length_mm ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ int64             │\n├─────────┼───────────┼───────────────────┤\n│ Adelie  │ Torgersen │              NULL │\n│ Gentoo  │ Biscoe    │              NULL │\n│ Gentoo  │ Biscoe    │               231 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ …       │ …         │                 … │\n└─────────┴───────────┴───────────────────┘\n\n\n\nIbis has several aggregate functions available to help summarize data.\nmean, max, min, count, sum (the list goes on).\nTo aggregate an entire column, call the corresponding method on that column.\n&gt;&gt;&gt; penguins.flipper_length_mm.mean()\n200.91520467836258\nYou can compute multiple aggregates at once using the aggregate method:\n&gt;&gt;&gt; penguins.aggregate(\n        [penguins.flipper_length_mm.mean(), penguins.bill_depth_mm.max()]\n    )\n┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ Mean(flipper_length_mm) ┃ Max(bill_depth_mm) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ float64                 │ float64            │\n├─────────────────────────┼────────────────────┤\n│              200.915205 │               21.5 │\n└─────────────────────────┴────────────────────┘\nBut aggregate really shines when it’s paired with group_by.\n\n\n\ngroup_by creates groupings of rows that have the same value for one or more columns.\nBut it doesn’t do much on its own – you can pair it with aggregate to get a result.\n&gt;&gt;&gt; penguins.group_by(\"species\").aggregate()\n┏━━━━━━━━━━━┓\n┃ species   ┃\n┡━━━━━━━━━━━┩\n│ string    │\n├───────────┤\n│ Adelie    │\n│ Gentoo    │\n│ Chinstrap │\n└───────────┘\nWe grouped by the species column and handed it an “empty” aggregate command. The result of that is a column of the unique values in the species column.\nIf we add a second column to the group_by, we’ll get each unique pairing of the values in those columns.\n&gt;&gt;&gt; penguins.group_by([\"species\", \"island\"]).aggregate()\n┏━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ species   ┃ island    ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string    │ string    │\n├───────────┼───────────┤\n│ Adelie    │ Torgersen │\n│ Adelie    │ Biscoe    │\n│ Adelie    │ Dream     │\n│ Gentoo    │ Biscoe    │\n│ Chinstrap │ Dream     │\n└───────────┴───────────┘\nNow, if we add an aggregation function to that, we start to really open things up.\n\n&gt;&gt;&gt; penguins.group_by([\"species\", \"island\"]).aggregate(penguins.bill_length_mm.mean())\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n┃ species   ┃ island    ┃ Mean(bill_length_mm) ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string    │ float64              │\n├───────────┼───────────┼──────────────────────┤\n│ Adelie    │ Torgersen │            38.950980 │\n│ Adelie    │ Biscoe    │            38.975000 │\n│ Adelie    │ Dream     │            38.501786 │\n│ Gentoo    │ Biscoe    │            47.504878 │\n│ Chinstrap │ Dream     │            48.833824 │\n└───────────┴───────────┴──────────────────────┘\nBy adding that mean to the aggregate, we now have a concise way to calculate aggregates over each of the distinct groups in the group_by. And we can calculate as many aggregates as we need.\n&gt;&gt;&gt; penguins.group_by([\"species\", \"island\"]).aggregate(\n        [penguins.bill_length_mm.mean(), penguins.flipper_length_mm.max()]\n    )\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ species   ┃ island    ┃ Mean(bill_length_mm) ┃ Max(flipper_length_mm) ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string    │ float64              │ int64                  │\n├───────────┼───────────┼──────────────────────┼────────────────────────┤\n│ Adelie    │ Torgersen │            38.950980 │                    210 │\n│ Adelie    │ Biscoe    │            38.975000 │                    203 │\n│ Adelie    │ Dream     │            38.501786 │                    208 │\n│ Gentoo    │ Biscoe    │            47.504878 │                    231 │\n│ Chinstrap │ Dream     │            48.833824 │                    212 │\n└───────────┴───────────┴──────────────────────┴────────────────────────┘\nIf we need more specific groups, we can add to the group_by.\n&gt;&gt;&gt; penguins.group_by([\"species\", \"island\", \"sex\"]).aggregate(\n        [penguins.bill_length_mm.mean(), penguins.flipper_length_mm.max()]\n    )\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ sex    ┃ Mean(bill_length_mm) ┃ Max(flipper_length_mm) ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ string │ float64              │ int64                  │\n├─────────┼───────────┼────────┼──────────────────────┼────────────────────────┤\n│ Adelie  │ Torgersen │ male   │            40.586957 │                    210 │\n│ Adelie  │ Torgersen │ female │            37.554167 │                    196 │\n│ Adelie  │ Torgersen │ NULL   │            37.925000 │                    193 │\n│ Adelie  │ Biscoe    │ female │            37.359091 │                    199 │\n│ Adelie  │ Biscoe    │ male   │            40.590909 │                    203 │\n│ Adelie  │ Dream     │ female │            36.911111 │                    202 │\n│ Adelie  │ Dream     │ male   │            40.071429 │                    208 │\n│ Adelie  │ Dream     │ NULL   │            37.500000 │                    179 │\n│ Gentoo  │ Biscoe    │ female │            45.563793 │                    222 │\n│ Gentoo  │ Biscoe    │ male   │            49.473770 │                    231 │\n│ …       │ …         │ …      │                    … │                      … │\n└─────────┴───────────┴────────┴──────────────────────┴────────────────────────┘\n\n\n\n\nWe’ve already chained some Ibis calls together. We used mutate to create a new column and then select to only view a subset of the new table. We were just chaining group_by with aggregate.\nThere’s nothing stopping us from putting all of these concepts together to ask questions of the data.\nHow about:\n\nWhat was the largest female penguin (by body mass) on each island in the year 2008?\n\n&gt;&gt;&gt; penguins.filter((penguins.sex == \"female\") & (penguins.year == 2008)).group_by(\n        [\"island\"]\n    ).aggregate(penguins.body_mass_g.max())\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃ island    ┃ Max(body_mass_g) ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ string    │ int64            │\n├───────────┼──────────────────┤\n│ Biscoe    │             5200 │\n│ Torgersen │             3800 │\n│ Dream     │             3900 │\n└───────────┴──────────────────┘\n\nWhat about the largest male penguin (by body mass) on each island for each year of data collection?\n\n&gt;&gt;&gt; penguins.filter(penguins.sex == \"male\").group_by([\"island\", \"year\"]).aggregate(\n        penguins.body_mass_g.max().name(\"max_body_mass\")\n    ).order_by([\"year\", \"max_body_mass\"])\n┏━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ island    ┃ year  ┃ max_body_mass ┃\n┡━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string    │ int64 │ int64         │\n├───────────┼───────┼───────────────┤\n│ Dream     │  2007 │          4650 │\n│ Torgersen │  2007 │          4675 │\n│ Biscoe    │  2007 │          6300 │\n│ Torgersen │  2008 │          4700 │\n│ Dream     │  2008 │          4800 │\n│ Biscoe    │  2008 │          6000 │\n│ Torgersen │  2009 │          4300 │\n│ Dream     │  2009 │          4475 │\n│ Biscoe    │  2009 │          6000 │\n└───────────┴───────┴───────────────┘\n\n\n\nThat’s all for this quick-start guide. If you want to learn more, check out the tutorial."
  },
  {
    "objectID": "tutorial/getting_started.html#install-ibis",
    "href": "tutorial/getting_started.html#install-ibis",
    "title": "Getting started with ibis",
    "section": "",
    "text": "This quick-start guide uses the DuckDB backend. You can check out the Install page for information on how to install other backends.\nshell title=\"Install Ibis using pip\" $ pip install 'ibis-framework[duckdb]'\nshell title=\"Install Ibis using conda\" $ conda install ibis-framework"
  },
  {
    "objectID": "tutorial/getting_started.html#download-a-database-file",
    "href": "tutorial/getting_started.html#download-a-database-file",
    "title": "Getting started with ibis",
    "section": "",
    "text": "Ibis can work with several file types, but at its core, it connects to existing databases and interacts with the data there. We’ll use a local database (DuckDB) to get the hang of this.1\npython title=\"Download an example dataset\" &gt;&gt;&gt; import urllib.request &gt;&gt;&gt; urllib.request.urlretrieve(         \"https://storage.googleapis.com/ibis-tutorial-data/palmer_penguins.ddb\",         \"palmer_penguins.ddb\",     )"
  },
  {
    "objectID": "tutorial/getting_started.html#connect-using-ibis",
    "href": "tutorial/getting_started.html#connect-using-ibis",
    "title": "Getting started with ibis",
    "section": "",
    "text": "python title=\"Connect to an existing database\" &gt;&gt;&gt; import ibis &gt;&gt;&gt; con = ibis.duckdb.connect(\"palmer_penguins.ddb\")\nWe’re connected! Let’s take a look at what tables are available.\n&gt;&gt;&gt; con.list_tables()\n['penguins']\nThere’s one table, called penguins. We can ask Ibis to give us an object that we can interact with.\n&gt;&gt;&gt; penguins = con.table(\"penguins\")\n&gt;&gt;&gt; penguins\nAlchemyTable: penguins\n  species           string\n  island            string\n  bill_length_mm    float64\n  bill_depth_mm     float64\n  flipper_length_mm int64\n  body_mass_g       int64\n  sex               string\n  year              int64\nIbis is lazily evaluated, so instead of seeing the data, we see the schema of the table, instead. To peek at the data, we can call head and then to_pandas to get the first few rows of the table as a pandas DataFrame.\n&gt;&gt;&gt; penguins.head().to_pandas()\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g     sex  year\n0  Adelie  Torgersen            39.1           18.7              181.0       3750.0    male  2007\n1  Adelie  Torgersen            39.5           17.4              186.0       3800.0  female  2007\n2  Adelie  Torgersen            40.3           18.0              195.0       3250.0  female  2007\n3  Adelie  Torgersen             NaN            NaN                NaN          NaN    None  2007\n4  Adelie  Torgersen            36.7           19.3              193.0       3450.0  female  2007\nto_pandas takes the existing lazy table expression and evaluates it. If we leave it off, you’ll see the Ibis representation of the table expression that to_pandas will evaluate (when you’re ready!).\n&gt;&gt;&gt; penguins.head()\nr0 := AlchemyTable: penguins\n  species           string\n  island            string\n  bill_length_mm    float64\n  bill_depth_mm     float64\n  flipper_length_mm int64\n  body_mass_g       int64\n  sex               string\n  year              int64\n\nLimit[r0, n=5]\n!!! note “Results in pandas DataFrame”\nIbis returns results as a pandas DataFrame using `to_pandas`, but isn't using pandas to\nperform any of the computation. The query is executed by the backend (DuckDB in\nthis case). Only when `to_pandas` is called does Ibis then pull back the results\nand convert them into a DataFrame."
  },
  {
    "objectID": "tutorial/getting_started.html#interactive-mode",
    "href": "tutorial/getting_started.html#interactive-mode",
    "title": "Getting started with ibis",
    "section": "",
    "text": "For the rest of this intro, we’ll turn on interactive mode, which partially executes queries to give users a preview of the results. There is a small difference in the way the output is formatted, but otherwise this is the same as calling to_pandas on the table expression with a limit of 10 result rows returned.\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; penguins.head()\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "tutorial/getting_started.html#common-operations",
    "href": "tutorial/getting_started.html#common-operations",
    "title": "Getting started with ibis",
    "section": "",
    "text": "Ibis has a collection of useful table methods to manipulate and query the data in a table (or tables).\n\n\nfilter allows you to select rows based on a condition or set of conditions.\nWe can filter so we only have penguins of the species Adelie:\n &gt;&gt;&gt; penguins.filter(penguins.species == \"Adelie\")\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\nOr filter for Adelie penguins that reside on the island of Torgersen:\n&gt;&gt;&gt; penguins.filter((penguins.island == \"Torgersen\") & (penguins.species == \"Adelie\"))\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\nYou can use any boolean comparison in a filter (although if you try to do something like use &lt; on a string, Ibis will yell at you).\n\n\n\nYour data analysis might not require all the columns present in a given table. select lets you pick out only those columns that you want to work with.\nTo select a column you can use the name of the column as a string:\n&gt;&gt;&gt; penguins.select(\"species\", \"island\", \"year\")\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ …       │ …         │     … │\n└─────────┴───────────┴───────┘\nOr you can use column objects directly (this can be convenient when paired with tab-completion):\n&gt;&gt;&gt; penguins.select(penguins.species, penguins.island, penguins.year)\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ …       │ …         │     … │\n└─────────┴───────────┴───────┘\nOr you can mix-and-match:\n\n&gt;&gt;&gt; penguins.select(\"species\", \"island\", penguins.year)\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ …       │ …         │     … │\n└─────────┴───────────┴───────┘\n\n\n\nmutate lets you add new columns to your table, derived from the values of existing columns.\n&gt;&gt;&gt; penguins.mutate(bill_length_cm=penguins.bill_length_mm / 10)\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ … │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │ … │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │ … │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │ … │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │        NULL │ NULL   │ … │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │ … │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │ … │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │ … │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │ … │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │ … │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │ … │\n│ …       │ …         │              … │             … │                 … │           … │ …      │ … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───┘\nNotice that the table is a little too wide to display all the columns now (depending on your screen-size). bill_length is now present in millimeters AND centimeters. Use a select to trim down the number of columns we’re looking at.\n&gt;&gt;&gt; penguins.mutate(bill_length_cm=penguins.bill_length_mm / 10).select(\n        \"species\",\n        \"island\",\n        \"bill_depth_mm\",\n        \"flipper_length_mm\",\n        \"body_mass_g\",\n        \"sex\",\n        \"year\",\n        \"bill_length_cm\",\n    )\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃ bill_length_cm ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ float64       │ int64             │ int64       │ string │ int64 │ float64        │\n├─────────┼───────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┼────────────────┤\n│ Adelie  │ Torgersen │          18.7 │               181 │        3750 │ male   │  2007 │           3.91 │\n│ Adelie  │ Torgersen │          17.4 │               186 │        3800 │ female │  2007 │           3.95 │\n│ Adelie  │ Torgersen │          18.0 │               195 │        3250 │ female │  2007 │           4.03 │\n│ Adelie  │ Torgersen │           nan │              NULL │        NULL │ NULL   │  2007 │            nan │\n│ Adelie  │ Torgersen │          19.3 │               193 │        3450 │ female │  2007 │           3.67 │\n│ Adelie  │ Torgersen │          20.6 │               190 │        3650 │ male   │  2007 │           3.93 │\n│ Adelie  │ Torgersen │          17.8 │               181 │        3625 │ female │  2007 │           3.89 │\n│ Adelie  │ Torgersen │          19.6 │               195 │        4675 │ male   │  2007 │           3.92 │\n│ Adelie  │ Torgersen │          18.1 │               193 │        3475 │ NULL   │  2007 │           3.41 │\n│ Adelie  │ Torgersen │          20.2 │               190 │        4250 │ NULL   │  2007 │           4.20 │\n│ …       │ …         │             … │                 … │           … │ …      │     … │              … │\n└─────────┴───────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┴────────────────┘\n\n\n\nTyping out ALL of the column names except one is a little annoying. Instead of doing that again, we can use a selector to quickly select or deselect groups of columns.\n&gt;&gt;&gt; from ibis import selectors as s\n\n&gt;&gt;&gt; penguins.mutate(bill_length_cm=penguins.bill_length_mm / 10).select(\n        ~s.matches(\"bill_length_mm\")\n        # match every column except `bill_length_mm`\n    )\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃ bill_length_cm ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ float64       │ int64             │ int64       │ string │ int64 │ float64        │\n├─────────┼───────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┼────────────────┤\n│ Adelie  │ Torgersen │          18.7 │               181 │        3750 │ male   │  2007 │           3.91 │\n│ Adelie  │ Torgersen │          17.4 │               186 │        3800 │ female │  2007 │           3.95 │\n│ Adelie  │ Torgersen │          18.0 │               195 │        3250 │ female │  2007 │           4.03 │\n│ Adelie  │ Torgersen │           nan │              NULL │        NULL │ NULL   │  2007 │            nan │\n│ Adelie  │ Torgersen │          19.3 │               193 │        3450 │ female │  2007 │           3.67 │\n│ Adelie  │ Torgersen │          20.6 │               190 │        3650 │ male   │  2007 │           3.93 │\n│ Adelie  │ Torgersen │          17.8 │               181 │        3625 │ female │  2007 │           3.89 │\n│ Adelie  │ Torgersen │          19.6 │               195 │        4675 │ male   │  2007 │           3.92 │\n│ Adelie  │ Torgersen │          18.1 │               193 │        3475 │ NULL   │  2007 │           3.41 │\n│ Adelie  │ Torgersen │          20.2 │               190 │        4250 │ NULL   │  2007 │           4.20 │\n│ …       │ …         │             … │                 … │           … │ …      │     … │              … │\n└─────────┴───────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┴────────────────┘\nYou can also use a selector alongside a column name.\n&gt;&gt;&gt; penguins.select(\"island\", s.numeric())\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━┓\n┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ year  ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━┩\n│ string    │ float64        │ float64       │ int64             │ int64       │ int64 │\n├───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼───────┤\n│ Torgersen │           39.1 │          18.7 │               181 │        3750 │  2007 │\n│ Torgersen │           39.5 │          17.4 │               186 │        3800 │  2007 │\n│ Torgersen │           40.3 │          18.0 │               195 │        3250 │  2007 │\n│ Torgersen │            nan │           nan │              NULL │        NULL │  2007 │\n│ Torgersen │           36.7 │          19.3 │               193 │        3450 │  2007 │\n│ Torgersen │           39.3 │          20.6 │               190 │        3650 │  2007 │\n│ Torgersen │           38.9 │          17.8 │               181 │        3625 │  2007 │\n│ Torgersen │           39.2 │          19.6 │               195 │        4675 │  2007 │\n│ Torgersen │           34.1 │          18.1 │               193 │        3475 │  2007 │\n│ Torgersen │           42.0 │          20.2 │               190 │        4250 │  2007 │\n│ …         │              … │             … │                 … │           … │     … │\n└───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴───────┘\nYou can read more about selectors in the docs!\n\n\n\norder_by arranges the values of one or more columns in ascending or descending order.\nBy default, ibis sorts in ascending order:\n&gt;&gt;&gt; penguins.order_by(penguins.flipper_length_mm).select(\n        \"species\", \"island\", \"flipper_length_mm\"\n    )\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ species   ┃ island    ┃ flipper_length_mm ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string    │ int64             │\n├───────────┼───────────┼───────────────────┤\n│ Adelie    │ Torgersen │              NULL │\n│ Gentoo    │ Biscoe    │              NULL │\n│ Adelie    │ Biscoe    │               172 │\n│ Adelie    │ Biscoe    │               174 │\n│ Adelie    │ Torgersen │               176 │\n│ Adelie    │ Dream     │               178 │\n│ Adelie    │ Dream     │               178 │\n│ Adelie    │ Dream     │               178 │\n│ Chinstrap │ Dream     │               178 │\n│ Adelie    │ Dream     │               179 │\n│ …         │ …         │                 … │\n└───────────┴───────────┴───────────────────┘\nYou can sort in descending order using the desc method of a column:\n&gt;&gt;&gt; penguins.order_by(penguins.flipper_length_mm.desc()).select(\n        \"species\", \"island\", \"flipper_length_mm\"\n    )\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ flipper_length_mm ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ int64             │\n├─────────┼───────────┼───────────────────┤\n│ Adelie  │ Torgersen │              NULL │\n│ Gentoo  │ Biscoe    │              NULL │\n│ Gentoo  │ Biscoe    │               231 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ …       │ …         │                 … │\n└─────────┴───────────┴───────────────────┘\nOr you can use ibis.desc\n&gt;&gt;&gt; penguins.order_by(ibis.desc(\"flipper_length_mm\")).select(\n        \"species\", \"island\", \"flipper_length_mm\"\n    )\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ flipper_length_mm ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ int64             │\n├─────────┼───────────┼───────────────────┤\n│ Adelie  │ Torgersen │              NULL │\n│ Gentoo  │ Biscoe    │              NULL │\n│ Gentoo  │ Biscoe    │               231 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ Gentoo  │ Biscoe    │               230 │\n│ …       │ …         │                 … │\n└─────────┴───────────┴───────────────────┘\n\n\n\nIbis has several aggregate functions available to help summarize data.\nmean, max, min, count, sum (the list goes on).\nTo aggregate an entire column, call the corresponding method on that column.\n&gt;&gt;&gt; penguins.flipper_length_mm.mean()\n200.91520467836258\nYou can compute multiple aggregates at once using the aggregate method:\n&gt;&gt;&gt; penguins.aggregate(\n        [penguins.flipper_length_mm.mean(), penguins.bill_depth_mm.max()]\n    )\n┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ Mean(flipper_length_mm) ┃ Max(bill_depth_mm) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ float64                 │ float64            │\n├─────────────────────────┼────────────────────┤\n│              200.915205 │               21.5 │\n└─────────────────────────┴────────────────────┘\nBut aggregate really shines when it’s paired with group_by.\n\n\n\ngroup_by creates groupings of rows that have the same value for one or more columns.\nBut it doesn’t do much on its own – you can pair it with aggregate to get a result.\n&gt;&gt;&gt; penguins.group_by(\"species\").aggregate()\n┏━━━━━━━━━━━┓\n┃ species   ┃\n┡━━━━━━━━━━━┩\n│ string    │\n├───────────┤\n│ Adelie    │\n│ Gentoo    │\n│ Chinstrap │\n└───────────┘\nWe grouped by the species column and handed it an “empty” aggregate command. The result of that is a column of the unique values in the species column.\nIf we add a second column to the group_by, we’ll get each unique pairing of the values in those columns.\n&gt;&gt;&gt; penguins.group_by([\"species\", \"island\"]).aggregate()\n┏━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ species   ┃ island    ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string    │ string    │\n├───────────┼───────────┤\n│ Adelie    │ Torgersen │\n│ Adelie    │ Biscoe    │\n│ Adelie    │ Dream     │\n│ Gentoo    │ Biscoe    │\n│ Chinstrap │ Dream     │\n└───────────┴───────────┘\nNow, if we add an aggregation function to that, we start to really open things up.\n\n&gt;&gt;&gt; penguins.group_by([\"species\", \"island\"]).aggregate(penguins.bill_length_mm.mean())\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n┃ species   ┃ island    ┃ Mean(bill_length_mm) ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string    │ float64              │\n├───────────┼───────────┼──────────────────────┤\n│ Adelie    │ Torgersen │            38.950980 │\n│ Adelie    │ Biscoe    │            38.975000 │\n│ Adelie    │ Dream     │            38.501786 │\n│ Gentoo    │ Biscoe    │            47.504878 │\n│ Chinstrap │ Dream     │            48.833824 │\n└───────────┴───────────┴──────────────────────┘\nBy adding that mean to the aggregate, we now have a concise way to calculate aggregates over each of the distinct groups in the group_by. And we can calculate as many aggregates as we need.\n&gt;&gt;&gt; penguins.group_by([\"species\", \"island\"]).aggregate(\n        [penguins.bill_length_mm.mean(), penguins.flipper_length_mm.max()]\n    )\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ species   ┃ island    ┃ Mean(bill_length_mm) ┃ Max(flipper_length_mm) ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string    │ float64              │ int64                  │\n├───────────┼───────────┼──────────────────────┼────────────────────────┤\n│ Adelie    │ Torgersen │            38.950980 │                    210 │\n│ Adelie    │ Biscoe    │            38.975000 │                    203 │\n│ Adelie    │ Dream     │            38.501786 │                    208 │\n│ Gentoo    │ Biscoe    │            47.504878 │                    231 │\n│ Chinstrap │ Dream     │            48.833824 │                    212 │\n└───────────┴───────────┴──────────────────────┴────────────────────────┘\nIf we need more specific groups, we can add to the group_by.\n&gt;&gt;&gt; penguins.group_by([\"species\", \"island\", \"sex\"]).aggregate(\n        [penguins.bill_length_mm.mean(), penguins.flipper_length_mm.max()]\n    )\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ sex    ┃ Mean(bill_length_mm) ┃ Max(flipper_length_mm) ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ string │ float64              │ int64                  │\n├─────────┼───────────┼────────┼──────────────────────┼────────────────────────┤\n│ Adelie  │ Torgersen │ male   │            40.586957 │                    210 │\n│ Adelie  │ Torgersen │ female │            37.554167 │                    196 │\n│ Adelie  │ Torgersen │ NULL   │            37.925000 │                    193 │\n│ Adelie  │ Biscoe    │ female │            37.359091 │                    199 │\n│ Adelie  │ Biscoe    │ male   │            40.590909 │                    203 │\n│ Adelie  │ Dream     │ female │            36.911111 │                    202 │\n│ Adelie  │ Dream     │ male   │            40.071429 │                    208 │\n│ Adelie  │ Dream     │ NULL   │            37.500000 │                    179 │\n│ Gentoo  │ Biscoe    │ female │            45.563793 │                    222 │\n│ Gentoo  │ Biscoe    │ male   │            49.473770 │                    231 │\n│ …       │ …         │ …      │                    … │                      … │\n└─────────┴───────────┴────────┴──────────────────────┴────────────────────────┘"
  },
  {
    "objectID": "tutorial/getting_started.html#chaining-it-all-together",
    "href": "tutorial/getting_started.html#chaining-it-all-together",
    "title": "Getting started with ibis",
    "section": "",
    "text": "We’ve already chained some Ibis calls together. We used mutate to create a new column and then select to only view a subset of the new table. We were just chaining group_by with aggregate.\nThere’s nothing stopping us from putting all of these concepts together to ask questions of the data.\nHow about:\n\nWhat was the largest female penguin (by body mass) on each island in the year 2008?\n\n&gt;&gt;&gt; penguins.filter((penguins.sex == \"female\") & (penguins.year == 2008)).group_by(\n        [\"island\"]\n    ).aggregate(penguins.body_mass_g.max())\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃ island    ┃ Max(body_mass_g) ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ string    │ int64            │\n├───────────┼──────────────────┤\n│ Biscoe    │             5200 │\n│ Torgersen │             3800 │\n│ Dream     │             3900 │\n└───────────┴──────────────────┘\n\nWhat about the largest male penguin (by body mass) on each island for each year of data collection?\n\n&gt;&gt;&gt; penguins.filter(penguins.sex == \"male\").group_by([\"island\", \"year\"]).aggregate(\n        penguins.body_mass_g.max().name(\"max_body_mass\")\n    ).order_by([\"year\", \"max_body_mass\"])\n┏━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ island    ┃ year  ┃ max_body_mass ┃\n┡━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string    │ int64 │ int64         │\n├───────────┼───────┼───────────────┤\n│ Dream     │  2007 │          4650 │\n│ Torgersen │  2007 │          4675 │\n│ Biscoe    │  2007 │          6300 │\n│ Torgersen │  2008 │          4700 │\n│ Dream     │  2008 │          4800 │\n│ Biscoe    │  2008 │          6000 │\n│ Torgersen │  2009 │          4300 │\n│ Dream     │  2009 │          4475 │\n│ Biscoe    │  2009 │          6000 │\n└───────────┴───────┴───────────────┘"
  },
  {
    "objectID": "tutorial/getting_started.html#learn-more",
    "href": "tutorial/getting_started.html#learn-more",
    "title": "Getting started with ibis",
    "section": "",
    "text": "That’s all for this quick-start guide. If you want to learn more, check out the tutorial."
  },
  {
    "objectID": "tutorial/getting_started.html#footnotes",
    "href": "tutorial/getting_started.html#footnotes",
    "title": "Getting started with ibis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/. doi: 10.5281/zenodo.3960218.↩︎"
  },
  {
    "objectID": "tutorial/ibis-for-pandas-users.html",
    "href": "tutorial/ibis-for-pandas-users.html",
    "title": "Install",
    "section": "",
    "text": "If you don’t have ibis installed, you can install it from:"
  },
  {
    "objectID": "tutorial/ibis-for-pandas-users.html#data-types",
    "href": "tutorial/ibis-for-pandas-users.html#data-types",
    "title": "Install",
    "section": "Data types",
    "text": "Data types\nThe data types of columns in pandas are accessed using the dtypes attribute. This returns a Series object.\n\ndf.dtypes\n\nIn Ibis, you use the schema method which returns an ibis.Schema object.\n\nt.schema()\n\nIt is possible to convert the schema information to pandas data types using the to_pandas method, if needed.\n\nt.schema().to_pandas()"
  },
  {
    "objectID": "tutorial/ibis-for-pandas-users.html#table-layout",
    "href": "tutorial/ibis-for-pandas-users.html#table-layout",
    "title": "Install",
    "section": "Table layout",
    "text": "Table layout\nIn pandas, the layout of the table is contained in the shape attribute which contains the number of rows and number of columns in a tuple. The number of columns in an Ibis table can be gotten from the length of the schema.\n\nlen(t.schema())\n\nTo get the number of rows of a table, you use the count method.\n\nt.count()\n\nTo mimic pandas’ behavior, you would use the following code. Note that you need to use the to_pandas method after count to evaluate the expression returned by count.\n\n(t.count().to_pandas(), len(t.schema()))\n\n\ndf.shape"
  },
  {
    "objectID": "tutorial/ibis-for-pandas-users.html#subsetting-columns",
    "href": "tutorial/ibis-for-pandas-users.html#subsetting-columns",
    "title": "Install",
    "section": "Subsetting columns",
    "text": "Subsetting columns\nSelecting columns is very similar to in pandas. In fact, you can use the same syntax.\n\nt[['one', 'two']]\n\nHowever, since row-level indexing is not supported in Ibis, the inner list is not necessary.\n\nt['one', 'two']"
  },
  {
    "objectID": "tutorial/ibis-for-pandas-users.html#selecting-columns",
    "href": "tutorial/ibis-for-pandas-users.html#selecting-columns",
    "title": "Install",
    "section": "Selecting columns",
    "text": "Selecting columns\nSelecting columns is done using the same syntax as in pandas DataFrames. You can use either the indexing syntax or attribute syntax.\n\nt['one']\n\nor:\n\nt.one"
  },
  {
    "objectID": "tutorial/ibis-for-pandas-users.html#adding-removing-and-modifying-columns",
    "href": "tutorial/ibis-for-pandas-users.html#adding-removing-and-modifying-columns",
    "title": "Install",
    "section": "Adding, removing, and modifying columns",
    "text": "Adding, removing, and modifying columns\nModifying the columns of an Ibis table is a bit different than doing the same operations in a pandas DataFrame. This is primarily due to the fact that in-place operations are not supported on Ibis tables. Each time you do a column modification to a table, a new table expression is returned.\n\nAdding columns\nAdding columns is done through the mutate method.\n\nmutated = t.mutate(new_col=t.three * 2)\nmutated\n\nNotice that the original table object remains unchanged. Only the mutated object that was returned contains the new column.\n\nt\n\nIt is also possible to create a column in isolation. This is similar to a Series in pandas. Note that the name of the column by default is a representation of the expression:\n\nunnamed = t.three * 2\nunnamed\n\nTo get a version with a specific name, you can use the name method:\n\nnew_col = unnamed.name(\"new_col\")\nnew_col\n\nYou can then add this column to the table using a projection.\n\nproj = t['one', 'two', new_col]\nproj\n\n\n\nRemoving columns\nRemoving a column is done using the drop method.\n\nt.columns\n\n\nsubset = t.drop('one', 'two')\nsubset.columns\n\nIt is also possible to drop columns by selecting the columns you want to remain.\n\nsubset = t['two', 'three']\nsubset.columns\n\n\n\nModifying columns\nReplacing existing columns is done using the mutate method just like adding columns. You simply add a column of the same name to replace it.\n\nt\n\n\nmutated = t.mutate(two=t.two * 2)\nmutated\n\n\n\nRenaming columns\nIn addition to replacing columns, you can simply rename them as well. This is done with the relabel method which takes a dictionary containing the name mappings.\n\nrelabeled = t.relabel(\n    dict(\n        one='a',\n        two='b',\n    )\n)\nrelabeled"
  },
  {
    "objectID": "tutorial/ibis-for-pandas-users.html#selecting-rows",
    "href": "tutorial/ibis-for-pandas-users.html#selecting-rows",
    "title": "Install",
    "section": "Selecting rows",
    "text": "Selecting rows\nThere are several methods that can be used to select rows of data in various ways. These are described in the sections below. We’ll use the Palmer Penguins\\(^1\\) dataset to investigate! Ibis has several built-in example datasets that you can access using the ibis.examples module.\n\\(^1\\): Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/. doi: 10.5281/zenodo.3960218.\n\npenguins = ibis.examples.penguins.fetch()\n\n\npenguins\n\n\nHead, tail and limit\nThe head method works the same ways as in pandas. Note that some Ibis backends may not have an inherent ordering of their rows and using head may not return deterministic results. In those cases, you can use sorting before calling head to ensure a stable result.\n\npenguins.head(5)\n\nHowever, the tail method is not implemented since it is not supported in all databases. It is possible to emulate the tail method if you use sorting in your table to do a reverse sort then use the head method to retrieve the “top” rows.\nAnother way to limit the number of retrieved rows is using the limit method. The following will return the same result as head(5). This is often used in conjunction with other filtering techniques that we will cover later.\n\npenguins.limit(5)\n\n\n\nFiltering rows\nIn addition to simply limiting the number of rows that are returned, it is possible to filter the rows using expressions. Expressions are constructed very similarly to the way they are in pandas. Ibis expressions are constructed from operations on columns in a table which return a boolean result. This result is then used to filter the table.\n\nexpr = penguins.bill_length_mm &gt; 37.0\nexpr\n\nWe can evaluate the value counts to see how many rows we will expect to get back after filtering.\n\nexpr.value_counts()\n\nNow we apply the filter to the table. Since there are 6 True values in the expression, we should get 6 rows back.\n\nfiltered = penguins[expr]\nfiltered\n\nOf course, the filtering expression can be applied inline as well.\n\nfiltered = penguins[penguins.bill_length_mm &gt; 37.0]\nfiltered\n\nMultiple filtering expressions can be combined into a single expression or chained onto existing table expressions.\n\nfiltered = penguins[(penguins.bill_length_mm &gt; 37.0) & (penguins.bill_depth_mm &gt; 18.0)]\nfiltered\n\nThe code above will return the same rows as the code below.\n\nfiltered = penguins[penguins.bill_length_mm &gt; 37.0][penguins.bill_depth_mm &gt; 18.0]\nfiltered\n\nAggregation has not been discussed yet, but aggregate values can be used in expressions to return things such as all of the rows in a data set where the value in a column is greater than the mean.\n\nfiltered = penguins[penguins.bill_length_mm &gt; penguins.bill_length_mm.mean()]\nfiltered\n\n\n\nModifying rows\nSometimes you want to modify the values in a column based on some condition. In pandas, you would do something like df.loc[condition] = new_value. In Ibis though, remember that all expressions are immutable, so you need to create a new table expression with the modified values. You do this using the ifelse method on boolean columns:\n\nlong_billed_penguins = penguins.bill_length_mm &gt; 37.0\nspecies_modified = long_billed_penguins.ifelse('wide', penguins.species)\npenguins.mutate(species_modified=species_modified)"
  },
  {
    "objectID": "tutorial/ibis-for-pandas-users.html#sorting-rows",
    "href": "tutorial/ibis-for-pandas-users.html#sorting-rows",
    "title": "Install",
    "section": "Sorting rows",
    "text": "Sorting rows\nSorting rows in Ibis uses a somewhat different API than in pandas. In pandas, you would use the sort_values method to order rows by values in specified columns. Ibis uses a method called order_by. To specify ascending or descending orders, pandas uses an ascending= argument to sort_values that indicates the order for each sorting column. Ibis allows you to tag the column name in the order_by list as ascending or descending by wrapping it with ibis.asc or ibis.desc.\nFirst, let’s ask Ibis for a pandas DataFrame version of the penguin data:\n\ndf = penguins.to_pandas()\n\nHere is an example of sorting a DataFrame using two sort keys. One key is sorting in ascending order and the other is in descending order.\n\ndf.sort_values(\n    ['bill_length_mm', 'bill_depth_mm'], ascending=[True, False], na_position=\"first\"\n).head(5)\n\nThe same operation in Ibis would look like the following. Note that the index values of the resulting DataFrame start from zero and count up, whereas in the example above, they retain their original index value. This is simply due to the fact that rows in tables don’t necessarily have a stable index in database backends, so the index is just generated on the result.\n\nsorted = penguins.order_by(['bill_length_mm', ibis.desc('bill_depth_mm')]).head(5)\nsorted"
  },
  {
    "objectID": "tutorial/ibis-for-pandas-users.html#aggregation",
    "href": "tutorial/ibis-for-pandas-users.html#aggregation",
    "title": "Install",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation in pandas is typically done by computing columns based on an aggregate function.\n\nstats = [df.bill_depth_mm.sum(), df.bill_length_mm.mean()]\npd.DataFrame([stats], columns=['total_bill_depth', 'avg.bill_length'])\n\nIn Ibis, you construct aggregate expressions then apply them to the table using the aggregate method.\n\nstats = [\n    penguins.bill_depth_mm.sum().name('total_bill_width'),\n    penguins.bill_length_mm.mean().name('avg_bill_length'),\n]\nagged = penguins.aggregate(stats)\nagged\n\nYou can also combine both operations into one and pass the aggregate expressions using keyword parameters.\n\nagged = penguins.aggregate(\n    total_bill_depth=penguins.bill_depth_mm.sum(),\n    avg_bill_length=penguins.bill_length_mm.mean(),\n)\nagged"
  },
  {
    "objectID": "tutorial/ibis-for-pandas-users.html#group-by",
    "href": "tutorial/ibis-for-pandas-users.html#group-by",
    "title": "Install",
    "section": "Group by",
    "text": "Group by\nUsing a similar API as above, aggregations can also be done across groupings using the by= parameter.\n\nagged = penguins.aggregate(\n    by='species',\n    total_bill_depth=penguins.bill_depth_mm.sum(),\n    avg_bill_length=penguins.bill_length_mm.mean(),\n)\nagged\n\nAlternatively, by groups can be computed using a grouped table.\n\nagged = penguins.group_by('species').aggregate(\n    total_bill_depth=penguins.bill_depth_mm.sum(),\n    avg_bill_length=penguins.bill_length_mm.mean(),\n)\nagged\n\nYou can group over multiple columns too, and rename them if you want.\nIf you only need to aggregate over a single column, then you don’t need to use the .aggregate() method.\n\npenguins.group_by([\"species\", \"sex\"], location=\"island\").body_mass_g.approx_median()\n\nInstead of aggregating after a group by, you can also transform the table so that the output table has the same number of rows as the input table. This is analogous to the groupby().transform() pattern in pandas. You can pass complex expressions to compute per-group:\n\n# Calculate how much the mass of each penguin deviates from the mean\npenguins.group_by([\"species\", \"sex\"]).mutate(\n    # This column isn't needed, but it makes it easier to see what's going on\n    mass_mean=penguins.body_mass_g.mean(),\n    mass_deviation=penguins.body_mass_g - penguins.body_mass_g.mean(),\n)"
  },
  {
    "objectID": "tutorial/ibis-for-pandas-users.html#null-values",
    "href": "tutorial/ibis-for-pandas-users.html#null-values",
    "title": "Install",
    "section": "NULL values",
    "text": "NULL values\nIbis has first-class support for NULL values. In pandas and numpy, NULL values are represented by NaN. This can be confusing when working with numeric data, since NaN is also a valid floating point value (along with +/-inf).\nIn Ibis, we try to be more precise: All data types are nullable, and we use ibis.NA to represent NULL values, and all datatypes have a .isnull() method. For floating point values, we use different values for NaN and +/-inf, and there are the additional methods .isnan() and .isinf().\n\nDropping rows with NULLs\nBoth pandas and Ibis allow you to drop rows from a table based on whether a set of columns contains a NULL value. This method is called dropna in both packages. The common set of parameters in the two are subset= and how=. The subset= parameter indicates which columns to inspect for NULL values. The how= parameter specifies whether ‘any’ or ‘all’ of the specified columns must be NULL in order for the row to be dropped.\n\nno_null_peng = penguins.dropna(['bill_depth_mm', 'bill_length_mm'], how='any')\n\n\n\nFilling NULL values\nBoth pandas and Ibis allow you to fill NULL values in a table. In Ibis, the replacement value can only be a scalar value of a dictionary of values. If it is a dictionary, the keys of the dictionary specify the column name for the value to apply to.\n\nno_null_peng = penguins.fillna(dict(bill_depth_mm=0, bill_length_mm=0))\n\n\n\nReplacing NULLs\nBoth pandas and Ibis have fillna methods which allow you to specify a replacement value for NULL values.\n\nbill_length_no_nulls = penguins.bill_length_mm.fillna(0)"
  },
  {
    "objectID": "tutorial/ibis-for-pandas-users.html#type-casts",
    "href": "tutorial/ibis-for-pandas-users.html#type-casts",
    "title": "Install",
    "section": "Type casts",
    "text": "Type casts\nType casting in pandas is done using the astype method on columns.\n\ndf.bill_depth_mm.astype(str)\n\nIn Ibis, you cast the column type using the cast method.\n\npenguins.bill_depth_mm.cast('int')\n\nCasted columns can be assigned back to the table using the mutate method described earlier.\n\ncasted = penguins.mutate(\n    bill_depth_mm=penguins.bill_depth_mm.cast('int'),\n    bill_length_mm=penguins.bill_length_mm.cast('int'),\n)\ncasted.schema()"
  },
  {
    "objectID": "tutorial/ibis-for-pandas-users.html#set-membership",
    "href": "tutorial/ibis-for-pandas-users.html#set-membership",
    "title": "Install",
    "section": "Set membership",
    "text": "Set membership\npandas set membership uses the in and not in operators such as 'a' in df.species. Ibis uses isin and notin methods. In addition to testing membership in a set, these methods allow you to specify an else case to assign a value when the value isn’t in the set.\n\npenguins.species.value_counts()\n\n\nrefined = penguins.species.isin(['Adelie', 'Chinstrap'])\nrefined.value_counts()"
  },
  {
    "objectID": "tutorial/ibis-for-pandas-users.html#merging-tables",
    "href": "tutorial/ibis-for-pandas-users.html#merging-tables",
    "title": "Install",
    "section": "Merging tables",
    "text": "Merging tables\nWhile pandas uses the merge method to combine data from multiple DataFrames, Ibis uses the join method. They both have similar capabilities.\nThe biggest difference between Ibis’ join method and pandas’ merge method is that pandas only accepts column names or index levels to join on, whereas Ibis can merge on expressions.\nHere are some examples of merging using pandas.\n\ndf_left = pd.DataFrame(\n    [\n        ['a', 1, 2],\n        ['b', 3, 4],\n        ['c', 4, 6],\n    ],\n    columns=['name', 'x', 'y'],\n)\n\ndf_right = pd.DataFrame(\n    [\n        ['a', 100, 200],\n        ['m', 300, 400],\n        ['n', 400, 600],\n    ],\n    columns=['name', 'x_100', 'y_100'],\n)\n\n\ndf_left.merge(df_right, on='name')\n\n\ndf_left.merge(df_right, on='name', how='outer')\n\nWe can now convert DataFrames to Ibis tables to do joins.\n\nt_left = ibis.memtable(df_left, name=\"t_left\")\nt_right = ibis.memtable(df_right, name='t_right')\n\n\nt_left.join(t_right, t_left.name == t_right.name)\n\nBelow is an outer join where missing values are filled with NaN.\n\nt_left.join(t_right, t_left.name == t_right.name, how='outer')"
  },
  {
    "objectID": "tutorial/ibis-for-pandas-users.html#concatenating-tables",
    "href": "tutorial/ibis-for-pandas-users.html#concatenating-tables",
    "title": "Install",
    "section": "Concatenating tables",
    "text": "Concatenating tables\nConcatenating DataFrames in pandas is done with the concat top-level function. It takes multiple DataFrames and concatenates the rows of one DataFrame to the next. If the columns are mis-matched, it extends the list of columns to include the full set of columns and inserts NaNs and Nones into the missing values.\nConcatenating tables in Ibis can only be done on tables with matching schemas. The concatenation is done using the top-level union function or the union method on a table.\nWe’ll demonstrate a pandas concat first.\n\ndf_1 = pd.DataFrame(\n    [\n        ['a', 1, 2],\n        ['b', 3, 4],\n        ['c', 4, 6],\n    ],\n    columns=['name', 'x', 'y'],\n)\n\ndf_2 = pd.DataFrame(\n    [\n        ['a', 100, 200],\n        ['m', 300, 400],\n        ['n', 400, 600],\n    ],\n    columns=['name', 'x', 'y'],\n)\n\n\npd.concat([df_1, df_2])\n\nNow we can convert the DataFrames to Ibis tables and combine the tables using a union.\n\nt_1 = ibis.memtable(df_1, name='t_1')\nt_2 = ibis.memtable(df_2, name='t_2')\n\n\nunioned = ibis.union(t_1, t_2)\nunioned"
  },
  {
    "objectID": "how_to/topk.html",
    "href": "how_to/topk.html",
    "title": "Compute the top K records",
    "section": "",
    "text": "Compute the top K records\n\nHere we use the [topk][ibis.expr.types.Column.topk] method to compute the top 5 customers for some generated TPC-H data by: \n\ncount (the default)\nsum of order totals\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; con = ibis.duckdb.connect()  # in-memory duckdb\n&gt;&gt;&gt; con.raw_sql(\"CALL dbgen(sf=0.1)\")\n&gt;&gt;&gt; orders = con.table(\"orders\")\n&gt;&gt;&gt; orders.o_custkey.topk(5)  # top 5 most frequent customers\n┏━━━━━━━━━━━┳━━━━━━━┓\n┃ o_custkey ┃ count ┃\n┡━━━━━━━━━━━╇━━━━━━━┩\n│ !int32    │ int64 │\n├───────────┼───────┤\n│      8761 │    36 │\n│     11998 │    36 │\n│      3151 │    35 │\n│      8362 │    35 │\n│       388 │    35 │\n└───────────┴───────┘\n&gt;&gt;&gt; topk = orders.o_custkey.topk(5, by=orders.o_totalprice.sum())  # top 5 largest spending customers\n&gt;&gt;&gt; topk\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ o_custkey ┃ sum            ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ !int32    │ decimal(38, 2) │\n├───────────┼────────────────┤\n│      8362 │     5793605.05 │\n│      6958 │     5370682.19 │\n│      9454 │     5354381.81 │\n│       346 │     5323350.43 │\n│     10354 │     5227957.24 │\n└───────────┴────────────────┘\n\nYou can also use [topk][ibis.expr.types.Column.topk] to retrieve the rows from the original table that match the key used, in this case o_custkey. This is done with a left semi join: \n&gt;&gt;&gt; orders.semi_join(topk, \"o_custkey\")\n┏━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ o_orderkey ┃ o_custkey ┃ o_orderstatus ┃ o_totalprice    ┃ o_orderdate ┃ o_orderpriority ┃ o_clerk         ┃ o_shippriority ┃ o_comment                                                    ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ !int32     │ !int32    │ !string       │ !decimal(15, 2) │ !date       │ !string         │ !string         │ !int32         │ !string                                                      │\n├────────────┼───────────┼───────────────┼─────────────────┼─────────────┼─────────────────┼─────────────────┼────────────────┼──────────────────────────────────────────────────────────────┤\n│       4000 │      6958 │ F             │       115722.85 │ 1992-01-04  │ 5-LOW           │ Clerk#000000339 │              0 │ le carefully closely even pinto beans. regular, ironic foxe… │\n│      14402 │      8362 │ F             │       131557.79 │ 1993-10-15  │ 3-MEDIUM        │ Clerk#000000672 │              0 │ azzle slyly. carefully regular instructions affix carefully… │\n│      14784 │     10354 │ F             │       216307.34 │ 1992-03-15  │ 3-MEDIUM        │ Clerk#000000479 │              0 │ lyly final theodoli                                          │\n│      17415 │     10354 │ O             │       110427.40 │ 1996-09-18  │ 2-HIGH          │ Clerk#000000148 │              0 │ . furiously even asymptotes wake carefully according to t    │\n│      17760 │      9454 │ F             │       167249.60 │ 1992-06-05  │ 4-NOT SPECIFIED │ Clerk#000000093 │              0 │ uriously final pinto beans wake furiously                    │\n│      18853 │      9454 │ F             │       163677.19 │ 1993-01-18  │ 1-URGENT        │ Clerk#000000046 │              0 │ sts. courts haggle furiously. even, enticing depo            │\n│      21317 │      8362 │ P             │       267386.98 │ 1995-04-10  │ 5-LOW           │ Clerk#000000737 │              0 │ Tiresias. accounts a                                         │\n│      23138 │      8362 │ O             │       174882.01 │ 1997-07-23  │ 1-URGENT        │ Clerk#000000253 │              0 │ uctions integrate carefully regular pinto beans. silent acc… │\n│      23972 │     10354 │ F             │       129646.66 │ 1993-08-17  │ 4-NOT SPECIFIED │ Clerk#000001000 │              0 │ s. blithely final packages sleep quickly idle pearls. even,… │\n│      24064 │       346 │ F             │       147095.22 │ 1993-07-26  │ 3-MEDIUM        │ Clerk#000000020 │              0 │ ithely final foxes. furiously final instructi                │\n│          … │         … │ …             │               … │ …           │ …               │ …               │              … │ …                                                            │\n└────────────┴───────────┴───────────────┴─────────────────┴─────────────┴─────────────────┴─────────────────┴────────────────┴──────────────────────────────────────────────────────────────┘"
  },
  {
    "objectID": "how_to/extending/reduction.html",
    "href": "how_to/extending/reduction.html",
    "title": "Add a reduction operation",
    "section": "",
    "text": "This notebook will show you how to add a new reduction operation last_date to the existing backend SQLite.\nA reduction operation is a function that maps \\(N\\) rows to 1 row, for example the sum function."
  },
  {
    "objectID": "how_to/extending/reduction.html#description",
    "href": "how_to/extending/reduction.html#description",
    "title": "Add a reduction operation",
    "section": "Description",
    "text": "Description\nWe’re going to add a last_date function to ibis. last_date simply returns the latest date of a list of dates."
  },
  {
    "objectID": "how_to/extending/reduction.html#step-1-define-the-operation",
    "href": "how_to/extending/reduction.html#step-1-define-the-operation",
    "title": "Add a reduction operation",
    "section": "Step 1: Define the Operation",
    "text": "Step 1: Define the Operation\nLet’s define the last_date operation as a function that takes any date column as input and returns a date:\nimport datetime\nimport typing\n\ndef last_date(dates: typing.List[datetime.date]) -&gt; datetime.date:\n    \"\"\"Latest date\"\"\"\n\nimport ibis.expr.datatypes as dt\nimport ibis.expr.rules as rlz\nfrom ibis.expr.operations import Reduction\n\n\nclass LastDate(Reduction):\n    arg = rlz.column(rlz.date)\n    where = rlz.optional(rlz.boolean)\n\n    output_dtype = rlz.dtype_like('arg')\n    output_shape = rlz.Shape.SCALAR\n\nWe just defined a LastDate class that takes one date column as input, and returns a scalar output of the same type as the input. This matches both the requirements of a reduction and the specifics of the function that we want to implement.\nNote: It is very important that you write the correct argument rules and output type here. The expression will not work otherwise."
  },
  {
    "objectID": "how_to/extending/reduction.html#step-2-define-the-api",
    "href": "how_to/extending/reduction.html#step-2-define-the-api",
    "title": "Add a reduction operation",
    "section": "Step 2: Define the API",
    "text": "Step 2: Define the API\nBecause every reduction in ibis has the ability to filter out values during aggregation (a typical feature in databases and analytics tools), to make an expression out of LastDate we need to pass an additional argument: where to our LastDate constructor.\n\nfrom ibis.expr.types import (\n    DateColumn,  # not DateValue! reductions are only valid on columns\n)\n\n\ndef last_date(date_column, where=None):\n    return LastDate(date_column, where=where).to_expr()\n\n\nDateColumn.last_date = last_date"
  },
  {
    "objectID": "how_to/extending/reduction.html#interlude-create-some-expressions-using-last_date",
    "href": "how_to/extending/reduction.html#interlude-create-some-expressions-using-last_date",
    "title": "Add a reduction operation",
    "section": "Interlude: Create some expressions using last_date",
    "text": "Interlude: Create some expressions using last_date\n\nimport ibis\n\n\npeople = ibis.table(\n    dict(name='string', country='string', date_of_birth='date'), name='people'\n)\n\n\npeople.date_of_birth.last_date()\n\nr0 := UnboundTable: people\n  name          string\n  country       string\n  date_of_birth date\n\nLastDate(date_of_birth): LastDate(r0.date_of_birth)\n\n\n\npeople.date_of_birth.last_date(people.country == 'Indonesia')\n\nr0 := UnboundTable: people\n  name          string\n  country       string\n  date_of_birth date\n\nLastDate(date_of_birth, Equals(country, 'Indonesia')): LastDate(r0.date_of_birth, where=r0.country == 'Indonesia')"
  },
  {
    "objectID": "how_to/extending/reduction.html#step-3-turn-the-expression-into-sql",
    "href": "how_to/extending/reduction.html#step-3-turn-the-expression-into-sql",
    "title": "Add a reduction operation",
    "section": "Step 3: Turn the Expression into SQL",
    "text": "Step 3: Turn the Expression into SQL\n\nimport sqlalchemy as sa\n\n\n@ibis.sqlite.add_operation(LastDate)\ndef _last_date(translator, expr):\n    # pull out the arguments to the expression\n    arg, where = expr.op().args\n\n    # compile the argument\n    compiled_arg = translator.translate(arg)\n\n    # call the appropriate SQLite function (`max` for the latest/maximum date)\n    agg = sa.func.max(compiled_arg)\n\n    # handle a non-None filter clause\n    if where is not None:\n        return agg.filter(translator.translate(where))\n    return agg"
  },
  {
    "objectID": "how_to/extending/reduction.html#step-4-putting-it-all-together",
    "href": "how_to/extending/reduction.html#step-4-putting-it-all-together",
    "title": "Add a reduction operation",
    "section": "Step 4: Putting it all Together",
    "text": "Step 4: Putting it all Together\n\n!curl -LsS -o geography.db 'https://storage.googleapis.com/ibis-tutorial-data/geography.db'\n\n\nimport os\nimport tempfile\n\nimport ibis\n\ndb_fname = 'geography.db'\n\ncon = ibis.sqlite.connect(db_fname)\n\n\nCreate and execute a bitwise_and expression\n\nindependence = con.table('independence')\nindependence\n\n\nDatabaseTable: independence\n  country_code      string\n  independence_date date\n  independence_from string\n\n\n\nLast country to gain independence in our database:\n\nexpr = independence.independence_date.last_date()\nexpr\n\nr0 := DatabaseTable: independence\n  country_code      string\n  independence_date date\n  independence_from string\n\nLastDate(independence_date): LastDate(r0.independence_date)\n\n\n\nsql_expr = expr.compile()\nprint(sql_expr)\n\nSELECT max(t0.independence_date) AS \"LastDate(independence_date)\" \nFROM independence AS t0\n\n\n/var/folders/r4/5821nk2d3nv2q7vr8v6gg5f00000gn/T/ipykernel_41137/3120781585.py:7: FutureWarning: `Node.op` is deprecated as of v4.0; remove intermediate .op() calls\n  arg, where = expr.op().args\n\n\n\nexpr.to_pandas()\n\n/var/folders/r4/5821nk2d3nv2q7vr8v6gg5f00000gn/T/ipykernel_41137/3120781585.py:7: FutureWarning: `Node.op` is deprecated as of v4.0; remove intermediate .op() calls\n  arg, where = expr.op().args\n\n\nTimestamp('2011-07-09 00:00:00')\n\n\nLast country to gain independence from the Spanish Empire, using the where parameter:\n\nexpr = independence.independence_date.last_date(\n    where=independence.independence_from == 'Spanish Empire'\n)\nexpr\n\nr0 := DatabaseTable: independence\n  country_code      string\n  independence_date date\n  independence_from string\n\nLastDate(independence_date, Equals(independence_from, 'Spanish Empire')): LastDate(r0.independence_date, where=r0.independence_from == 'Spanish Empire')\n\n\n\nresult = expr.to_pandas()\nresult\n\n/var/folders/r4/5821nk2d3nv2q7vr8v6gg5f00000gn/T/ipykernel_41137/3120781585.py:7: FutureWarning: `Node.op` is deprecated as of v4.0; remove intermediate .op() calls\n  arg, where = expr.op().args\n\n\nTimestamp('1898-06-12 00:00:00')"
  },
  {
    "objectID": "how_to/extending/elementwise.html",
    "href": "how_to/extending/elementwise.html",
    "title": "Add an elementwise operation",
    "section": "",
    "text": "This notebook will show you how to add a new elementwise operation to an existing backend.\nWe are going to add julianday, a function supported by the SQLite database, to the SQLite Ibis backend.\nThe Julian day of a date, is the number of days since January 1st, 4713 BC. For more information check the Julian day wikipedia page."
  },
  {
    "objectID": "how_to/extending/elementwise.html#step-1-define-the-operation",
    "href": "how_to/extending/elementwise.html#step-1-define-the-operation",
    "title": "Add an elementwise operation",
    "section": "Step 1: Define the Operation",
    "text": "Step 1: Define the Operation\nLet’s define the julianday operation as a function that takes one string input argument and returns a float.\ndef julianday(date: str) -&gt; float:\n    \"\"\"Julian date\"\"\"\n\nimport ibis.expr.datatypes as dt\nimport ibis.expr.rules as rlz\nfrom ibis.expr.operations import ValueOp\n\n\nclass JulianDay(ValueOp):\n    arg = rlz.string\n\n    output_dtype = dt.float32\n    output_shape = rlz.shape_like('arg')\n\nWe just defined a JulianDay class that takes one argument of type string or binary, and returns a float."
  },
  {
    "objectID": "how_to/extending/elementwise.html#step-2-define-the-api",
    "href": "how_to/extending/elementwise.html#step-2-define-the-api",
    "title": "Add an elementwise operation",
    "section": "Step 2: Define the API",
    "text": "Step 2: Define the API\nBecause we know the output type of the operation, to make an expression out of JulianDay we simply need to construct it and call its ibis.expr.types.Node.to_expr method.\nWe still need to add a method to StringValue and BinaryValue (this needs to work on both scalars and columns).\nWhen you add a method to any of the expression classes whose name matches *Value both the scalar and column child classes will pick it up, making it easy to define operations for both scalars and columns in one place.\nWe can do this by defining a function and assigning it to the appropriate class of expressions.\n\nfrom ibis.expr.types import BinaryValue, StringValue\n\n\ndef julianday(string_value):\n    return JulianDay(string_value).to_expr()\n\n\nStringValue.julianday = julianday"
  },
  {
    "objectID": "how_to/extending/elementwise.html#interlude-create-some-expressions-with-sha1",
    "href": "how_to/extending/elementwise.html#interlude-create-some-expressions-with-sha1",
    "title": "Add an elementwise operation",
    "section": "Interlude: Create some expressions with sha1",
    "text": "Interlude: Create some expressions with sha1\n\nimport ibis\n\nt = ibis.table([('string_col', 'string')], name='t')\n\nt.string_col.julianday()\n\n\nr0 := UnboundTable: t\n  string_col string\n\nSelection[r0]\n  selections:\n    JulianDay(string_col): JulianDay(r0.string_col)"
  },
  {
    "objectID": "how_to/extending/elementwise.html#step-3-turn-the-expression-into-sql",
    "href": "how_to/extending/elementwise.html#step-3-turn-the-expression-into-sql",
    "title": "Add an elementwise operation",
    "section": "Step 3: Turn the Expression into SQL",
    "text": "Step 3: Turn the Expression into SQL\n\nimport sqlalchemy as sa\n\n\n@ibis.sqlite.add_operation(JulianDay)\ndef _julianday(translator, expr):\n    # pull out the arguments to the expression\n    (arg,) = expr.args\n\n    # compile the argument\n    compiled_arg = translator.translate(arg)\n\n    # return a SQLAlchemy expression that calls into the SQLite julianday function\n    return sa.func.julianday(compiled_arg)"
  },
  {
    "objectID": "how_to/extending/elementwise.html#step-4-putting-it-all-together",
    "href": "how_to/extending/elementwise.html#step-4-putting-it-all-together",
    "title": "Add an elementwise operation",
    "section": "Step 4: Putting it all Together",
    "text": "Step 4: Putting it all Together\n\n!curl -LsS -o geography.db 'https://storage.googleapis.com/ibis-tutorial-data/geography.db'\n\n\nimport os\nimport tempfile\n\nimport ibis\n\ndb_fname = 'geography.db'\n\ncon = ibis.sqlite.connect(db_fname)\n\n\nCreate and execute a julianday expression\n\nindependence = con.table('independence')\nindependence\n\n\nDatabaseTable: independence\n  country_code      string\n  independence_date date\n  independence_from string\n\n\n\n\nday = independence.independence_date.cast('string')\nday\n\n\nr0 := DatabaseTable: independence\n  country_code      string\n  independence_date date\n  independence_from string\n\nSelection[r0]\n  selections:\n    Cast(independence_date, string): Cast(r0.independence_date, to=string)\n\n\n\n\njulianday_expr = day.julianday().name(\"jday\")\njulianday_expr\n\n\nr0 := DatabaseTable: independence\n  country_code      string\n  independence_date date\n  independence_from string\n\nSelection[r0]\n  selections:\n    jday: JulianDay(Cast(r0.independence_date, to=string))\n\n\n\n\nsql_expr = julianday_expr.compile()\nprint(sql_expr)\n\nSELECT julianday(CAST(t0.independence_date AS TEXT)) AS jday \nFROM independence AS t0\n\n\n\nresult = julianday_expr.to_pandas()\nresult.head()\n\n0    2422189.5\n1    2419734.5\n2    2437850.5\n3    2442727.5\n4    2444909.5\nName: jday, dtype: float32\n\n\nBecause we’ve defined our operation on StringValue, and not just on StringColumn we get operations on both string scalars and string columns for free\n\nscalar = ibis.literal('2010-03-14')\nscalar\n\n'2010-03-14'\n\n\n\njulianday_scalar = scalar.julianday()\n\n\ncon.execute(julianday_scalar)\n\n2455269.5"
  },
  {
    "objectID": "how_to/memtable_join.html",
    "href": "how_to/memtable_join.html",
    "title": "Join an in-memory DataFrame to a TableExpression",
    "section": "",
    "text": "You might have an in-memory DataFrame that you want to join to a TableExpression. For example, you might have a file on your local machine that you don’t want to upload to your backend, but you need to join it to a table in that backend.\nYou can perform joins on local data to TableExpressions from your backend easily with Ibis MemTables.\nIn this guide, you will learn how to join a pandas DataFrame to a TableExpression.\n\n\nIn this example, we will create two DataFrames: one containing events and one containing event names. We will save the events to a parquet file and read that as a TableExpression in the DuckDB backend. We will then convert the event names DataFrame to a PandasInMemoryTable (MemTable), which is a pandas DataFrame as a TableExpression and join the two expressions together as we would two TableExpressions in a backend.\n    In [1]: import ibis\n\n    In [2]: import pandas as pd\n       ...: from datetime import date\n\n    In [3]: # create a pandas DataFrame that we will convert to a\n       ...: # PandasInMemoryTable (Ibis MemTable)\n       ...: events = pd.DataFrame(\n       ...:     {\n       ...:         'event_id': range(4),\n       ...:         'event_name': [f'e{k}' for k in range(4)],\n       ...:     }\n       ...: )\n\n    In [4]: # Create a parquet file that we will read in using the DuckDB backend\n       ...: # as a TableExpression\n       ...: measures = pd.DataFrame({\n       ...:     \"event_id\": [0] * 2 + [1] * 3 + [2] * 5 + [3] * 2\n       ...:     ,\"measured_on\": map(\n       ...:         date\n       ...:         ,[2021] * 12, [6] * 4 + [5] * 6 + [7] * 2\n       ...:         ,range(1, 13)\n       ...:     )\n       ...:     ,\"measurement\": None\n       ...: })\n\n    In [5]: measures.at[1, \"measurement\"] = 5.\n       ...: measures.at[4, \"measurement\"] = 42.\n       ...: measures.at[5, \"measurement\"] = 42.\n       ...: measures.at[7, \"measurement\"] = 11.\n\n    In [6]: # Save measures to parquet:\n       ...: measures.to_parquet('measures.parquet')\n\n    In [7]: # connect to a DuckDB backend\n       ...: conn = ibis.connect('duckdb://:memory:')\n       ...: measures = conn.register('measures.parquet', 'measures')\n\n    In [8]: # `measures` is a TableExpression in a DuckDB backend connection:\n       ...: measures\n    Out[8]:\n    AlchemyTable: measures\n      event_id    int64\n      measured_on date\n      measurement float64\nConverting a pandas DataFrame to a MemTable is as simple as feeding it to ibis.memtable:\n    In [9]: # To join, convert your DataFrame to a memtable\n       ...: mem_events = ibis.memtable(events)\n\n    In [10]: mem_events\n    Out[10]:\n    PandasInMemoryTable\n      data:\n        PandasDataFrameProxy:\n             event_id event_name\n          0         0         e0\n          1         1         e1\n          2         2         e2\n          3         3         e3\nand joining is the same as joining any two TableExpressions:\n    In [11]: # Join as you would two table expressions\n        ...: measures.join(\n        ...:     mem_events,\n        ...:     measures['event_id'] == mem_events['event_id']\n        ...: ).to_pandas()\n    Out[11]:\n        event_id measured_on  measurement  event_name\n    0          0  2021-06-01          NaN          e0\n    1          0  2021-06-02          5.0          e0\n    2          1  2021-06-03          NaN          e1\n    3          1  2021-06-04          NaN          e1\n    4          1  2021-05-05         42.0          e1\n    5          2  2021-05-06         42.0          e2\n    6          2  2021-05-07          NaN          e2\n    7          2  2021-05-08         11.0          e2\n    8          2  2021-05-09          NaN          e2\n    9          2  2021-05-10          NaN          e2\n    10         3  2021-07-11          NaN          e3\n    11         3  2021-07-12          NaN          e3\nNote that the return result of the join is a TableExpression and that to_pandas returns a pandas DataFrame."
  },
  {
    "objectID": "how_to/memtable_join.html#data-setup",
    "href": "how_to/memtable_join.html#data-setup",
    "title": "Join an in-memory DataFrame to a TableExpression",
    "section": "",
    "text": "In this example, we will create two DataFrames: one containing events and one containing event names. We will save the events to a parquet file and read that as a TableExpression in the DuckDB backend. We will then convert the event names DataFrame to a PandasInMemoryTable (MemTable), which is a pandas DataFrame as a TableExpression and join the two expressions together as we would two TableExpressions in a backend.\n    In [1]: import ibis\n\n    In [2]: import pandas as pd\n       ...: from datetime import date\n\n    In [3]: # create a pandas DataFrame that we will convert to a\n       ...: # PandasInMemoryTable (Ibis MemTable)\n       ...: events = pd.DataFrame(\n       ...:     {\n       ...:         'event_id': range(4),\n       ...:         'event_name': [f'e{k}' for k in range(4)],\n       ...:     }\n       ...: )\n\n    In [4]: # Create a parquet file that we will read in using the DuckDB backend\n       ...: # as a TableExpression\n       ...: measures = pd.DataFrame({\n       ...:     \"event_id\": [0] * 2 + [1] * 3 + [2] * 5 + [3] * 2\n       ...:     ,\"measured_on\": map(\n       ...:         date\n       ...:         ,[2021] * 12, [6] * 4 + [5] * 6 + [7] * 2\n       ...:         ,range(1, 13)\n       ...:     )\n       ...:     ,\"measurement\": None\n       ...: })\n\n    In [5]: measures.at[1, \"measurement\"] = 5.\n       ...: measures.at[4, \"measurement\"] = 42.\n       ...: measures.at[5, \"measurement\"] = 42.\n       ...: measures.at[7, \"measurement\"] = 11.\n\n    In [6]: # Save measures to parquet:\n       ...: measures.to_parquet('measures.parquet')\n\n    In [7]: # connect to a DuckDB backend\n       ...: conn = ibis.connect('duckdb://:memory:')\n       ...: measures = conn.register('measures.parquet', 'measures')\n\n    In [8]: # `measures` is a TableExpression in a DuckDB backend connection:\n       ...: measures\n    Out[8]:\n    AlchemyTable: measures\n      event_id    int64\n      measured_on date\n      measurement float64\nConverting a pandas DataFrame to a MemTable is as simple as feeding it to ibis.memtable:\n    In [9]: # To join, convert your DataFrame to a memtable\n       ...: mem_events = ibis.memtable(events)\n\n    In [10]: mem_events\n    Out[10]:\n    PandasInMemoryTable\n      data:\n        PandasDataFrameProxy:\n             event_id event_name\n          0         0         e0\n          1         1         e1\n          2         2         e2\n          3         3         e3\nand joining is the same as joining any two TableExpressions:\n    In [11]: # Join as you would two table expressions\n        ...: measures.join(\n        ...:     mem_events,\n        ...:     measures['event_id'] == mem_events['event_id']\n        ...: ).to_pandas()\n    Out[11]:\n        event_id measured_on  measurement  event_name\n    0          0  2021-06-01          NaN          e0\n    1          0  2021-06-02          5.0          e0\n    2          1  2021-06-03          NaN          e1\n    3          1  2021-06-04          NaN          e1\n    4          1  2021-05-05         42.0          e1\n    5          2  2021-05-06         42.0          e2\n    6          2  2021-05-07          NaN          e2\n    7          2  2021-05-08         11.0          e2\n    8          2  2021-05-09          NaN          e2\n    9          2  2021-05-10          NaN          e2\n    10         3  2021-07-11          NaN          e3\n    11         3  2021-07-12          NaN          e3\nNote that the return result of the join is a TableExpression and that to_pandas returns a pandas DataFrame."
  },
  {
    "objectID": "how_to/sessionize.html",
    "href": "how_to/sessionize.html",
    "title": "Sessionize a log of events",
    "section": "",
    "text": "Suppose you have entities (users, objects, actions, etc) that have event logs through polling or event triggers.\nYou might be interested in partitioning these logs by something called sessions, which can be defined as groups of consecutive event records without long interruptions for a given entity.\nIn the case of a user portal, it might be grouping the navigation events that result in completing a task or buying a product. For online games, it might be a the grouping of activity events of a given user playing the game while remaining logged in.\nSessionization can also be useful on longer time scales, for instance to reconstruct active subscription data from a raw payment or activity log, so as to model customer churn.\nThis guide on sessionization is inspired by The Expressions API in Polars is Amazing, a blog post in the Polars community demonstrating the strength of Polars expressions.\n\n\nFor this example, we use an activity log from the online game “World of Warcraft” with more than 10 million records for 37,354 unique players made available under the CC0 / Public Domain license. A copy of the data can be found at https://storage.googleapis.com/ibis-tutorial-data/wowah_data/wowah_data_raw.parquet (75 MB) under the parquet format to reduce load times. You can use ibis.read_parquet to quickly get it into a table expression via the default DuckDB backend.\nThis data contains the following fields:\n\nchar : a unique identifier for a character (or a player). This is our entity column.\ntimestamp: a timestamp denoting when a char was polled. This occurs every ~10 minutes.\n\nWe can take this information, along with a definition of what separates two sessions for an entity, and break our dataset up into sessions without using any joins:\n# Imports\nimport ibis\nfrom ibis import deferred as c\n\n# Read files into table expressions with ibis.read_parquet:\ndata = ibis.read_parquet(\n    \"https://storage.googleapis.com/ibis-tutorial-data/wowah_data/wowah_data_raw.parquet\"\n)\n\n# Integer delay in seconds noting if a row should be included in the previous\n# session for an entity.\nsession_boundary_threshold = 30 * 60\n\n# Window for finding session ids per character\nentity_window = ibis.cumulative_window(group_by=c.char, order_by=c.timestamp)\n\n# Take the previous timestamp within a window (by character ordered by timestamp):\n# Note: the first value in a window will be null.\nts_lag = c.timestamp.lag().over(entity_window)\n\n# Subtract the lag from the current timestamp to get a timedelta.\nts_delta = c.timestamp - ts_lag\n\n# Compare timedelta to our session delay in seconds to determine if the\n# current timestamp falls outside of the session.\n# Cast as int for aggregation.\nis_new_session = (ts_delta &gt; ibis.interval(seconds=session_boundary_threshold))\n\n# Window to compute session min/max and duration.\nsession_window = ibis.window(group_by=[c.char, c.session_id])\n\n# Generate all of the data we need to analyze sessions:\nsessionized = (\n    data\n    # Create a session id for each character by using a cumulative sum\n    # over the `new_session` column.\n    .mutate(new_session=is_new_session.fillna(True))\n    # Create a session id for each character by using a cumulative sum\n    # over the `new_session` column.\n    .mutate(session_id=c.new_session.sum().over(entity_window))\n    # Drop `new_session` because it is no longer needed.\n    .drop(\"new_session\")\n    .mutate(\n        # Get session duration using max(timestamp) - min(timestamp) over our window.\n        session_duration=c.timestamp.max().over(session_window) - c.timestamp.min().over(session_window)\n    )\n    # Sort for convenience.\n    .order_by([c.char, c.timestamp])\n)\nCalling ibis.show_sql(sessionized) displays the SQL query and can be used to confirm that this Ibis table expression does not rely on any join operations.\nCalling sessionized.to_pandas() should complete in less than a minute, depending on the speed of the internet connection to download the data and the number of CPU cores available to parallelize the processing of this nested query."
  },
  {
    "objectID": "how_to/sessionize.html#sessionizing-logs-on-a-cadence",
    "href": "how_to/sessionize.html#sessionizing-logs-on-a-cadence",
    "title": "Sessionize a log of events",
    "section": "",
    "text": "For this example, we use an activity log from the online game “World of Warcraft” with more than 10 million records for 37,354 unique players made available under the CC0 / Public Domain license. A copy of the data can be found at https://storage.googleapis.com/ibis-tutorial-data/wowah_data/wowah_data_raw.parquet (75 MB) under the parquet format to reduce load times. You can use ibis.read_parquet to quickly get it into a table expression via the default DuckDB backend.\nThis data contains the following fields:\n\nchar : a unique identifier for a character (or a player). This is our entity column.\ntimestamp: a timestamp denoting when a char was polled. This occurs every ~10 minutes.\n\nWe can take this information, along with a definition of what separates two sessions for an entity, and break our dataset up into sessions without using any joins:\n# Imports\nimport ibis\nfrom ibis import deferred as c\n\n# Read files into table expressions with ibis.read_parquet:\ndata = ibis.read_parquet(\n    \"https://storage.googleapis.com/ibis-tutorial-data/wowah_data/wowah_data_raw.parquet\"\n)\n\n# Integer delay in seconds noting if a row should be included in the previous\n# session for an entity.\nsession_boundary_threshold = 30 * 60\n\n# Window for finding session ids per character\nentity_window = ibis.cumulative_window(group_by=c.char, order_by=c.timestamp)\n\n# Take the previous timestamp within a window (by character ordered by timestamp):\n# Note: the first value in a window will be null.\nts_lag = c.timestamp.lag().over(entity_window)\n\n# Subtract the lag from the current timestamp to get a timedelta.\nts_delta = c.timestamp - ts_lag\n\n# Compare timedelta to our session delay in seconds to determine if the\n# current timestamp falls outside of the session.\n# Cast as int for aggregation.\nis_new_session = (ts_delta &gt; ibis.interval(seconds=session_boundary_threshold))\n\n# Window to compute session min/max and duration.\nsession_window = ibis.window(group_by=[c.char, c.session_id])\n\n# Generate all of the data we need to analyze sessions:\nsessionized = (\n    data\n    # Create a session id for each character by using a cumulative sum\n    # over the `new_session` column.\n    .mutate(new_session=is_new_session.fillna(True))\n    # Create a session id for each character by using a cumulative sum\n    # over the `new_session` column.\n    .mutate(session_id=c.new_session.sum().over(entity_window))\n    # Drop `new_session` because it is no longer needed.\n    .drop(\"new_session\")\n    .mutate(\n        # Get session duration using max(timestamp) - min(timestamp) over our window.\n        session_duration=c.timestamp.max().over(session_window) - c.timestamp.min().over(session_window)\n    )\n    # Sort for convenience.\n    .order_by([c.char, c.timestamp])\n)\nCalling ibis.show_sql(sessionized) displays the SQL query and can be used to confirm that this Ibis table expression does not rely on any join operations.\nCalling sessionized.to_pandas() should complete in less than a minute, depending on the speed of the internet connection to download the data and the number of CPU cores available to parallelize the processing of this nested query."
  },
  {
    "objectID": "how_to/self_joins.html",
    "href": "how_to/self_joins.html",
    "title": "Perform self joins",
    "section": "",
    "text": "Perform self joins\nIf you’re a relational data guru, you may have wondered how it’s possible to join tables with themselves, because joins clauses involve column references back to the original table.\nConsider the SQL\nSELECT t1.key, sum(t1.value - t2.value) AS metric\nFROM my_table t1\n JOIN my_table t2\n   ON t1.key = t2.subkey\nGROUP BY 1\nHere, we have an unambiguous way to refer to each of the tables through aliasing.\nLet’s consider the TPC-H database, and support we want to compute year-over-year change in total order amounts by region using joins.\n&gt;&gt;&gt; region = con.table('tpch_region')\n&gt;&gt;&gt; nation = con.table('tpch_nation')\n&gt;&gt;&gt; customer = con.table('tpch_customer')\n&gt;&gt;&gt; orders = con.table('tpch_orders')\n&gt;&gt;&gt; orders.limit(5)\n   o_orderkey  o_custkey o_orderstatus o_totalprice o_orderdate  \\\n0           1      36901             O    173665.47  1996-01-02\n1           2      78002             O     46929.18  1996-12-01\n2           3     123314             F    193846.25  1993-10-14\n3           4     136777             O     32151.78  1995-10-11\n4           5      44485             F    144659.20  1994-07-30\n\n  o_orderpriority          o_clerk  o_shippriority  \\\n0           5-LOW  Clerk#000000951               0\n1        1-URGENT  Clerk#000000880               0\n2           5-LOW  Clerk#000000955               0\n3           5-LOW  Clerk#000000124               0\n4           5-LOW  Clerk#000000925               0\n\n                                           o_comment\n0                 nstructions sleep furiously among\n1   foxes. pending accounts at the pending, silen...\n2  sly final accounts boost. carefully regular id...\n3  sits. slyly regular warthogs cajole. regular, ...\n4  quickly. bold deposits sleep slyly. packages u...\nFirst, let’s join all the things and select the fields we care about:\n&gt;&gt;&gt; fields_of_interest = [region.r_name.name('region'),\n...                       nation.n_name.name('nation'),\n...                       orders.o_totalprice.name('amount'),\n...                       orders.o_orderdate.cast('timestamp').name('odate') # these are strings\n...                       ]\n&gt;&gt;&gt; joined_all = (region.join(nation, region.r_regionkey == nation.n_regionkey)\n...               .join(customer, customer.c_nationkey == nation.n_nationkey)\n...               .join(orders, orders.o_custkey == customer.c_custkey)\n...               [fields_of_interest])\nOkay, great, let’s have a look:\n&gt;&gt;&gt; joined_all.limit(5)\n        region         nation     amount      odate\n0      AMERICA  UNITED STATES  160843.35 1992-06-22\n1  MIDDLE EAST           IRAN   78307.91 1996-04-19\n2       EUROPE         FRANCE  103237.90 1994-10-12\n3       EUROPE         FRANCE  201463.59 1997-09-12\n4         ASIA          JAPAN  166098.86 1995-09-12\nSweet, now let’s aggregate by year and region:\n&gt;&gt;&gt; year = joined_all.odate.year().name('year')\n&gt;&gt;&gt; total = joined_all.amount.sum().cast('float').name('total')\n&gt;&gt;&gt; annual_amounts = (joined_all\n...                   .group_by(['region', year])\n...                   .aggregate(total))\n    &gt;&gt;&gt; annual_amounts.limit(5)\n         region  year         total\n0        EUROPE  1994  6.979473e+09\n1        EUROPE  1996  7.015421e+09\n2          ASIA  1997  6.910663e+09\n3          ASIA  1998  4.058824e+09\n4        EUROPE  1992  6.926705e+09\nLooking good so far. Now, we need to join this table on itself, by subtracting 1 from one of the year columns.\nWe do this by creating a “joinable” view of a table that is considered a distinct object within Ibis. To do this, use the view function:\n&gt;&gt;&gt; current = annual_amounts\n&gt;&gt;&gt; prior = annual_amounts.view()\n&gt;&gt;&gt; yoy_change = (current.total - prior.total).name('yoy_change')\n&gt;&gt;&gt; results = (current.join(prior, ((current.region == prior.region) &\n...                                 (current.year == (prior.year - 1))))\n...            [current.region, current.year, yoy_change])\n&gt;&gt;&gt; df = results.to_pandas()\n&gt;&gt;&gt; df['yoy_pretty'] = df.yoy_change.map(lambda x: '$%.2fmm' % (x / 1000000.))\nIf you’re being fastidious and want to consider the first year occurring in the dataset for each region to have 0 for the prior year, you will instead need to do an outer join and treat nulls in the prior side of the join as zero:\n&gt;&gt;&gt; yoy_change = (current.total - prior.total.zeroifnull()).name('yoy_change')\n&gt;&gt;&gt; results = (current.outer_join(prior, ((current.region == prior.region) &\n...                                       (current.year == (prior.year - 1))))\n...            [current.region, current.year, current.total,\n...             prior.total.zeroifnull().name('prior_total'),\n...             yoy_change])\n&gt;&gt;&gt; results.limit(5)\n        region  year         total   prior_total    yoy_change\n0         ASIA  1998  4.058824e+09  0.000000e+00  4.058824e+09\n1       AFRICA  1994  6.837587e+09  6.908429e+09 -7.084172e+07\n2      AMERICA  1996  6.883057e+09  6.922465e+09 -3.940791e+07\n3       AFRICA  1996  6.878112e+09  6.848983e+09  2.912979e+07\n4       AFRICA  1992  6.873319e+09  6.859733e+09  1.358699e+07"
  },
  {
    "objectID": "how_to/ffill_bfill_w_window.html",
    "href": "how_to/ffill_bfill_w_window.html",
    "title": "Forward and backward fill data using window functions",
    "section": "",
    "text": "Forward and backward fill data using window functions\nIf you have gaps in your data and need to fill them in using a simple forward fill (given an order, null values are replaced by the value preceding) or backward fill (given an order, null values are replaced by the value following), then you can do this in Ibis:\n=== “ffill”\n~~~python\n# Create a window that orders your series, default ascending\nwin = ibis.window(order_by=data.measured_on, following=0)\n# Create a grouping that is a rolling count of non-null values\n# This creates a partition where each set has no more than one non-null value\ngrouped = data.mutate(grouper=data.measurement.count().over(win))\n# Group by your newly-created grouping and, in each set,\n# set all values to the one non-null value in that set (if it exists)\nresult = (\n    grouped\n    .group_by([grouped.grouper])\n    .mutate(ffill=grouped.measurement.max())\n)\n# execute to get a pandas dataframe, sort values in case your backend shuffles\nresult.execute().sort_values(by=['measured_on'])\n~~~\n=== “bfill”\n~~~python\n# Create a window that orders your series (use ibis.desc to get descending order)\nwin = ibis.window(order_by=ibis.desc(data.measured_on), following=0)\n# Create a grouping that is a rolling count of non-null values\n# This creates a partition where each set has no more than one non-null value\ngrouped = data.mutate(grouper=data.measurement.count().over(win))\n# Group by your newly-created grouping and, in each set,\n# set all values to the one non-null value in that set (if it exists)\nresult = (\n    grouped\n    .group_by([grouped.grouper])\n    .mutate(ffill=grouped.measurement.max())\n)\n# execute to get a pandas dataframe, sort values in case your backend shuffles\nresult.execute().sort_values(by=['measured_on'])\n~~~\nIf you have an event partition, which means there’s another segment you need to consider for your ffill or bfill operations, you can do this as well:\n=== “ffill with event partition”\n~~~python\n# Group your data by your event partition and then order your series (default ascending)\nwin = ibis.window(group_by=data.event_id, order_by=data.measured_on, following=0)\n# Create a grouping that is a rolling count of non-null values within each event\n# This creates a partition where each set has no more than one non-null value\ngrouped = data.mutate(grouper=data.measurement.count().over(win))\n# Group by your newly-created grouping and, in each set,\n# set all values to the one non-null value in that set (if it exists)\nresult = (\n    grouped\n    .group_by([grouped.event_id, grouped.grouper])\n    .mutate(ffill=grouped.measurement.max())\n)\n# execute to get a pandas dataframe, sort values in case your backend shuffles\nresult.execute().sort_values(by=['event_id', 'measured_on'])\n~~~\n=== “bfill with event partition”\n~~~python\n# Group your data by your event partition and then order your series (use ibis.desc for desc)\nwin = ibis.window(group_by=data.event_id, order_by=ibis.desc(data.measured_on), following=0)\n# Create a grouping that is a rolling count of non-null values within each event\n# This creates a partition where each set has no more than one non-null value\ngrouped = data.mutate(grouper=data.measurement.count().over(win))\n# Group by your newly-created grouping and, in each set,\n# set all values to the one non-null value in that set (if it exists)\nresult = (\n    grouped\n    .group_by([grouped.event_id, grouped.grouper])\n    .mutate(ffill=grouped.measurement.max())\n)\n# execute to get a pandas dataframe, sort values in case your backend shuffles\nresult.execute().sort_values(by=['event_id', 'measured_on'])\n~~~\nWe wrote a deeper dive into how this works on the ibis-project blog here."
  },
  {
    "objectID": "how_to/configuration.html",
    "href": "how_to/configuration.html",
    "title": "Configure Ibis",
    "section": "",
    "text": "Ibis configuration happens through the ibis.options attribute. Attributes can be get and set like class attributes.\n\n\nIbis out of the box is in developer mode. Expressions display their internal details when printed to the console. For a better interactive experience, set the interactive option:\nibis.options.interactive = True\nThis will cause expressions to be executed immediately when printed to the console.\n\n\n\nIf an Ibis table expression has no row limit set using the limit API, a default one is applied to prevent too much data from being retrieved from the query engine. The default is currently 10000 rows, but this can be configured with the sql.default_limit option:\nibis.options.sql.default_limit = 100\nSet this to None to retrieve all rows in all queries\n!!! warning “Be careful with None”\nSetting the default limit to `None` will result in *all* rows from a query\ncoming back to the client from the backend.\nibis.options.sql.default_limit = None\n\n\n\nTo see all internal Ibis activity (like queries being executed) set ibis.options.verbose:\nibis.options.verbose = True\nBy default this information is sent to sys.stdout, but you can set some other logging function:\ndef cowsay(msg):\n    print(f\"Cow says: {msg}\")\n\n\nibis.options.verbose_log = cowsay\n\n\n\nibis.options.default_backend controls which backend is used by table expressions returned by top-level functions such as ibis.memtable, ibis.read_csv or ibis.read_parquet.\nBy default, it points to an instance of DuckDB backend. Assuming the backend dependencies have been installed, it can be updated by passing the name of the backend to ibis.set_backend as follows:\nimport ibis\n\nexpr = ibis.memtable({\"column\": [0, 1, 2, 3, 4]})\nibis.get_backend(expr)\n# &lt;ibis.backends.duckdb.Backend at 0x12fa0fb50&gt;\n\nibis.set_backend(\"sqlite\")\nibis.get_backend(expr)\n# &lt;ibis.backends.sqlite.Backend at 0x158411d10&gt;"
  },
  {
    "objectID": "how_to/configuration.html#interactive-mode",
    "href": "how_to/configuration.html#interactive-mode",
    "title": "Configure Ibis",
    "section": "",
    "text": "Ibis out of the box is in developer mode. Expressions display their internal details when printed to the console. For a better interactive experience, set the interactive option:\nibis.options.interactive = True\nThis will cause expressions to be executed immediately when printed to the console."
  },
  {
    "objectID": "how_to/configuration.html#sql-query-execution",
    "href": "how_to/configuration.html#sql-query-execution",
    "title": "Configure Ibis",
    "section": "",
    "text": "If an Ibis table expression has no row limit set using the limit API, a default one is applied to prevent too much data from being retrieved from the query engine. The default is currently 10000 rows, but this can be configured with the sql.default_limit option:\nibis.options.sql.default_limit = 100\nSet this to None to retrieve all rows in all queries\n!!! warning “Be careful with None”\nSetting the default limit to `None` will result in *all* rows from a query\ncoming back to the client from the backend.\nibis.options.sql.default_limit = None"
  },
  {
    "objectID": "how_to/configuration.html#verbose-option-and-logging",
    "href": "how_to/configuration.html#verbose-option-and-logging",
    "title": "Configure Ibis",
    "section": "",
    "text": "To see all internal Ibis activity (like queries being executed) set ibis.options.verbose:\nibis.options.verbose = True\nBy default this information is sent to sys.stdout, but you can set some other logging function:\ndef cowsay(msg):\n    print(f\"Cow says: {msg}\")\n\n\nibis.options.verbose_log = cowsay"
  },
  {
    "objectID": "how_to/configuration.html#default-backend",
    "href": "how_to/configuration.html#default-backend",
    "title": "Configure Ibis",
    "section": "",
    "text": "ibis.options.default_backend controls which backend is used by table expressions returned by top-level functions such as ibis.memtable, ibis.read_csv or ibis.read_parquet.\nBy default, it points to an instance of DuckDB backend. Assuming the backend dependencies have been installed, it can be updated by passing the name of the backend to ibis.set_backend as follows:\nimport ibis\n\nexpr = ibis.memtable({\"column\": [0, 1, 2, 3, 4]})\nibis.get_backend(expr)\n# &lt;ibis.backends.duckdb.Backend at 0x12fa0fb50&gt;\n\nibis.set_backend(\"sqlite\")\nibis.get_backend(expr)\n# &lt;ibis.backends.sqlite.Backend at 0x158411d10&gt;"
  },
  {
    "objectID": "how_to/duckdb_register.html",
    "href": "how_to/duckdb_register.html",
    "title": "Load external data files with the DuckDB backend",
    "section": "",
    "text": "Load external data files with the DuckDB backend\n\nHere we use the register method to load external data files and join them. \nWe’re going to download one month of NYC Taxi data in parquet format and also download the “Taxi Zone Lookup Table” which is a csv\nhttps://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\nCreate an in-memory DuckDB connection via ibis\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; con = ibis.duckdb.connect()  # in-memory database\n&gt;&gt;&gt; con.list_tables()\n[]\nNow we call register with the filepath (the table_name argument is optional, if it isn’t specified, Ibis will use the filename minus the extension)\n&gt;&gt;&gt; con.register(\"taxi+_zone_lookup.csv\", table_name=\"taxi_zone_lookup\")\nAlchemyTable: taxi+_zone_lookup\n  LocationID   int32\n  Borough      string\n  Zone         string\n  service_zone string\n\n&gt;&gt;&gt; con.register(\"green_tripdata_2022-01.parquet\", table_name=\"tripdata\")\nAlchemyTable: green_tripdata_2022_01\n  VendorID              int64\n  lpep_pickup_datetime  timestamp\n  lpep_dropoff_datetime timestamp\n  store_and_fwd_flag    string\n  RatecodeID            float64\n  PULocationID          int64\n  DOLocationID          int64\n  passenger_count       float64\n  trip_distance         float64\n  fare_amount           float64\n  extra                 float64\n  mta_tax               float64\n  tip_amount            float64\n  tolls_amount          float64\n  ehail_fee             int32\n  improvement_surcharge float64\n  total_amount          float64\n  payment_type          float64\n  trip_type             float64\n  congestion_surcharge  float64\n&gt;&gt;&gt; con.list_tables()\n['tripdata, 'taxi_zone_lookup']\nWe now have a schema parsed from the files and corresponding tables (they are actually views that are lazily-loaded) are available.\nNow we can interact with these tables just like a table or view in any backend connection:\n&gt;&gt;&gt; lookup = con.table(\"taxi_zone_lookup\")\n&gt;&gt;&gt; tripdata = con.table(\"tripdata\")\n\n&gt;&gt;&gt; tripdata.columns\n['VendorID', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'RatecodeID', 'PULocationID', 'DOLocationID', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge']\n\n&gt;&gt;&gt; lookup.columns\n['LocationID', 'Borough', 'Zone', 'service_zone']\nWe can grab a small subset of the tripdata columns and then join them to the lookup table to get human-readable values for the pickup locations:\n&gt;&gt;&gt; ibis.options.interactive = True\n\n&gt;&gt;&gt; tripdata = tripdata[[\"lpep_pickup_datetime\", \"PULocationID\"]]\n\n&gt;&gt;&gt; tripdata.head()\n  lpep_pickup_datetime  PULocationID\n0  2022-01-01 00:14:21            42\n1  2022-01-01 00:20:55           116\n2  2022-01-01 00:57:02            41\n3  2022-01-01 00:07:42           181\n4  2022-01-01 00:07:50            33\n\n&gt;&gt;&gt; tripdata.join(lookup, tripdata.PULocationID == lookup.LocationID).head()\n  lpep_pickup_datetime  PULocationID  LocationID    Borough                  Zone service_zone\n0  2022-01-01 00:14:21            42          42  Manhattan  Central Harlem North    Boro Zone\n1  2022-01-01 00:20:55           116         116  Manhattan      Hamilton Heights    Boro Zone\n2  2022-01-01 00:57:02            41          41  Manhattan        Central Harlem    Boro Zone\n3  2022-01-01 00:07:42           181         181   Brooklyn            Park Slope    Boro Zone\n4  2022-01-01 00:07:50            33          33   Brooklyn      Brooklyn Heights    Boro Zone\nThat’s it!\nIbis+duckdb currently supports registering parquet, csv, and csv.gz.\nYou can pass in the filename and the filetype will be inferred from the extension, or you can pass it explicitly using a file URI, e.g.\ncon.register(\"csv://some_csv_file_without_an_extension\")\ncon.register(\"csv.gz://a_compressed_csv_file.csv\")\ncon.register(\"parquet://a_parquet_file_with_truncated_extension.parq\")"
  },
  {
    "objectID": "how_to/chain_expressions.html",
    "href": "how_to/chain_expressions.html",
    "title": "Chain expressions with the underscore API",
    "section": "",
    "text": "Expressions can easily be chained using the deferred expression API, also known as the Underscore (_) API.\nIn this guide, we use the _ API to concisely create column expressions and then chain table expressions.\n\n\nTo get started, import _ from ibis:\nimport ibis\nfrom ibis import _\n\nimport pandas as pd\nLet’s create two in-memory tables using ibis.memtable, an API introduced in 3.2:\nt1 = ibis.memtable(pd.DataFrame({'x': range(5), 'y': list('ab')*2 + list('e')}))\nt2 = ibis.memtable(pd.DataFrame({'x': range(10), 'z': list(reversed(list('ab')*2 + list('e')))*2}))\n\n\n\nWe can use _ to create new column expressions without explicit reference to the previous table expression:\n# We can pass a deferred expression into a function:\ndef modf(t):\n    return t.x % 3\n\nxmod = modf(_)\n\n# We can create ColumnExprs like aggregate expressions:\nymax = _.y.max()\nzmax = _.z.max()\nzct = _.z.count()\n\n\n\nWe can also use it to chain Ibis expressions in one Python expression:\njoin = (\n    t1\n    # _ is t1\n    .join(t2, _.x == t2.x)\n    # _ is the join result:\n    .mutate(xmod=xmod)\n    # _ is the TableExpression after mutate:\n    .group_by(_.xmod)\n    # `ct` is a ColumnExpression derived from a deferred expression:\n    .aggregate(ymax=ymax, zmax=zmax)\n    # _ is the aggregation result:\n    .filter(_.ymax == _.zmax)\n    # _ is the filtered result, and re-create xmod in t2 using modf:\n    .join(t2, _.xmod == modf(t2))\n    # _ is the second join result:\n    .join(t1, _.xmod == modf(t1))\n    # _ is the third join result:\n    .select(_.x, _.y, _.z)\n    # Finally, _ is the selection result:\n    .order_by(_.x)\n)"
  },
  {
    "objectID": "how_to/chain_expressions.html#setup",
    "href": "how_to/chain_expressions.html#setup",
    "title": "Chain expressions with the underscore API",
    "section": "",
    "text": "To get started, import _ from ibis:\nimport ibis\nfrom ibis import _\n\nimport pandas as pd\nLet’s create two in-memory tables using ibis.memtable, an API introduced in 3.2:\nt1 = ibis.memtable(pd.DataFrame({'x': range(5), 'y': list('ab')*2 + list('e')}))\nt2 = ibis.memtable(pd.DataFrame({'x': range(10), 'z': list(reversed(list('ab')*2 + list('e')))*2}))"
  },
  {
    "objectID": "how_to/chain_expressions.html#creating-column-expressions",
    "href": "how_to/chain_expressions.html#creating-column-expressions",
    "title": "Chain expressions with the underscore API",
    "section": "",
    "text": "We can use _ to create new column expressions without explicit reference to the previous table expression:\n# We can pass a deferred expression into a function:\ndef modf(t):\n    return t.x % 3\n\nxmod = modf(_)\n\n# We can create ColumnExprs like aggregate expressions:\nymax = _.y.max()\nzmax = _.z.max()\nzct = _.z.count()"
  },
  {
    "objectID": "how_to/chain_expressions.html#chaining-ibis-expressions",
    "href": "how_to/chain_expressions.html#chaining-ibis-expressions",
    "title": "Chain expressions with the underscore API",
    "section": "",
    "text": "We can also use it to chain Ibis expressions in one Python expression:\njoin = (\n    t1\n    # _ is t1\n    .join(t2, _.x == t2.x)\n    # _ is the join result:\n    .mutate(xmod=xmod)\n    # _ is the TableExpression after mutate:\n    .group_by(_.xmod)\n    # `ct` is a ColumnExpression derived from a deferred expression:\n    .aggregate(ymax=ymax, zmax=zmax)\n    # _ is the aggregation result:\n    .filter(_.ymax == _.zmax)\n    # _ is the filtered result, and re-create xmod in t2 using modf:\n    .join(t2, _.xmod == modf(t2))\n    # _ is the second join result:\n    .join(t1, _.xmod == modf(t1))\n    # _ is the third join result:\n    .select(_.x, _.y, _.z)\n    # Finally, _ is the selection result:\n    .order_by(_.x)\n)"
  },
  {
    "objectID": "how_to/streamlit.html",
    "href": "how_to/streamlit.html",
    "title": "Write a Streamlit app with Ibis",
    "section": "",
    "text": "Write a Streamlit app with Ibis\nStreamlit + Ibis = :heart:\nIbis supports the streamlit experimental_connection interface, making it easier than ever to combine the powers of both tools!\nCheck out the example application below that shows the top N ingredients from a corpus of recipes using the ClickHouse backend!\n\n\n\nAnd here’s the source code for the application:\n??? example “Source code”\n```python title=\"example_streamlit_app.py\"\n--8&lt;-- \"docs/example_streamlit_app/example_streamlit_app.py\"\n```"
  },
  {
    "objectID": "community/contribute/01_environment.html",
    "href": "community/contribute/01_environment.html",
    "title": "Setting Up a Development Environment",
    "section": "",
    "text": "git\n\n=== “Conda”\n!!! info \"Some optional dependencies for Windows are not available through `conda`/`mamba`\"\n\n    1. `clickhouse-cityhash`. Required for compression support in the ClickHouse backend.\n\n#### Support Matrix\n\n|      Python Version :material-arrow-right: |                      Python 3.9                  |                  Python 3.10                     |                  Python 3.11                     |\n| -----------------------------------------: | :----------------------------------------------: | :----------------------------------------------: | :----------------------------------------------: |\n| **Operating System** :material-arrow-down: |                                                  |                                                  |                                                  |\n|                                  **Linux** | {{ config.extra.support_levels.supported.icon }} | {{ config.extra.support_levels.supported.icon }} | {{ config.extra.support_levels.supported.icon }} |\n|                         **macOS (x86_64)** | {{ config.extra.support_levels.supported.icon }} | {{ config.extra.support_levels.supported.icon }} | {{ config.extra.support_levels.supported.icon }} |\n|                        **macOS (aarch64)** | {{ config.extra.support_levels.supported.icon }} | {{ config.extra.support_levels.supported.icon }} | {{ config.extra.support_levels.supported.icon }} |\n|                                **Windows** | {{ config.extra.support_levels.supported.icon }} | {{ config.extra.support_levels.supported.icon }} | {{ config.extra.support_levels.supported.icon }} |\n\n{% set managers = {\"conda\": {\"name\": \"Miniconda\", \"url\": \"https://docs.conda.io/en/latest/miniconda.html\"}, \"mamba\": {\"name\": \"Mamba\", \"url\": \"https://github.com/mamba-org/mamba\"}} %}\n{% for manager, params in managers.items() %}\n\n=== \"`{{ manager }}`\"\n\n    1. Install [{{ params[\"name\"] }}]({{ params[\"url\"] }})\n\n    1. Install `gh`\n\n        ```sh\n        {{ manager }} install -c conda-forge gh\n        ```\n\n    1. Fork and clone the ibis repository:\n\n        ```sh\n        gh repo fork --clone --remote ibis-project/ibis\n        ```\n\n    1. Create a Conda environment from a lock file in the repo:\n\n        {% set platforms = {\"Linux\": \"linux-64\", \"macOS (x86_64)\": \"osx-64\", \"macOS (aarch64)\": \"osx-arm64\", \"Windows\": \"win-64\"} %}\n        {% for os, platform in platforms.items() %}\n        === \"{{ os }}\"\n\n            ```sh\n            # Create a dev environment for {{platform}}\n            cd ibis\n            {{ manager }} create -n ibis-dev --file=conda-lock/{{ platform }}-3.10.lock\n            ```\n        {% endfor %}\n\n    1. Activate the environment\n\n        ```sh\n        {{ manager }} activate ibis-dev\n        ```\n\n    1. Install your local copy of `ibis` into the Conda environment.\n\n        ```sh\n        cd ibis\n        pip install -e .\n        ```\n\n{% endfor %}\n=== “pip”\n!!! warning \"`pip` will not handle installation of system dependencies\"\n\n    `pip` will not install system dependencies needed for some packages\n    such as `psycopg2` and `kerberos`.\n\n    For a better development experience see the `conda` or `nix` setup\n    instructions.\n\n1. [Install `gh`](https://cli.github.com/manual/installation)\n\n1. Fork and clone the ibis repository:\n\n    ```sh\n    gh repo fork --clone --remote ibis-project/ibis\n    ```\n\n1. Change directory into `ibis`:\n\n    ```sh\n    cd ibis\n    ```\n\n1. Install development dependencies\n\n    ```sh\n    pip install 'poetry&gt;=1.3,&lt;1.4'\n    pip install -r requirements.txt\n    ```\n\n1. Install ibis in development mode\n\n    ```sh\n    pip install -e .\n    ```\n=== “Nix”\n#### Support Matrix\n\n|      Python Version :material-arrow-right: |                     Python 3.9                     |                    Python 3.10                     |                    Python 3.11                     |\n| -----------------------------------------: | :------------------------------------------------: | :------------------------------------------------: | :------------------------------------------------: |\n| **Operating System** :material-arrow-down: |                                                    |                                                    |                                                    |\n|                                  **Linux** |  {{ config.extra.support_levels.supported.icon }}  |  {{ config.extra.support_levels.supported.icon }}  |  {{ config.extra.support_levels.supported.icon }}  |\n|                         **macOS (x86_64)** |  {{ config.extra.support_levels.supported.icon }}  |  {{ config.extra.support_levels.supported.icon }}  |  {{ config.extra.support_levels.supported.icon }}  |\n|                        **macOS (aarch64)** |   {{ config.extra.support_levels.unknown.icon }}   |   {{ config.extra.support_levels.unknown.icon }}   |   {{ config.extra.support_levels.unknown.icon }}   |\n|                                **Windows** | {{ config.extra.support_levels.unsupported.icon }} | {{ config.extra.support_levels.unsupported.icon }} | {{ config.extra.support_levels.unsupported.icon }} |\n\n1. [Install `nix`](https://nixos.org/download.html)\n1. Install `gh`:\n\n    === \"`nix-shell`\"\n\n        ```sh\n        nix-shell -p gh\n        ```\n\n    === \"`nix-env`\"\n\n        ```sh\n        nix-env -iA gh\n        ```\n\n1. Fork and clone the ibis repository:\n\n    ```sh\n    gh repo fork --clone --remote ibis-project/ibis\n    ```\n\n1. Set up the public `ibis` Cachix cache to pull pre-built dependencies:\n\n    ```sh\n    nix-shell -p cachix --run 'cachix use ibis'\n    ```\n\n1. Run `nix-shell` in the checkout directory:\n\n    ```sh\n    cd ibis\n    nix-shell\n    ```\n\n    This may take a while due to artifact download from the cache.\n\n\n\nRun\nmkdocs serve --strict\nto build and serve the documentation.\n{% for data in config.extra.support_levels.values() %} [^{{ loop.index }}]: {{ data.description }} {% endfor %}"
  },
  {
    "objectID": "community/contribute/01_environment.html#required-dependencies",
    "href": "community/contribute/01_environment.html#required-dependencies",
    "title": "Setting Up a Development Environment",
    "section": "",
    "text": "git\n\n=== “Conda”\n!!! info \"Some optional dependencies for Windows are not available through `conda`/`mamba`\"\n\n    1. `clickhouse-cityhash`. Required for compression support in the ClickHouse backend.\n\n#### Support Matrix\n\n|      Python Version :material-arrow-right: |                      Python 3.9                  |                  Python 3.10                     |                  Python 3.11                     |\n| -----------------------------------------: | :----------------------------------------------: | :----------------------------------------------: | :----------------------------------------------: |\n| **Operating System** :material-arrow-down: |                                                  |                                                  |                                                  |\n|                                  **Linux** | {{ config.extra.support_levels.supported.icon }} | {{ config.extra.support_levels.supported.icon }} | {{ config.extra.support_levels.supported.icon }} |\n|                         **macOS (x86_64)** | {{ config.extra.support_levels.supported.icon }} | {{ config.extra.support_levels.supported.icon }} | {{ config.extra.support_levels.supported.icon }} |\n|                        **macOS (aarch64)** | {{ config.extra.support_levels.supported.icon }} | {{ config.extra.support_levels.supported.icon }} | {{ config.extra.support_levels.supported.icon }} |\n|                                **Windows** | {{ config.extra.support_levels.supported.icon }} | {{ config.extra.support_levels.supported.icon }} | {{ config.extra.support_levels.supported.icon }} |\n\n{% set managers = {\"conda\": {\"name\": \"Miniconda\", \"url\": \"https://docs.conda.io/en/latest/miniconda.html\"}, \"mamba\": {\"name\": \"Mamba\", \"url\": \"https://github.com/mamba-org/mamba\"}} %}\n{% for manager, params in managers.items() %}\n\n=== \"`{{ manager }}`\"\n\n    1. Install [{{ params[\"name\"] }}]({{ params[\"url\"] }})\n\n    1. Install `gh`\n\n        ```sh\n        {{ manager }} install -c conda-forge gh\n        ```\n\n    1. Fork and clone the ibis repository:\n\n        ```sh\n        gh repo fork --clone --remote ibis-project/ibis\n        ```\n\n    1. Create a Conda environment from a lock file in the repo:\n\n        {% set platforms = {\"Linux\": \"linux-64\", \"macOS (x86_64)\": \"osx-64\", \"macOS (aarch64)\": \"osx-arm64\", \"Windows\": \"win-64\"} %}\n        {% for os, platform in platforms.items() %}\n        === \"{{ os }}\"\n\n            ```sh\n            # Create a dev environment for {{platform}}\n            cd ibis\n            {{ manager }} create -n ibis-dev --file=conda-lock/{{ platform }}-3.10.lock\n            ```\n        {% endfor %}\n\n    1. Activate the environment\n\n        ```sh\n        {{ manager }} activate ibis-dev\n        ```\n\n    1. Install your local copy of `ibis` into the Conda environment.\n\n        ```sh\n        cd ibis\n        pip install -e .\n        ```\n\n{% endfor %}\n=== “pip”\n!!! warning \"`pip` will not handle installation of system dependencies\"\n\n    `pip` will not install system dependencies needed for some packages\n    such as `psycopg2` and `kerberos`.\n\n    For a better development experience see the `conda` or `nix` setup\n    instructions.\n\n1. [Install `gh`](https://cli.github.com/manual/installation)\n\n1. Fork and clone the ibis repository:\n\n    ```sh\n    gh repo fork --clone --remote ibis-project/ibis\n    ```\n\n1. Change directory into `ibis`:\n\n    ```sh\n    cd ibis\n    ```\n\n1. Install development dependencies\n\n    ```sh\n    pip install 'poetry&gt;=1.3,&lt;1.4'\n    pip install -r requirements.txt\n    ```\n\n1. Install ibis in development mode\n\n    ```sh\n    pip install -e .\n    ```\n=== “Nix”\n#### Support Matrix\n\n|      Python Version :material-arrow-right: |                     Python 3.9                     |                    Python 3.10                     |                    Python 3.11                     |\n| -----------------------------------------: | :------------------------------------------------: | :------------------------------------------------: | :------------------------------------------------: |\n| **Operating System** :material-arrow-down: |                                                    |                                                    |                                                    |\n|                                  **Linux** |  {{ config.extra.support_levels.supported.icon }}  |  {{ config.extra.support_levels.supported.icon }}  |  {{ config.extra.support_levels.supported.icon }}  |\n|                         **macOS (x86_64)** |  {{ config.extra.support_levels.supported.icon }}  |  {{ config.extra.support_levels.supported.icon }}  |  {{ config.extra.support_levels.supported.icon }}  |\n|                        **macOS (aarch64)** |   {{ config.extra.support_levels.unknown.icon }}   |   {{ config.extra.support_levels.unknown.icon }}   |   {{ config.extra.support_levels.unknown.icon }}   |\n|                                **Windows** | {{ config.extra.support_levels.unsupported.icon }} | {{ config.extra.support_levels.unsupported.icon }} | {{ config.extra.support_levels.unsupported.icon }} |\n\n1. [Install `nix`](https://nixos.org/download.html)\n1. Install `gh`:\n\n    === \"`nix-shell`\"\n\n        ```sh\n        nix-shell -p gh\n        ```\n\n    === \"`nix-env`\"\n\n        ```sh\n        nix-env -iA gh\n        ```\n\n1. Fork and clone the ibis repository:\n\n    ```sh\n    gh repo fork --clone --remote ibis-project/ibis\n    ```\n\n1. Set up the public `ibis` Cachix cache to pull pre-built dependencies:\n\n    ```sh\n    nix-shell -p cachix --run 'cachix use ibis'\n    ```\n\n1. Run `nix-shell` in the checkout directory:\n\n    ```sh\n    cd ibis\n    nix-shell\n    ```\n\n    This may take a while due to artifact download from the cache."
  },
  {
    "objectID": "community/contribute/01_environment.html#building-the-docs",
    "href": "community/contribute/01_environment.html#building-the-docs",
    "title": "Setting Up a Development Environment",
    "section": "",
    "text": "Run\nmkdocs serve --strict\nto build and serve the documentation.\n{% for data in config.extra.support_levels.values() %} [^{{ loop.index }}]: {{ data.description }} {% endfor %}"
  },
  {
    "objectID": "community/contribute/02_workflow.html",
    "href": "community/contribute/02_workflow.html",
    "title": "Contribute to the Ibis Codebase",
    "section": "",
    "text": "First, set up a development environment.\n\n\n\nIf you find an issue you want to work on, write a comment with the text /take on the issue. GitHub will then assign the issue to you.\n\n\n\nTo run tests that do not require a backend:\npytest -m core\n\n\n!!! info “You may be able to skip this section”\nIf you haven't made changes to the core of ibis (e.g., `ibis/expr`)\nor any specific backends (`ibis/backends`) this material isn't necessary to\nfollow to make a pull request.\nFirst, we need to download example data to run the tests successfully:\njust download-data\nTo run the tests for a specific backend (e.g. sqlite):\npytest -m sqlite\n\n\n\n\nThese client-server backends need to be started before testing them. They can be started with docker-compose directly, or using the just tool.\n\nClickHouse: just up clickhouse\nPostgreSQL: just up postgres\nMySQL: just up mysql\nimpala: just up impala\n\n\n\nIf anything seems amiss with a backend, you can of course test it locally:\nexport PGPASSWORD=postgres\npsql -t -A -h localhost -U postgres -d ibis_testing -c \"select 'success'\"\n\n\n\n\nIbis follows the Conventional Commits structure. In brief, the commit summary should look like:\nfix(types): make all floats doubles\nThe type (e.g. fix) can be:\n\nfix: A bug fix. Correlates with PATCH in SemVer\nfeat: A new feature. Correlates with MINOR in SemVer\ndocs: Documentation only changes\nstyle: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) ` If the commit fixes a Github issue, add something like this to the bottom of the description:\nfixes #4242\n\n\n\n\nIbis follows the standard Git Pull Request process. The team will review the PR and merge when it’s ready."
  },
  {
    "objectID": "community/contribute/02_workflow.html#getting-started",
    "href": "community/contribute/02_workflow.html#getting-started",
    "title": "Contribute to the Ibis Codebase",
    "section": "",
    "text": "First, set up a development environment."
  },
  {
    "objectID": "community/contribute/02_workflow.html#taking-issues",
    "href": "community/contribute/02_workflow.html#taking-issues",
    "title": "Contribute to the Ibis Codebase",
    "section": "",
    "text": "If you find an issue you want to work on, write a comment with the text /take on the issue. GitHub will then assign the issue to you."
  },
  {
    "objectID": "community/contribute/02_workflow.html#running-the-test-suite",
    "href": "community/contribute/02_workflow.html#running-the-test-suite",
    "title": "Contribute to the Ibis Codebase",
    "section": "",
    "text": "To run tests that do not require a backend:\npytest -m core\n\n\n!!! info “You may be able to skip this section”\nIf you haven't made changes to the core of ibis (e.g., `ibis/expr`)\nor any specific backends (`ibis/backends`) this material isn't necessary to\nfollow to make a pull request.\nFirst, we need to download example data to run the tests successfully:\njust download-data\nTo run the tests for a specific backend (e.g. sqlite):\npytest -m sqlite"
  },
  {
    "objectID": "community/contribute/02_workflow.html#setting-up-non-trivial-backends",
    "href": "community/contribute/02_workflow.html#setting-up-non-trivial-backends",
    "title": "Contribute to the Ibis Codebase",
    "section": "",
    "text": "These client-server backends need to be started before testing them. They can be started with docker-compose directly, or using the just tool.\n\nClickHouse: just up clickhouse\nPostgreSQL: just up postgres\nMySQL: just up mysql\nimpala: just up impala\n\n\n\nIf anything seems amiss with a backend, you can of course test it locally:\nexport PGPASSWORD=postgres\npsql -t -A -h localhost -U postgres -d ibis_testing -c \"select 'success'\""
  },
  {
    "objectID": "community/contribute/02_workflow.html#writing-the-commit",
    "href": "community/contribute/02_workflow.html#writing-the-commit",
    "title": "Contribute to the Ibis Codebase",
    "section": "",
    "text": "Ibis follows the Conventional Commits structure. In brief, the commit summary should look like:\nfix(types): make all floats doubles\nThe type (e.g. fix) can be:\n\nfix: A bug fix. Correlates with PATCH in SemVer\nfeat: A new feature. Correlates with MINOR in SemVer\ndocs: Documentation only changes\nstyle: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) ` If the commit fixes a Github issue, add something like this to the bottom of the description:\nfixes #4242"
  },
  {
    "objectID": "community/contribute/02_workflow.html#submit-a-pr",
    "href": "community/contribute/02_workflow.html#submit-a-pr",
    "title": "Contribute to the Ibis Codebase",
    "section": "",
    "text": "Ibis follows the standard Git Pull Request process. The team will review the PR and merge when it’s ready."
  },
  {
    "objectID": "community/contribute/index.html",
    "href": "community/contribute/index.html",
    "title": "Contribute to Ibis",
    "section": "",
    "text": "{{ config.extra.project_name }} is developed and maintained by a community of volunteer contributors.\n{% for group in config.extra.team %}\n\n\n{% for person in group.members %}\n\nhttps://github.com/{{ person }} {% endfor %}\n\n{% endfor %}"
  },
  {
    "objectID": "community/contribute/index.html#group.name",
    "href": "community/contribute/index.html#group.name",
    "title": "Contribute to Ibis",
    "section": "",
    "text": "{% for person in group.members %}\n\nhttps://github.com/{{ person }} {% endfor %}\n\n{% endfor %}"
  },
  {
    "objectID": "community/contribute/05_maintainers_guide.html",
    "href": "community/contribute/05_maintainers_guide.html",
    "title": "Maintaining the Codebase",
    "section": "",
    "text": "Ibis maintainers are expected to handle the following tasks as they arise:\n\nReviewing and merging pull requests\nTriaging new issues\n\n\n\nA number of tasks that are typically associated with maintenance are partially or fully automated.\n\nWhiteSource Renovate (Python library dependencies and GitHub Actions)\nCustom GitHub Action (Nix dependencies)\n\n\n\nOccasionally you may need to lock poetry dependencies. Edit pyproject.toml as needed, then run:\npoetry lock --no-update\n\n\n\n\nIf you’re not a maintainer, please open an issue asking us to add your example.\n\n\nYou need the ability to write to the gs://ibis-examples GCS bucket to add an example.\n\n\n\nMake sure you’re in the root of the ibis git repository.\nAssuming your file is called example.csv:\n\nAdd a gzip-compressed CSV file with the path ibis/examples/data/example.csv.gz.\nAdd a file named ibis/examples/descriptions/example that contains a description of your example. One line is best, but not necessary.\nRun one of the following from the git root of an ibis clone:\n\npython ibis/examples/gen_registry.py (doesn’t include R dependenices)\nnix run '.#gen-examples' (includes R dependenices)\n\n\n\n\n\n\nIbis is released on PyPI and Conda Forge.\n=== “PyPI”\nReleases to PyPI are handled automatically using [semantic\nrelease](https://egghead.io/lessons/javascript-automating-releases-with-semantic-release).\n\nTo trigger a release use the [Release GitHub Action](https://github.com/ibis-project/ibis/actions/workflows/release.yml).\n=== “conda-forge”\nThe conda-forge package is maintained as a [conda-forge feedstock](https://github.com/conda-forge/ibis-framework-feedstock).\n\nAfter a release to PyPI, the conda-forge bot automatically updates the ibis\npackage."
  },
  {
    "objectID": "community/contribute/05_maintainers_guide.html#dependencies",
    "href": "community/contribute/05_maintainers_guide.html#dependencies",
    "title": "Maintaining the Codebase",
    "section": "",
    "text": "A number of tasks that are typically associated with maintenance are partially or fully automated.\n\nWhiteSource Renovate (Python library dependencies and GitHub Actions)\nCustom GitHub Action (Nix dependencies)\n\n\n\nOccasionally you may need to lock poetry dependencies. Edit pyproject.toml as needed, then run:\npoetry lock --no-update"
  },
  {
    "objectID": "community/contribute/05_maintainers_guide.html#adding-examples",
    "href": "community/contribute/05_maintainers_guide.html#adding-examples",
    "title": "Maintaining the Codebase",
    "section": "",
    "text": "If you’re not a maintainer, please open an issue asking us to add your example.\n\n\nYou need the ability to write to the gs://ibis-examples GCS bucket to add an example.\n\n\n\nMake sure you’re in the root of the ibis git repository.\nAssuming your file is called example.csv:\n\nAdd a gzip-compressed CSV file with the path ibis/examples/data/example.csv.gz.\nAdd a file named ibis/examples/descriptions/example that contains a description of your example. One line is best, but not necessary.\nRun one of the following from the git root of an ibis clone:\n\npython ibis/examples/gen_registry.py (doesn’t include R dependenices)\nnix run '.#gen-examples' (includes R dependenices)"
  },
  {
    "objectID": "community/contribute/05_maintainers_guide.html#release",
    "href": "community/contribute/05_maintainers_guide.html#release",
    "title": "Maintaining the Codebase",
    "section": "",
    "text": "Ibis is released on PyPI and Conda Forge.\n=== “PyPI”\nReleases to PyPI are handled automatically using [semantic\nrelease](https://egghead.io/lessons/javascript-automating-releases-with-semantic-release).\n\nTo trigger a release use the [Release GitHub Action](https://github.com/ibis-project/ibis/actions/workflows/release.yml).\n=== “conda-forge”\nThe conda-forge package is maintained as a [conda-forge feedstock](https://github.com/conda-forge/ibis-framework-feedstock).\n\nAfter a release to PyPI, the conda-forge bot automatically updates the ibis\npackage."
  },
  {
    "objectID": "community/contribute/03_style.html",
    "href": "community/contribute/03_style.html",
    "title": "Style and Formatting",
    "section": "",
    "text": "black: Formatting Python code\nruff: Formatting and sorting import statements\nshellcheck: Linting shell scripts\nshfmt: Formatting shell scripts\nstatix: Linting nix files\nnixpkgs-fmt: Formatting nix files\n\n!!! tip\nIf you use `nix-shell`, all of these are already setup for you and ready to use, and you don't need to do anything to install these tools.\nWe use numpydoc as our standard format for docstrings."
  },
  {
    "objectID": "community/contribute/03_style.html#code-style",
    "href": "community/contribute/03_style.html#code-style",
    "title": "Style and Formatting",
    "section": "",
    "text": "black: Formatting Python code\nruff: Formatting and sorting import statements\nshellcheck: Linting shell scripts\nshfmt: Formatting shell scripts\nstatix: Linting nix files\nnixpkgs-fmt: Formatting nix files\n\n!!! tip\nIf you use `nix-shell`, all of these are already setup for you and ready to use, and you don't need to do anything to install these tools.\nWe use numpydoc as our standard format for docstrings."
  },
  {
    "objectID": "community/index.html",
    "href": "community/index.html",
    "title": "Community",
    "section": "",
    "text": "Community\nIbis aims to be a welcoming, friendly, diverse and inclusive community. Everybody is welcome, regardless of gender, sexual orientation, gender identity, and expression, disability, physical appearance, body size, race, or religion."
  },
  {
    "objectID": "reference/top_level.html",
    "href": "reference/top_level.html",
    "title": "Top-level APIs",
    "section": "",
    "text": "These methods and objecst are available directly on the ibis module."
  },
  {
    "objectID": "reference/top_level.html#parameters",
    "href": "reference/top_level.html#parameters",
    "title": "Top-level APIs",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npredicates\nibis.BooleanValue\nBoolean value expressions\n()"
  },
  {
    "objectID": "reference/top_level.html#returns",
    "href": "reference/top_level.html#returns",
    "title": "Top-level APIs",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nA new predicate that evaluates to True if all composing predicates are True. If no predicates were provided, returns True."
  },
  {
    "objectID": "reference/expr.schema.Schema.html",
    "href": "reference/expr.schema.Schema.html",
    "title": "expr.schema.Schema",
    "section": "",
    "text": "expr.schema.Schema()\nAn object for holding table schema information.\n\n\n\n\n\nName\nDescription\n\n\n\n\nfields\nA mapping of [str][str] to [DataType][ibis.expr.datatypes.DataType] objects\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nequals\nReturn whether other is equal to self.\n\n\nfrom_dask\nReturn the equivalent ibis schema.\n\n\nfrom_numpy\nReturn the equivalent ibis schema.\n\n\nfrom_pandas\nReturn the equivalent ibis schema.\n\n\nfrom_pyarrow\nReturn the equivalent ibis schema.\n\n\nfrom_tuples\nConstruct a Schema from an iterable of pairs.\n\n\nname_at_position\nReturn the name of a schema column at position i.\n\n\nto_dask\nReturn the equivalent dask dtypes.\n\n\nto_numpy\nReturn the equivalent numpy dtypes.\n\n\nto_pandas\nReturn the equivalent pandas datatypes.\n\n\nto_pyarrow\nReturn the equivalent pyarrow schema.\n\n\n\n\n\nexpr.schema.Schema.equals(self, other)\nReturn whether other is equal to self.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nSchema\nSchema to compare self to.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; first = ibis.schema({\"a\": \"int\"})\n&gt;&gt;&gt; second = ibis.schema({\"a\": \"int\"})\n&gt;&gt;&gt; assert first.equals(second)\n&gt;&gt;&gt; third = ibis.schema({\"a\": \"array&lt;int&gt;\"})\n&gt;&gt;&gt; assert not first.equals(third)\n\n\n\n\nexpr.schema.Schema.from_dask(cls, dask_schema)\nReturn the equivalent ibis schema.\n\n\n\nexpr.schema.Schema.from_numpy(cls, numpy_schema)\nReturn the equivalent ibis schema.\n\n\n\nexpr.schema.Schema.from_pandas(cls, pandas_schema)\nReturn the equivalent ibis schema.\n\n\n\nexpr.schema.Schema.from_pyarrow(cls, pyarrow_schema)\nReturn the equivalent ibis schema.\n\n\n\nexpr.schema.Schema.from_tuples(cls, values)\nConstruct a Schema from an iterable of pairs.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\ncollections.abc.Iterable[tuple[str, str | ibis.DataType]]\nAn iterable of pairs of name and type.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nSchema\nA new schema\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.Schema.from_tuples([(\"a\", \"int\"), (\"b\", \"string\")])\nibis.Schema {\n  a  int64\n  b  string\n}\n\n\n\n\nexpr.schema.Schema.name_at_position(self, i)\nReturn the name of a schema column at position i.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ni\nint\nThe position of the column\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe name of the column in the schema at position i.\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; sch = ibis.Schema({\"a\": \"int\", \"b\": \"string\"})\n&gt;&gt;&gt; sch.name_at_position(0)\n'a'\n&gt;&gt;&gt; sch.name_at_position(1)\n'b'\n\n\n\n\nexpr.schema.Schema.to_dask(self)\nReturn the equivalent dask dtypes.\n\n\n\nexpr.schema.Schema.to_numpy(self)\nReturn the equivalent numpy dtypes.\n\n\n\nexpr.schema.Schema.to_pandas(self)\nReturn the equivalent pandas datatypes.\n\n\n\nexpr.schema.Schema.to_pyarrow(self)\nReturn the equivalent pyarrow schema."
  },
  {
    "objectID": "reference/expr.schema.Schema.html#attributes",
    "href": "reference/expr.schema.Schema.html#attributes",
    "title": "expr.schema.Schema",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfields\nA mapping of [str][str] to [DataType][ibis.expr.datatypes.DataType] objects"
  },
  {
    "objectID": "reference/expr.schema.Schema.html#methods",
    "href": "reference/expr.schema.Schema.html#methods",
    "title": "expr.schema.Schema",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nequals\nReturn whether other is equal to self.\n\n\nfrom_dask\nReturn the equivalent ibis schema.\n\n\nfrom_numpy\nReturn the equivalent ibis schema.\n\n\nfrom_pandas\nReturn the equivalent ibis schema.\n\n\nfrom_pyarrow\nReturn the equivalent ibis schema.\n\n\nfrom_tuples\nConstruct a Schema from an iterable of pairs.\n\n\nname_at_position\nReturn the name of a schema column at position i.\n\n\nto_dask\nReturn the equivalent dask dtypes.\n\n\nto_numpy\nReturn the equivalent numpy dtypes.\n\n\nto_pandas\nReturn the equivalent pandas datatypes.\n\n\nto_pyarrow\nReturn the equivalent pyarrow schema.\n\n\n\n\n\nexpr.schema.Schema.equals(self, other)\nReturn whether other is equal to self.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nSchema\nSchema to compare self to.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; first = ibis.schema({\"a\": \"int\"})\n&gt;&gt;&gt; second = ibis.schema({\"a\": \"int\"})\n&gt;&gt;&gt; assert first.equals(second)\n&gt;&gt;&gt; third = ibis.schema({\"a\": \"array&lt;int&gt;\"})\n&gt;&gt;&gt; assert not first.equals(third)\n\n\n\n\nexpr.schema.Schema.from_dask(cls, dask_schema)\nReturn the equivalent ibis schema.\n\n\n\nexpr.schema.Schema.from_numpy(cls, numpy_schema)\nReturn the equivalent ibis schema.\n\n\n\nexpr.schema.Schema.from_pandas(cls, pandas_schema)\nReturn the equivalent ibis schema.\n\n\n\nexpr.schema.Schema.from_pyarrow(cls, pyarrow_schema)\nReturn the equivalent ibis schema.\n\n\n\nexpr.schema.Schema.from_tuples(cls, values)\nConstruct a Schema from an iterable of pairs.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\ncollections.abc.Iterable[tuple[str, str | ibis.DataType]]\nAn iterable of pairs of name and type.\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nSchema\nA new schema\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.Schema.from_tuples([(\"a\", \"int\"), (\"b\", \"string\")])\nibis.Schema {\n  a  int64\n  b  string\n}\n\n\n\n\nexpr.schema.Schema.name_at_position(self, i)\nReturn the name of a schema column at position i.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ni\nint\nThe position of the column\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe name of the column in the schema at position i.\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; sch = ibis.Schema({\"a\": \"int\", \"b\": \"string\"})\n&gt;&gt;&gt; sch.name_at_position(0)\n'a'\n&gt;&gt;&gt; sch.name_at_position(1)\n'b'\n\n\n\n\nexpr.schema.Schema.to_dask(self)\nReturn the equivalent dask dtypes.\n\n\n\nexpr.schema.Schema.to_numpy(self)\nReturn the equivalent numpy dtypes.\n\n\n\nexpr.schema.Schema.to_pandas(self)\nReturn the equivalent pandas datatypes.\n\n\n\nexpr.schema.Schema.to_pyarrow(self)\nReturn the equivalent pyarrow schema."
  },
  {
    "objectID": "reference/config.Repr.html",
    "href": "reference/config.Repr.html",
    "title": "config.Repr",
    "section": "",
    "text": "config.Repr()\nExpression printing options.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndepth\nint\nThe maximum number of expression nodes to print when repring.\n\n\ntable_columns\nint\nThe number of columns to show in leaf table expressions.\n\n\nquery_text_length\nint\nThe maximum number of characters to show in the query field repr of SQLQueryResult operations.\n\n\nshow_types\nbool\nShow the inferred type of value expressions in the repr.\n\n\ninteractive\nbool\nOptions controlling the interactive repr."
  },
  {
    "objectID": "reference/config.Repr.html#attributes",
    "href": "reference/config.Repr.html#attributes",
    "title": "config.Repr",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ndepth\nint\nThe maximum number of expression nodes to print when repring.\n\n\ntable_columns\nint\nThe number of columns to show in leaf table expressions.\n\n\nquery_text_length\nint\nThe maximum number of characters to show in the query field repr of SQLQueryResult operations.\n\n\nshow_types\nbool\nShow the inferred type of value expressions in the repr.\n\n\ninteractive\nbool\nOptions controlling the interactive repr."
  },
  {
    "objectID": "reference/expression-strings.html",
    "href": "reference/expression-strings.html",
    "title": "String Expressions",
    "section": "",
    "text": "All string operations are valid for both scalars and columns."
  },
  {
    "objectID": "reference/expression-strings.html#methods",
    "href": "reference/expression-strings.html#methods",
    "title": "String Expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nascii_str\nReturn the numeric ASCII code of the first character of a string.\n\n\nauthority\nParse a URL and extract authority.\n\n\ncapitalize\nCapitalize the input string.\n\n\nconcat\nConcatenate strings.\n\n\ncontains\nReturn whether the expression contains substr.\n\n\nconvert_base\nConvert a string representing an integer from one base to another.\n\n\nendswith\nDetermine if self ends with end.\n\n\nfile\nParse a URL and extract file.\n\n\nfind\nReturn the position of the first occurrence of substring.\n\n\nfind_in_set\nFind the first occurrence of str_list within a list of strings.\n\n\nfragment\nParse a URL and extract fragment identifier.\n\n\nhashbytes\nCompute the binary hash value of the input.\n\n\nhost\nParse a URL and extract host.\n\n\nilike\nMatch patterns against self, case-insensitive.\n\n\njoin\nJoin a list of strings using self as the separator.\n\n\nleft\nReturn the nchars left-most characters.\n\n\nlength\nCompute the length of a string.\n\n\nlike\nMatch patterns against self, case-sensitive.\n\n\nlower\nConvert string to all lowercase.\n\n\nlpad\nPad arg by truncating on the right or padding on the left.\n\n\nlstrip\nRemove whitespace from the left side of string.\n\n\npath\nParse a URL and extract path.\n\n\nprotocol\nParse a URL and extract protocol.\n\n\nquery\nParse a URL and returns query strring or query string parameter.\n\n\nre_extract\nReturn the specified match at index from a regex pattern.\n\n\nre_replace\nReplace match found by regex pattern with replacement.\n\n\nre_search\nReturn whether the values match pattern.\n\n\nrepeat\nRepeat a string n times.\n\n\nreplace\nReplace each exact match of pattern with replacement.\n\n\nreverse\nReverse the characters of a string.\n\n\nright\nReturn up to nchars from the end of each string.\n\n\nrpad\nPad self by truncating or padding on the right.\n\n\nrstrip\nRemove whitespace from the right side of string.\n\n\nsplit\nSplit as string on delimiter.\n\n\nstartswith\nDetermine whether self starts with end.\n\n\nstrip\nRemove whitespace from left and right sides of a string.\n\n\nsubstr\nExtract a substring.\n\n\nto_timestamp\nParse a string and return a timestamp.\n\n\ntranslate\nReplace from_str characters in self characters in to_str.\n\n\nupper\nConvert string to all uppercase.\n\n\nuserinfo\nParse a URL and extract user info.\n\n\n\n\nascii_str\nexpr.types.strings.StringValue.ascii_str(self)\nReturn the numeric ASCII code of the first character of a string.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nASCII code of the first character of the input\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"def\", \"ghi\"]})\n&gt;&gt;&gt; t.s.ascii_str()\n┏━━━━━━━━━━━━━━━━┓\n┃ StringAscii(s) ┃\n┡━━━━━━━━━━━━━━━━┩\n│ int32          │\n├────────────────┤\n│             97 │\n│            100 │\n│            103 │\n└────────────────┘\n\n\n\nauthority\nexpr.types.strings.StringValue.authority(self)\nParse a URL and extract authority.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; url = ibis.literal(\"https://user:pass@example.com:80/docs/books\")\n&gt;&gt;&gt; result = url.authority()  # user:pass@example.com:80\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted string value\n\n\n\n\n\n\ncapitalize\nexpr.types.strings.StringValue.capitalize(self)\nCapitalize the input string.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nCapitalized string\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"def\", \"ghi\"]})\n&gt;&gt;&gt; t.s.capitalize()\n┏━━━━━━━━━━━━━━━┓\n┃ Capitalize(s) ┃\n┡━━━━━━━━━━━━━━━┩\n│ string        │\n├───────────────┤\n│ Abc           │\n│ Def           │\n│ Ghi           │\n└───────────────┘\n\n\n\nconcat\nexpr.types.strings.StringValue.concat(self, other, *args)\nConcatenate strings.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nstr | ibis.expr.types.strings.StringValue\nString to concatenate\nrequired\n\n\nargs\nstr | ibis.expr.types.strings.StringValue\nAdditional strings to concatenate\n()\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nAll strings concatenated\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"bac\", \"bca\"]})\n&gt;&gt;&gt; t.s.concat(\"xyz\")\n┏━━━━━━━━━━━━━━━━┓\n┃ StringConcat() ┃\n┡━━━━━━━━━━━━━━━━┩\n│ string         │\n├────────────────┤\n│ abcxyz         │\n│ bacxyz         │\n│ bcaxyz         │\n└────────────────┘\n\n\n\ncontains\nexpr.types.strings.StringValue.contains(self, substr)\nReturn whether the expression contains substr.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsubstr\nstr | ibis.expr.types.strings.StringValue\nSubstring for which to check\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nBoolean indicating the presence of substr in the expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"bab\", \"ddd\", \"eaf\"]})\n&gt;&gt;&gt; t.s.contains(\"a\")\n┏━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ StringContains(s, 'a') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                │\n├────────────────────────┤\n│ True                   │\n│ False                  │\n│ True                   │\n└────────────────────────┘\n\n\n\nconvert_base\nexpr.types.strings.StringValue.convert_base(self, from_base, to_base)\nConvert a string representing an integer from one base to another.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfrom_base\nint | ibis.expr.types.IntegerValue\nNumeric base of the expression\nrequired\n\n\nto_base\nint | ibis.expr.types.IntegerValue\nNew base\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nConverted expression\n\n\n\n\n\n\nendswith\nexpr.types.strings.StringValue.endswith(self, end)\nDetermine if self ends with end.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nend\nstr | ibis.expr.types.strings.StringValue\nSuffix to check for\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nBoolean indicating whether self ends with end\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"Ibis project\", \"GitHub\"]})\n&gt;&gt;&gt; t.s.endswith(\"project\")\n┏━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ EndsWith(s, 'project') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                │\n├────────────────────────┤\n│ True                   │\n│ False                  │\n└────────────────────────┘\n\n\n\nfile\nexpr.types.strings.StringValue.file(self)\nParse a URL and extract file.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; url = ibis.literal(\"https://example.com:80/docs/books/tutorial/index.html?name=networking\")\n&gt;&gt;&gt; result = url.file()  # docs/books/tutorial/index.html?name=networking\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted string value\n\n\n\n\n\n\nfind\nexpr.types.strings.StringValue.find(self, substr, start=None, end=None)\nReturn the position of the first occurrence of substring.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsubstr\nstr | ibis.expr.types.strings.StringValue\nSubstring to search for\nrequired\n\n\nstart\nint | ibis.expr.types.IntegerValue | None\nZero based index of where to start the search\nNone\n\n\nend\nint | ibis.expr.types.IntegerValue | None\nZero based index of where to stop the search. Currently not implemented.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nPosition of substr in arg starting from start\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"bac\", \"bca\"]})\n&gt;&gt;&gt; t.s.find(\"a\")\n┏━━━━━━━━━━━━━━━━━━━━┓\n┃ StringFind(s, 'a') ┃\n┡━━━━━━━━━━━━━━━━━━━━┩\n│ int64              │\n├────────────────────┤\n│                  0 │\n│                  1 │\n│                  2 │\n└────────────────────┘\n&gt;&gt;&gt; t.s.find(\"z\")\n┏━━━━━━━━━━━━━━━━━━━━┓\n┃ StringFind(s, 'z') ┃\n┡━━━━━━━━━━━━━━━━━━━━┩\n│ int64              │\n├────────────────────┤\n│                 -1 │\n│                 -1 │\n│                 -1 │\n└────────────────────┘\n\n\n\nfind_in_set\nexpr.types.strings.StringValue.find_in_set(self, str_list)\nFind the first occurrence of str_list within a list of strings.\nNo string in str_list can have a comma.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstr_list\ntyping.Sequence[str]\nSequence of strings\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nPosition of str_list in self. Returns -1 if self isn’t found or if self contains ','.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; table = ibis.table(dict(string_col='string'))\n&gt;&gt;&gt; result = table.string_col.find_in_set(['a', 'b'])\n\n\n\nfragment\nexpr.types.strings.StringValue.fragment(self)\nParse a URL and extract fragment identifier.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; url = ibis.literal(\"https://example.com:80/docs/#DOWNLOADING\")\n&gt;&gt;&gt; result = url.fragment()  # DOWNLOADING\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted string value\n\n\n\n\n\n\nhashbytes\nexpr.types.strings.StringValue.hashbytes(self, how='sha256')\nCompute the binary hash value of the input.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nhow\ntyping.Literal[‘md5’, ‘sha1’, ‘sha256’, ‘sha512’]\nHash algorithm to use\n'sha256'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBinaryValue\nBinary expression\n\n\n\n\n\n\nhost\nexpr.types.strings.StringValue.host(self)\nParse a URL and extract host.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; url = ibis.literal(\"https://user:pass@example.com:80/docs/books\")\n&gt;&gt;&gt; result = url.host()  # example.com\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted string value\n\n\n\n\n\n\nilike\nexpr.types.strings.StringValue.ilike(self, patterns)\nMatch patterns against self, case-insensitive.\nThis function is modeled after SQL’s ILIKE directive. Use % as a multiple-character wildcard or _ as a single-character wildcard.\nUse re_search or rlike for regular expression-based matching.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npatterns\nstr | ibis.expr.types.strings.StringValue | typing.Iterable[str | ibis.expr.types.strings.StringValue]\nIf pattern is a list, then if any pattern matches the input then the corresponding row in the output is True.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nColumn indicating matches\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"Ibis project\", \"GitHub\"]})\n&gt;&gt;&gt; t.s.ilike(\"%PROJect\")\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ StringSQLILike(s, '%PROJect') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                       │\n├───────────────────────────────┤\n│ True                          │\n│ False                         │\n└───────────────────────────────┘\n\n\n\njoin\nexpr.types.strings.StringValue.join(self, strings)\nJoin a list of strings using self as the separator.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstrings\ntyping.Sequence[str | ibis.expr.types.strings.StringValue] | ibis.expr.types.ArrayValue\nStrings to join with arg\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nJoined string\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"arr\": [[\"a\", \"b\", \"c\"], None, [], [\"b\", None]]})\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ arr                  ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;string&gt;        │\n├──────────────────────┤\n│ ['a', 'b', ... +1]   │\n│ NULL                 │\n│ []                   │\n│ ['b', None]          │\n└──────────────────────┘\n&gt;&gt;&gt; ibis.literal(\"|\").join(t.arr)\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayStringJoin('|', arr) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                    │\n├───────────────────────────┤\n│ a|b|c                     │\n│ NULL                      │\n│ NULL                      │\n│ b                         │\n└───────────────────────────┘\n\n\nSee Also\n[ArrayValue.join][ibis.expr.types.arrays.ArrayValue.join]\n\n\n\nleft\nexpr.types.strings.StringValue.left(self, nchars)\nReturn the nchars left-most characters.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnchars\nint | ibis.expr.types.IntegerValue\nMaximum number of characters to return\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nCharacters from the start\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"defg\", \"hijlk\"]})\n&gt;&gt;&gt; t.s.left(2)\n┏━━━━━━━━━━━━━━━━━━━━┓\n┃ Substring(s, 0, 2) ┃\n┡━━━━━━━━━━━━━━━━━━━━┩\n│ string             │\n├────────────────────┤\n│ ab                 │\n│ de                 │\n│ hi                 │\n└────────────────────┘\n\n\n\nlength\nexpr.types.strings.StringValue.length(self)\nCompute the length of a string.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nThe length of each string in the expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"aaa\", \"a\", \"aa\"]})\n&gt;&gt;&gt; t.s.length()\n┏━━━━━━━━━━━━━━━━━┓\n┃ StringLength(s) ┃\n┡━━━━━━━━━━━━━━━━━┩\n│ int32           │\n├─────────────────┤\n│               3 │\n│               1 │\n│               2 │\n└─────────────────┘\n\n\n\nlike\nexpr.types.strings.StringValue.like(self, patterns)\nMatch patterns against self, case-sensitive.\nThis function is modeled after the SQL LIKE directive. Use % as a multiple-character wildcard or _ as a single-character wildcard.\nUse re_search or rlike for regular expression-based matching.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npatterns\nstr | ibis.expr.types.strings.StringValue | typing.Iterable[str | ibis.expr.types.strings.StringValue]\nIf pattern is a list, then if any pattern matches the input then the corresponding row in the output is True.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nColumn indicating matches\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"Ibis project\", \"GitHub\"]})\n&gt;&gt;&gt; t.s.like(\"%project\")\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ StringSQLLike(s, '%project') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                      │\n├──────────────────────────────┤\n│ True                         │\n│ False                        │\n└──────────────────────────────┘\n\n\n\nlower\nexpr.types.strings.StringValue.lower(self)\nConvert string to all lowercase.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nLowercase string\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"AAA\", \"a\", \"AA\"]})\n&gt;&gt;&gt; t\n┏━━━━━━━━┓\n┃ s      ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ AAA    │\n│ a      │\n│ AA     │\n└────────┘\n&gt;&gt;&gt; t.s.lower()\n┏━━━━━━━━━━━━━━┓\n┃ Lowercase(s) ┃\n┡━━━━━━━━━━━━━━┩\n│ string       │\n├──────────────┤\n│ aaa          │\n│ a            │\n│ aa           │\n└──────────────┘\n\n\n\nlpad\nexpr.types.strings.StringValue.lpad(self, length, pad=' ')\nPad arg by truncating on the right or padding on the left.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlength\nint | ibis.expr.types.IntegerValue\nLength of output string\nrequired\n\n\npad\nstr | ibis.expr.types.strings.StringValue\nPad character\n' '\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nLeft-padded string\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"def\", \"ghij\"]})\n&gt;&gt;&gt; t.s.lpad(5, \"-\")\n┏━━━━━━━━━━━━━━━━━┓\n┃ LPad(s, 5, '-') ┃\n┡━━━━━━━━━━━━━━━━━┩\n│ string          │\n├─────────────────┤\n│ --abc           │\n│ --def           │\n│ -ghij           │\n└─────────────────┘\n\n\n\nlstrip\nexpr.types.strings.StringValue.lstrip(self)\nRemove whitespace from the left side of string.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nLeft-stripped string\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"\\ta\\t\", \"\\nb\\n\", \"\\vc\\t\"]})\n&gt;&gt;&gt; t\n┏━━━━━━━━┓\n┃ s      ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ \\ta\\t  │\n│ \\nb\\n  │\n│ \\vc\\t  │\n└────────┘\n&gt;&gt;&gt; t.s.lstrip()\n┏━━━━━━━━━━━┓\n┃ LStrip(s) ┃\n┡━━━━━━━━━━━┩\n│ string    │\n├───────────┤\n│ a\\t       │\n│ b\\n       │\n│ c\\t       │\n└───────────┘\n\n\n\npath\nexpr.types.strings.StringValue.path(self)\nParse a URL and extract path.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; url = ibis.literal(\"https://example.com:80/docs/books/tutorial/index.html?name=networking\")\n&gt;&gt;&gt; result = url.path()  # docs/books/tutorial/index.html\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted string value\n\n\n\n\n\n\nprotocol\nexpr.types.strings.StringValue.protocol(self)\nParse a URL and extract protocol.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; url = ibis.literal(\"https://user:pass@example.com:80/docs/books\")\n&gt;&gt;&gt; result = url.protocol()  # https\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted string value\n\n\n\n\n\n\nquery\nexpr.types.strings.StringValue.query(self, key=None)\nParse a URL and returns query strring or query string parameter.\nIf key is passed, return the value of the query string parameter named. If key is absent, return the query string.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nstr | ibis.expr.types.strings.StringValue | None\nQuery component to extract\nNone\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; url = ibis.literal(\"https://example.com:80/docs/books/tutorial/index.html?name=networking\")\n&gt;&gt;&gt; result = url.query()  # name=networking\n&gt;&gt;&gt; query_name = url.query('name')  # networking\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted string value\n\n\n\n\n\n\nre_extract\nexpr.types.strings.StringValue.re_extract(self, pattern, index)\nReturn the specified match at index from a regex pattern.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npattern\nstr | ibis.expr.types.strings.StringValue\nRegular expression pattern string\nrequired\n\n\nindex\nint | ibis.expr.types.IntegerValue\nThe index of the match group to return. The behavior of this function follows the behavior of Python’s match objects: when index is zero and there’s a match, return the entire match, otherwise return the content of the index-th match group.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted match or whole string if index is zero\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"bac\", \"bca\"]})\nExtract a specific group\n&gt;&gt;&gt; t.s.re_extract(r\"^(a)bc\", 1)\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ RegexExtract(s, '^(a)bc', 1) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                       │\n├──────────────────────────────┤\n│ a                            │\n│ ~                            │\n│ ~                            │\n└──────────────────────────────┘\nExtract the entire match\n&gt;&gt;&gt; t.s.re_extract(r\"^(a)bc\", 0)\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ RegexExtract(s, '^(a)bc', 0) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                       │\n├──────────────────────────────┤\n│ abc                          │\n│ ~                            │\n│ ~                            │\n└──────────────────────────────┘\n\n\n\nre_replace\nexpr.types.strings.StringValue.re_replace(self, pattern, replacement)\nReplace match found by regex pattern with replacement.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npattern\nstr | ibis.expr.types.strings.StringValue\nRegular expression string\nrequired\n\n\nreplacement\nstr | ibis.expr.types.strings.StringValue\nReplacement string or regular expression\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nModified string\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"bac\", \"bca\"]})\n&gt;&gt;&gt; t.s.re_replace(\"^(a)\", \"b\")\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ RegexReplace(s, '^(a)', 'b') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                       │\n├──────────────────────────────┤\n│ bbc                          │\n│ bac                          │\n│ bca                          │\n└──────────────────────────────┘\n\n\n\nre_search\nexpr.types.strings.StringValue.re_search(self, pattern)\nReturn whether the values match pattern.\nReturns True if the regex matches a string and False otherwise.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npattern\nstr | ibis.expr.types.strings.StringValue\nRegular expression use for searching\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nIndicator of matches\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"Ibis project\", \"GitHub\"]})\n&gt;&gt;&gt; t.s.re_search(\".+Hub\")\n┏━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ RegexSearch(s, '.+Hub') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                 │\n├─────────────────────────┤\n│ False                   │\n│ True                    │\n└─────────────────────────┘\n\n\n\nrepeat\nexpr.types.strings.StringValue.repeat(self, n)\nRepeat a string n times.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint | ibis.expr.types.IntegerValue\nNumber of repetitions\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nRepeated string\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"a\", \"bb\", \"c\"]})\n&gt;&gt;&gt; t.s.repeat(5)\n┏━━━━━━━━━━━━━━┓\n┃ Repeat(s, 5) ┃\n┡━━━━━━━━━━━━━━┩\n│ string       │\n├──────────────┤\n│ aaaaa        │\n│ bbbbbbbbbb   │\n│ ccccc        │\n└──────────────┘\n\n\n\nreplace\nexpr.types.strings.StringValue.replace(self, pattern, replacement)\nReplace each exact match of pattern with replacement.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npattern\nStringValue\nString pattern\nrequired\n\n\nreplacement\nStringValue\nString replacement\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nReplaced string\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"bac\", \"bca\"]})\n&gt;&gt;&gt; t.s.replace(\"b\", \"z\")\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ StringReplace(s, 'b', 'z') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                     │\n├────────────────────────────┤\n│ azc                        │\n│ zac                        │\n│ zca                        │\n└────────────────────────────┘\n\n\n\nreverse\nexpr.types.strings.StringValue.reverse(self)\nReverse the characters of a string.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nReversed string\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"def\", \"ghi\"]})\n&gt;&gt;&gt; t\n┏━━━━━━━━┓\n┃ s      ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ abc    │\n│ def    │\n│ ghi    │\n└────────┘\n&gt;&gt;&gt; t.s.reverse()\n┏━━━━━━━━━━━━┓\n┃ Reverse(s) ┃\n┡━━━━━━━━━━━━┩\n│ string     │\n├────────────┤\n│ cba        │\n│ fed        │\n│ ihg        │\n└────────────┘\n\n\n\nright\nexpr.types.strings.StringValue.right(self, nchars)\nReturn up to nchars from the end of each string.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnchars\nint | ibis.expr.types.IntegerValue\nMaximum number of characters to return\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nCharacters from the end\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"defg\", \"hijlk\"]})\n&gt;&gt;&gt; t.s.right(2)\n┏━━━━━━━━━━━━━━━━┓\n┃ StrRight(s, 2) ┃\n┡━━━━━━━━━━━━━━━━┩\n│ string         │\n├────────────────┤\n│ bc             │\n│ fg             │\n│ lk             │\n└────────────────┘\n\n\n\nrpad\nexpr.types.strings.StringValue.rpad(self, length, pad=' ')\nPad self by truncating or padding on the right.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nself\n\nString to pad\nrequired\n\n\nlength\nint | ibis.expr.types.IntegerValue\nLength of output string\nrequired\n\n\npad\nstr | ibis.expr.types.strings.StringValue\nPad character\n' '\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nRight-padded string\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"def\", \"ghij\"]})\n&gt;&gt;&gt; t.s.rpad(5, \"-\")\n┏━━━━━━━━━━━━━━━━━┓\n┃ RPad(s, 5, '-') ┃\n┡━━━━━━━━━━━━━━━━━┩\n│ string          │\n├─────────────────┤\n│ abc--           │\n│ def--           │\n│ ghij-           │\n└─────────────────┘\n\n\n\nrstrip\nexpr.types.strings.StringValue.rstrip(self)\nRemove whitespace from the right side of string.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nRight-stripped string\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"\\ta\\t\", \"\\nb\\n\", \"\\vc\\t\"]})\n&gt;&gt;&gt; t\n┏━━━━━━━━┓\n┃ s      ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ \\ta\\t  │\n│ \\nb\\n  │\n│ \\vc\\t  │\n└────────┘\n&gt;&gt;&gt; t.s.rstrip()\n┏━━━━━━━━━━━┓\n┃ RStrip(s) ┃\n┡━━━━━━━━━━━┩\n│ string    │\n├───────────┤\n│ \\ta       │\n│ \\nb       │\n│ \\vc       │\n└───────────┘\n\n\n\nsplit\nexpr.types.strings.StringValue.split(self, delimiter)\nSplit as string on delimiter.\n!!! note “This API only works on backends with array support.”\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndelimiter\nstr | ibis.expr.types.strings.StringValue\nValue to split by\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nThe string split by delimiter\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"col\": [\"a,b,c\", \"d,e\", \"f\"]})\n&gt;&gt;&gt; t\n┏━━━━━━━━┓\n┃ col    ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ a,b,c  │\n│ d,e    │\n│ f      │\n└────────┘\n&gt;&gt;&gt; t.col.split(\",\")\n┏━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ StringSplit(col, ',') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;string&gt;         │\n├───────────────────────┤\n│ ['a', 'b', ... +1]    │\n│ ['d', 'e']            │\n│ ['f']                 │\n└───────────────────────┘\n\n\n\nstartswith\nexpr.types.strings.StringValue.startswith(self, start)\nDetermine whether self starts with end.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart\nstr | ibis.expr.types.strings.StringValue\nprefix to check for\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nBoolean indicating whether self starts with start\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"Ibis project\", \"GitHub\"]})\n&gt;&gt;&gt; t.s.startswith(\"Ibis\")\n┏━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ StartsWith(s, 'Ibis') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean               │\n├───────────────────────┤\n│ True                  │\n│ False                 │\n└───────────────────────┘\n\n\n\nstrip\nexpr.types.strings.StringValue.strip(self)\nRemove whitespace from left and right sides of a string.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nStripped string\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"\\ta\\t\", \"\\nb\\n\", \"\\vc\\t\"]})\n&gt;&gt;&gt; t\n┏━━━━━━━━┓\n┃ s      ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ \\ta\\t  │\n│ \\nb\\n  │\n│ \\vc\\t  │\n└────────┘\n&gt;&gt;&gt; t.s.strip()\n┏━━━━━━━━━━┓\n┃ Strip(s) ┃\n┡━━━━━━━━━━┩\n│ string   │\n├──────────┤\n│ a        │\n│ b        │\n│ c        │\n└──────────┘\n\n\n\nsubstr\nexpr.types.strings.StringValue.substr(self, start, length=None)\nExtract a substring.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart\nint | ibis.expr.types.IntegerValue\nFirst character to start splitting, indices start at 0\nrequired\n\n\nlength\nint | ibis.expr.types.IntegerValue | None\nMaximum length of each substring. If not supplied, searches the entire string\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nFound substring\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"defg\", \"hijlk\"]})\n&gt;&gt;&gt; t.s.substr(2)\n┏━━━━━━━━━━━━━━━━━┓\n┃ Substring(s, 2) ┃\n┡━━━━━━━━━━━━━━━━━┩\n│ string          │\n├─────────────────┤\n│ c               │\n│ fg              │\n│ jlk             │\n└─────────────────┘\n\n\n\nto_timestamp\nexpr.types.strings.StringValue.to_timestamp(self, format_str)\nParse a string and return a timestamp.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nformat_str\nstr\nFormat string in strptime format\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTimestampValue\nParsed timestamp value\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"ts\": [\"20170206\"]})\n&gt;&gt;&gt; t.ts.to_timestamp(\"%Y%m%d\")\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ StringToTimestamp(ts, '%Y%m%d') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ timestamp('UTC')                │\n├─────────────────────────────────┤\n│ 2017-02-06 00:00:00+00:00       │\n└─────────────────────────────────┘\n\n\n\ntranslate\nexpr.types.strings.StringValue.translate(self, from_str, to_str)\nReplace from_str characters in self characters in to_str.\nTo avoid unexpected behavior, from_str should be shorter than to_str.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfrom_str\nStringValue\nCharacters in arg to replace\nrequired\n\n\nto_str\nStringValue\nCharacters to use for replacement\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nTranslated string\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; table = ibis.table(dict(string_col='string'))\n&gt;&gt;&gt; result = table.string_col.translate('a', 'b')\n\n\n\nupper\nexpr.types.strings.StringValue.upper(self)\nConvert string to all uppercase.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nUppercase string\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"aaa\", \"A\", \"aa\"]})\n&gt;&gt;&gt; t\n┏━━━━━━━━┓\n┃ s      ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ aaa    │\n│ A      │\n│ aa     │\n└────────┘\n&gt;&gt;&gt; t.s.upper()\n┏━━━━━━━━━━━━━━┓\n┃ Uppercase(s) ┃\n┡━━━━━━━━━━━━━━┩\n│ string       │\n├──────────────┤\n│ AAA          │\n│ A            │\n│ AA           │\n└──────────────┘\n\n\n\nuserinfo\nexpr.types.strings.StringValue.userinfo(self)\nParse a URL and extract user info.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; url = ibis.literal(\"https://user:pass@example.com:80/docs/books\")\n&gt;&gt;&gt; result = url.userinfo()  # user:pass\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted string value"
  },
  {
    "objectID": "reference/expression-collections.html",
    "href": "reference/expression-collections.html",
    "title": "Complex Type Expressions",
    "section": "",
    "text": "These APIs are available on arrays, maps and structs."
  },
  {
    "objectID": "reference/expression-collections.html#methods",
    "href": "reference/expression-collections.html#methods",
    "title": "Complex Type Expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\ncontains\nReturn whether the array contains other.\n\n\nfilter\nFilter array elements using predicate.\n\n\nindex\nReturn the position of other in an array.\n\n\njoin\nJoin the elements of this array expression with sep.\n\n\nlength\nCompute the length of an array.\n\n\nmap\nApply a callable func to each element of this array expression.\n\n\nremove\nRemove other from self.\n\n\nsort\nSort the elements in an array.\n\n\nunion\nUnion two arrays.\n\n\nunique\nReturn the unique values in an array.\n\n\nunnest\nFlatten an array into a column.\n\n\nzip\nZip two or more arrays together.\n\n\n\n\ncontains\nexpr.types.arrays.ArrayValue.contains(self, other)\nReturn whether the array contains other.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nibis.Value\nIbis expression to check for existence of in self\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether other is contained in self\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"arr\": [[1], [], [42, 42], None]})\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ arr                  ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;         │\n├──────────────────────┤\n│ [1]                  │\n│ []                   │\n│ [42, 42]             │\n│ NULL                 │\n└──────────────────────┘\n&gt;&gt;&gt; t.arr.contains(42)\n┏━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayContains(arr, 42) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                │\n├────────────────────────┤\n│ False                  │\n│ False                  │\n│ True                   │\n│ NULL                   │\n└────────────────────────┘\n&gt;&gt;&gt; t.arr.contains(None)\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayContains(arr, None) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                  │\n├──────────────────────────┤\n│ NULL                     │\n│ NULL                     │\n│ NULL                     │\n│ NULL                     │\n└──────────────────────────┘\n\n\n\nfilter\nexpr.types.arrays.ArrayValue.filter(self, predicate)\nFilter array elements using predicate.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npredicate\ntyping.Callable[[ibis.Value], ibis.BooleanValue]\nFunction to use to filter array elements\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nArray elements filtered using predicate\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [[1, None, 2], [4], []]})\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ a                    ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;         │\n├──────────────────────┤\n│ [1, None, ... +1]    │\n│ [4]                  │\n│ []                   │\n└──────────────────────┘\n&gt;&gt;&gt; t.a.filter(lambda x: x &gt; 1)\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayFilter(a)       ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;         │\n├──────────────────────┤\n│ [2]                  │\n│ [4]                  │\n│ []                   │\n└──────────────────────┘\n\n\n\nindex\nexpr.types.arrays.ArrayValue.index(self, other)\nReturn the position of other in an array.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nibis.Value\nIbis expression to existence of in self\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nThe position of other in self\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"arr\": [[1], [], [42, 42], None]})\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ arr                  ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;         │\n├──────────────────────┤\n│ [1]                  │\n│ []                   │\n│ [42, 42]             │\n│ NULL                 │\n└──────────────────────┘\n&gt;&gt;&gt; t.arr.index(42)\n┏━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayPosition(arr, 42) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64                  │\n├────────────────────────┤\n│                     -1 │\n│                     -1 │\n│                      0 │\n│                   NULL │\n└────────────────────────┘\n&gt;&gt;&gt; t.arr.index(800)\n┏━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayPosition(arr, 800) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64                   │\n├─────────────────────────┤\n│                      -1 │\n│                      -1 │\n│                      -1 │\n│                    NULL │\n└─────────────────────────┘\n&gt;&gt;&gt; t.arr.index(None)\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayPosition(arr, None) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64                    │\n├──────────────────────────┤\n│                     NULL │\n│                     NULL │\n│                     NULL │\n│                     NULL │\n└──────────────────────────┘\n\n\n\njoin\nexpr.types.arrays.ArrayValue.join(self, sep)\nJoin the elements of this array expression with sep.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsep\nstr | ibis.StringValue\nSeparator to use for joining array elements\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nElements of self joined with sep\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"arr\": [[\"a\", \"b\", \"c\"], None, [], [\"b\", None]]})\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ arr                  ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;string&gt;        │\n├──────────────────────┤\n│ ['a', 'b', ... +1]   │\n│ NULL                 │\n│ []                   │\n│ ['b', None]          │\n└──────────────────────┘\n&gt;&gt;&gt; t.arr.join(\"|\")\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayStringJoin('|', arr) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                    │\n├───────────────────────────┤\n│ a|b|c                     │\n│ NULL                      │\n│ NULL                      │\n│ b                         │\n└───────────────────────────┘\n\n\nSee Also\n[StringValue.join][ibis.expr.types.strings.StringValue.join]\n\n\n\nlength\nexpr.types.arrays.ArrayValue.length(self)\nCompute the length of an array.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nThe integer length of each element of self\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [[7, 42], [3], None]})\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ a                    ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;         │\n├──────────────────────┤\n│ [7, 42]              │\n│ [3]                  │\n│ NULL                 │\n└──────────────────────┘\n&gt;&gt;&gt; t.a.length()\n┏━━━━━━━━━━━━━━━━┓\n┃ ArrayLength(a) ┃\n┡━━━━━━━━━━━━━━━━┩\n│ int64          │\n├────────────────┤\n│              2 │\n│              1 │\n│           NULL │\n└────────────────┘\n\n\n\nmap\nexpr.types.arrays.ArrayValue.map(self, func)\nApply a callable func to each element of this array expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\ntyping.Callable[[ibis.Value], ibis.Value]\nFunction to apply to each element of this array\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nfunc applied to every element of this array expression.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [[1, None, 2], [4], []]})\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ a                    ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;         │\n├──────────────────────┤\n│ [1, None, ... +1]    │\n│ [4]                  │\n│ []                   │\n└──────────────────────┘\n&gt;&gt;&gt; t.a.map(lambda x: (x + 100).cast(\"float\"))\n┏━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayMap(a)           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;float64&gt;        │\n├───────────────────────┤\n│ [101.0, None, ... +1] │\n│ [104.0]               │\n│ []                    │\n└───────────────────────┘\n\n\n\nremove\nexpr.types.arrays.ArrayValue.remove(self, other)\nRemove other from self.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nibis.Value\nElement to remove from self.\nrequired\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"arr\": [[3, 2], [], [42, 2], [2, 2], None]})\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ arr                  ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;         │\n├──────────────────────┤\n│ [3, 2]               │\n│ []                   │\n│ [42, 2]              │\n│ [2, 2]               │\n│ NULL                 │\n└──────────────────────┘\n&gt;&gt;&gt; t.arr.remove(2)\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayRemove(arr, 2)  ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;         │\n├──────────────────────┤\n│ [3]                  │\n│ []                   │\n│ [42]                 │\n│ []                   │\n│ NULL                 │\n└──────────────────────┘\n\n\n\nsort\nexpr.types.arrays.ArrayValue.sort(self)\nSort the elements in an array.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nSorted values in an array\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"arr\": [[3, 2], [], [42, 42], None]})\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ arr                  ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;         │\n├──────────────────────┤\n│ [3, 2]               │\n│ []                   │\n│ [42, 42]             │\n│ NULL                 │\n└──────────────────────┘\n&gt;&gt;&gt; t.arr.sort()\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArraySort(arr)       ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;         │\n├──────────────────────┤\n│ [2, 3]               │\n│ []                   │\n│ [42, 42]             │\n│ NULL                 │\n└──────────────────────┘\n\n\n\nunion\nexpr.types.arrays.ArrayValue.union(self, other)\nUnion two arrays.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nibis.ArrayValue\nAnother array to union with self\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nUnioned arrays\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"arr1\": [[3, 2], [], None], \"arr2\": [[1, 3], [None], [5]]})\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n┃ arr1                 ┃ arr2                 ┃\n┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;         │ array&lt;int64&gt;         │\n├──────────────────────┼──────────────────────┤\n│ [3, 2]               │ [1, 3]               │\n│ []                   │ [None]               │\n│ NULL                 │ [5]                  │\n└──────────────────────┴──────────────────────┘\n&gt;&gt;&gt; t.arr1.union(t.arr2)\n┏━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayUnion(arr1, arr2) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;           │\n├────────────────────────┤\n│ [1, 2, ... +1]         │\n│ []                     │\n│ [5]                    │\n└────────────────────────┘\n&gt;&gt;&gt; t.arr1.union(t.arr2).contains(3)\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayContains(ArrayUnion(arr1, arr2), 3) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                                  │\n├──────────────────────────────────────────┤\n│ True                                     │\n│ False                                    │\n│ False                                    │\n└──────────────────────────────────────────┘\n\n\n\nunique\nexpr.types.arrays.ArrayValue.unique(self)\nReturn the unique values in an array.\n!!! note “Element ordering in array may not be retained.”\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nUnique values in an array\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"arr\": [[1, 3, 3], [], [42, 42], None]})\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ arr                  ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;         │\n├──────────────────────┤\n│ [1, 3, ... +1]       │\n│ []                   │\n│ [42, 42]             │\n│ NULL                 │\n└──────────────────────┘\n&gt;&gt;&gt; t.arr.unique()\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayDistinct(arr)   ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;         │\n├──────────────────────┤\n│ [3, 1]               │\n│ []                   │\n│ [42]                 │\n│ NULL                 │\n└──────────────────────┘\n\n\n\nunnest\nexpr.types.arrays.ArrayValue.unnest(self)\nFlatten an array into a column.\n!!! note “This operation changes the cardinality of the result”\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [[7, 42], [3, 3] , None]})\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ a                    ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;         │\n├──────────────────────┤\n│ [7, 42]              │\n│ [3, 3]               │\n│ NULL                 │\n└──────────────────────┘\n&gt;&gt;&gt; t.a.unnest()\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     7 │\n│    42 │\n│     3 │\n│     3 │\n└───────┘\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.Value\nUnnested array\n\n\n\n\n\n\nzip\nexpr.types.arrays.ArrayValue.zip(self, other, *others)\nZip two or more arrays together.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nibis.Array\nAnother array to zip with self\nrequired\n\n\nothers\nibis.Array\nAdditional arrays to zip with self\n()\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nArray\nArray of structs where each struct field is an element of each input array.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"numbers\": [[3, 2], [], None], \"strings\": [[\"a\", \"c\"], None, [\"e\"]]})\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n┃ numbers              ┃ strings              ┃\n┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;         │ array&lt;string&gt;        │\n├──────────────────────┼──────────────────────┤\n│ [3, 2]               │ ['a', 'c']           │\n│ []                   │ NULL                 │\n│ NULL                 │ ['e']                │\n└──────────────────────┴──────────────────────┘\n&gt;&gt;&gt; expr = t.numbers.zip(t.strings)\n&gt;&gt;&gt; expr\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayZip()                           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;struct&lt;f1: int64, f2: string&gt;&gt; │\n├──────────────────────────────────────┤\n│ [{...}, {...}]                       │\n│ []                                   │\n│ [{...}]                              │\n└──────────────────────────────────────┘\n&gt;&gt;&gt; expr.unnest()\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayZip()                    ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ struct&lt;f1: int64, f2: string&gt; │\n├───────────────────────────────┤\n│ {'f1': 3, 'f2': 'a'}          │\n│ {'f1': 2, 'f2': 'c'}          │\n│ {'f1': None, 'f2': 'e'}       │\n└───────────────────────────────┘"
  },
  {
    "objectID": "reference/expression-collections.html#examples-12",
    "href": "reference/expression-collections.html#examples-12",
    "title": "Complex Type Expressions",
    "section": "Examples",
    "text": "Examples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({'s': [{'a': 1, 'b': 'foo'}, {'a': 3, 'b': None}, None]})\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ s                           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ struct&lt;a: int64, b: string&gt; │\n├─────────────────────────────┤\n│ {'a': 1, 'b': 'foo'}        │\n│ {'a': 3, 'b': None}         │\n│ NULL                        │\n└─────────────────────────────┘\nCan use either . or [] to access fields:\n&gt;&gt;&gt; t.s.a\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     3 │\n│  NULL │\n└───────┘\n&gt;&gt;&gt; t.s['a']\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     3 │\n│  NULL │\n└───────┘"
  },
  {
    "objectID": "reference/expression-collections.html#attributes",
    "href": "reference/expression-collections.html#attributes",
    "title": "Complex Type Expressions",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\nfields\nReturn a mapping from field name to field type of the struct.\n\n\nnames\nReturn the field names of the struct.\n\n\ntypes\nReturn the field types of the struct."
  },
  {
    "objectID": "reference/expression-collections.html#methods-1",
    "href": "reference/expression-collections.html#methods-1",
    "title": "Complex Type Expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\ndestructure\nDestructure a StructValue into the corresponding struct fields.\n\n\nlift\nProject the fields of self into a table.\n\n\n\n\ndestructure\nexpr.types.structs.StructValue.destructure(self)\nDestructure a StructValue into the corresponding struct fields.\nWhen assigned, a destruct value will be destructured and assigned to multiple columns.\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[AnyValue]\nValue expressions corresponding to the struct fields.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({'s': [{'a': 1, 'b': 'foo'}, {'a': 3, 'b': None}, None]})\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ s                           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ struct&lt;a: int64, b: string&gt; │\n├─────────────────────────────┤\n│ {'a': 1, 'b': 'foo'}        │\n│ {'a': 3, 'b': None}         │\n│ NULL                        │\n└─────────────────────────────┘\n&gt;&gt;&gt; a, b = t.s.destructure()\n&gt;&gt;&gt; a\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     3 │\n│  NULL │\n└───────┘\n&gt;&gt;&gt; b\n┏━━━━━━━━┓\n┃ b      ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ foo    │\n│ NULL   │\n│ NULL   │\n└────────┘\n\n\n\nlift\nexpr.types.structs.StructValue.lift(self)\nProject the fields of self into a table.\nThis method is useful when analyzing data that has deeply nested structs or arrays of structs. lift can be chained to avoid repeating column names and table references.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA projection with this struct expression’s fields.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable(\n...     {\n...         \"pos\": [\n...             {\"lat\": 10.1, \"lon\": 30.3},\n...             {\"lat\": 10.2, \"lon\": 30.2},\n...             {\"lat\": 10.3, \"lon\": 30.1},\n...         ]\n...     }\n... )\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ pos                                ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ struct&lt;lat: float64, lon: float64&gt; │\n├────────────────────────────────────┤\n│ {'lat': 10.1, 'lon': 30.3}         │\n│ {'lat': 10.2, 'lon': 30.2}         │\n│ {'lat': 10.3, 'lon': 30.1}         │\n└────────────────────────────────────┘\n&gt;&gt;&gt; t.pos.lift()\n┏━━━━━━━━━┳━━━━━━━━━┓\n┃ lat     ┃ lon     ┃\n┡━━━━━━━━━╇━━━━━━━━━┩\n│ float64 │ float64 │\n├─────────┼─────────┤\n│    10.1 │    30.3 │\n│    10.2 │    30.2 │\n│    10.3 │    30.1 │\n└─────────┴─────────┘\n\n\nSee Also\n[Table.unpack][ibis.expr.types.relations.Table.unpack]."
  },
  {
    "objectID": "reference/expression-collections.html#examples-15",
    "href": "reference/expression-collections.html#examples-15",
    "title": "Complex Type Expressions",
    "section": "Examples",
    "text": "Examples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; tab = pa.table({\n...    \"m\": pa.array([[(\"a\", 1), (\"b\", 2)], [(\"a\", 1)], None],\n...                  type=pa.map_(pa.utf8(), pa.int64()))})\n&gt;&gt;&gt; t = ibis.memtable(tab)\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ m                    ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ map&lt;!string, int64&gt;  │\n├──────────────────────┤\n│ {'a': 1, 'b': 2}     │\n│ {'a': 1}             │\n│ NULL                 │\n└──────────────────────┘\nCan use [] to access values:\n&gt;&gt;&gt; t.m['a']\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ MapGet(m, 'a', None) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64                │\n├──────────────────────┤\n│                    1 │\n│                    1 │\n│                 NULL │\n└──────────────────────┘\nTo provide default values, use get:\n&gt;&gt;&gt; t.m.get('b', 0)\n┏━━━━━━━━━━━━━━━━━━━┓\n┃ MapGet(m, 'b', 0) ┃\n┡━━━━━━━━━━━━━━━━━━━┩\n│ int64             │\n├───────────────────┤\n│                 2 │\n│                 0 │\n│                 0 │\n└───────────────────┘"
  },
  {
    "objectID": "reference/expression-collections.html#methods-2",
    "href": "reference/expression-collections.html#methods-2",
    "title": "Complex Type Expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\ncontains\nReturn whether the map contains key.\n\n\nget\nReturn the value for key from expr.\n\n\nkeys\nExtract the keys of a map.\n\n\nlength\nReturn the number of key-value pairs in the map.\n\n\nvalues\nExtract the values of a map.\n\n\n\n\ncontains\nexpr.types.maps.MapValue.contains(self, key)\nReturn whether the map contains key.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nint | str | ibis.IntegerValue | ibis.StringValue\nMapping key for which to check\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nBoolean indicating the presence of key in the map expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; tab = pa.table({\n...    \"m\": pa.array([[(\"a\", 1), (\"b\", 2)], [(\"a\", 1)], None],\n...                  type=pa.map_(pa.utf8(), pa.int64()))})\n&gt;&gt;&gt; t = ibis.memtable(tab)\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ m                    ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ map&lt;!string, int64&gt;   │\n├──────────────────────┤\n│ {'a': 1, 'b': 2}     │\n│ {'a': 1}             │\n│ NULL                 │\n└──────────────────────┘\n&gt;&gt;&gt; t.m.contains(\"b\")\n┏━━━━━━━━━━━━━━━━━━━━━┓\n┃ MapContains(m, 'b') ┃\n┡━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean             │\n├─────────────────────┤\n│ True                │\n│ False               │\n│ False               │\n└─────────────────────┘\n\n\n\nget\nexpr.types.maps.MapValue.get(self, key, default=None)\nReturn the value for key from expr.\nReturn default if key is not in the map.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nibis.Value\nExpression to use for key\nrequired\n\n\ndefault\nibis.Value | None\nExpression to return if key is not a key in expr\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nThe element type of self\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; tab = pa.table({\n...    \"m\": pa.array([[(\"a\", 1), (\"b\", 2)], [(\"a\", 1)], None],\n...                  type=pa.map_(pa.utf8(), pa.int64()))})\n&gt;&gt;&gt; t = ibis.memtable(tab)\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ m                    ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ map&lt;!string, int64&gt;  │\n├──────────────────────┤\n│ {'a': 1, 'b': 2}     │\n│ {'a': 1}             │\n│ NULL                 │\n└──────────────────────┘\n&gt;&gt;&gt; t.m.get(\"a\")\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ MapGet(m, 'a', None) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64                │\n├──────────────────────┤\n│                    1 │\n│                    1 │\n│                 NULL │\n└──────────────────────┘\n&gt;&gt;&gt; t.m.get(\"b\")\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ MapGet(m, 'b', None) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64                │\n├──────────────────────┤\n│                    2 │\n│                 NULL │\n│                 NULL │\n└──────────────────────┘\n&gt;&gt;&gt; t.m.get(\"b\", 0)\n┏━━━━━━━━━━━━━━━━━━━┓\n┃ MapGet(m, 'b', 0) ┃\n┡━━━━━━━━━━━━━━━━━━━┩\n│ int64             │\n├───────────────────┤\n│                 2 │\n│                 0 │\n│                 0 │\n└───────────────────┘\n\n\n\nkeys\nexpr.types.maps.MapValue.keys(self)\nExtract the keys of a map.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nThe keys of self\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; tab = pa.table({\n...    \"m\": pa.array([[(\"a\", 1), (\"b\", 2)], [(\"a\", 1)], None],\n...                  type=pa.map_(pa.utf8(), pa.int64()))})\n&gt;&gt;&gt; t = ibis.memtable(tab)\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ m                    ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ map&lt;!string, int64&gt;  │\n├──────────────────────┤\n│ {'a': 1, 'b': 2}     │\n│ {'a': 1}             │\n│ NULL                 │\n└──────────────────────┘\n&gt;&gt;&gt; t.m.keys()\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ MapKeys(m)           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;!string&gt;       │\n├──────────────────────┤\n│ ['a', 'b']           │\n│ ['a']                │\n│ NULL                 │\n└──────────────────────┘\n\n\n\nlength\nexpr.types.maps.MapValue.length(self)\nReturn the number of key-value pairs in the map.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nThe number of elements in self\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; tab = pa.table({\n...    \"m\": pa.array([[(\"a\", 1), (\"b\", 2)], [(\"a\", 1)], None],\n...                  type=pa.map_(pa.utf8(), pa.int64()))})\n&gt;&gt;&gt; t = ibis.memtable(tab)\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ m                    ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ map&lt;!string, int64&gt;  │\n├──────────────────────┤\n│ {'a': 1, 'b': 2}     │\n│ {'a': 1}             │\n│ NULL                 │\n└──────────────────────┘\n&gt;&gt;&gt; t.m.length()\n┏━━━━━━━━━━━━━━┓\n┃ MapLength(m) ┃\n┡━━━━━━━━━━━━━━┩\n│ int64        │\n├──────────────┤\n│            2 │\n│            1 │\n│         NULL │\n└──────────────┘\n\n\n\nvalues\nexpr.types.maps.MapValue.values(self)\nExtract the values of a map.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nThe values of self\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; m = ibis.map({\"a\": 1, \"b\": 2})\n&gt;&gt;&gt; m.values()\n[1, 2]"
  },
  {
    "objectID": "reference/config.Options.html",
    "href": "reference/config.Options.html",
    "title": "config.Options",
    "section": "",
    "text": "config.Options()\nIbis configuration options.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninteractive\nbool\nShow the first few rows of computing an expression when in a repl.\n\n\nrepr\nRepr\nOptions controlling expression printing.\n\n\nverbose\nbool\nRun in verbose mode if [True][True]\n\n\nverbose_log\ntyping.Callable[[str], None] | None\nA callable to use when logging.\n\n\ngraphviz_repr\nbool\nRender expressions as GraphViz PNGs when running in a Jupyter notebook.\n\n\ndefault_backend\nOptional[ibis.backends.base.BaseBackend], default None\nThe default backend to use for execution, defaults to DuckDB if not set.\n\n\ncontext_adjustment\nContextAdjustment\nOptions related to time context adjustment.\n\n\nsql\nSQL\nSQL-related options.\n\n\nclickhouse\nibis.config.Config | None\nClickhouse specific options.\n\n\ndask\nibis.config.Config | None\nDask specific options.\n\n\nimpala\nibis.config.Config | None\nImpala specific options.\n\n\npandas\nibis.config.Config | None\nPandas specific options.\n\n\npyspark\nibis.config.Config | None\nPySpark specific options."
  },
  {
    "objectID": "reference/config.Options.html#attributes",
    "href": "reference/config.Options.html#attributes",
    "title": "config.Options",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ninteractive\nbool\nShow the first few rows of computing an expression when in a repl.\n\n\nrepr\nRepr\nOptions controlling expression printing.\n\n\nverbose\nbool\nRun in verbose mode if [True][True]\n\n\nverbose_log\ntyping.Callable[[str], None] | None\nA callable to use when logging.\n\n\ngraphviz_repr\nbool\nRender expressions as GraphViz PNGs when running in a Jupyter notebook.\n\n\ndefault_backend\nOptional[ibis.backends.base.BaseBackend], default None\nThe default backend to use for execution, defaults to DuckDB if not set.\n\n\ncontext_adjustment\nContextAdjustment\nOptions related to time context adjustment.\n\n\nsql\nSQL\nSQL-related options.\n\n\nclickhouse\nibis.config.Config | None\nClickhouse specific options.\n\n\ndask\nibis.config.Config | None\nDask specific options.\n\n\nimpala\nibis.config.Config | None\nImpala specific options.\n\n\npandas\nibis.config.Config | None\nPandas specific options.\n\n\npyspark\nibis.config.Config | None\nPySpark specific options."
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Reference",
    "section": "",
    "text": "Top-level APIs\nThese methods and objecst are available directly on the ibis module.\n\n\nTable Expressions\nTable expressions form the basis for most Ibis expressions.\n\n\nGeneric Expression APIs\nThese expressions are available on scalars and columns of any element type.\n\n\nNumeric and Boolean Expressions\nThese APIs are available on numeric and boolean expressions.\n\n\nString Expressions\nAll string operations are valid for both scalars and columns.\n\n\ntemporal\n\n\n\nComplex Type Expressions\nThese APIs are available on arrays, maps and structs.\n\n\nGeospatial Expressions\nIbis supports the following geospatial expression APIs\n\n\n\n\n\n\n\n\n\nselectors\nConvenient column selectors.\n\n\nexpr.schema.Schema\nAn object for holding table schema information.\n\n\nconfig.Options\nIbis configuration options.\n\n\nconfig.Repr\nExpression printing options.\n\n\nconfig.SQL\nSQL-related options.\n\n\nconfig.ContextAdjustment\nOptions related to time context adjustment.\n\n\nexpr.datatypes.core"
  },
  {
    "objectID": "reference/index.html#expressions",
    "href": "reference/index.html#expressions",
    "title": "Reference",
    "section": "",
    "text": "Top-level APIs\nThese methods and objecst are available directly on the ibis module.\n\n\nTable Expressions\nTable expressions form the basis for most Ibis expressions.\n\n\nGeneric Expression APIs\nThese expressions are available on scalars and columns of any element type.\n\n\nNumeric and Boolean Expressions\nThese APIs are available on numeric and boolean expressions.\n\n\nString Expressions\nAll string operations are valid for both scalars and columns.\n\n\ntemporal\n\n\n\nComplex Type Expressions\nThese APIs are available on arrays, maps and structs.\n\n\nGeospatial Expressions\nIbis supports the following geospatial expression APIs"
  },
  {
    "objectID": "reference/index.html#other",
    "href": "reference/index.html#other",
    "title": "Reference",
    "section": "",
    "text": "selectors\nConvenient column selectors.\n\n\nexpr.schema.Schema\nAn object for holding table schema information.\n\n\nconfig.Options\nIbis configuration options.\n\n\nconfig.Repr\nExpression printing options.\n\n\nconfig.SQL\nSQL-related options.\n\n\nconfig.ContextAdjustment\nOptions related to time context adjustment.\n\n\nexpr.datatypes.core"
  },
  {
    "objectID": "reference/expression-numeric.html",
    "href": "reference/expression-numeric.html",
    "title": "Numeric and Boolean Expressions",
    "section": "",
    "text": "These APIs are available on numeric and boolean expressions."
  },
  {
    "objectID": "reference/expression-numeric.html#methods",
    "href": "reference/expression-numeric.html#methods",
    "title": "Numeric and Boolean Expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nabs\nReturn the absolute value of self.\n\n\nacos\nCompute the arc cosine of self.\n\n\nasin\nCompute the arc sine of self.\n\n\natan\nCompute the arc tangent of self.\n\n\natan2\nCompute the two-argument version of arc tangent.\n\n\nceil\nReturn the ceiling of self.\n\n\nclip\nTrim values outside of lower and upper bounds.\n\n\ncos\nCompute the cosine of self.\n\n\ncot\nCompute the cotangent of self.\n\n\ndegrees\nCompute the degrees of self radians.\n\n\nexp\nCompute \\(e^\\texttt{self}\\).\n\n\nfloor\nReturn the floor of an expression.\n\n\nln\nCompute \\(\\ln\\left(\\texttt{self}\\right)\\).\n\n\nlog\nCompute \\(\\log_{\\texttt{base}}\\left(\\texttt{self}\\right)\\).\n\n\nlog10\nCompute \\(\\log_{10}\\left(\\texttt{self}\\right)\\).\n\n\nlog2\nCompute \\(\\log_{2}\\left(\\texttt{self}\\right)\\).\n\n\nnegate\nNegate a numeric expression.\n\n\nnullifzero\nReturn NULL if an expression is zero.\n\n\npoint\nReturn a point constructed from the coordinate values.\n\n\nradians\nCompute radians from self degrees.\n\n\nround\nRound values to an indicated number of decimal places.\n\n\nsign\nReturn the sign of the input.\n\n\nsin\nCompute the sine of self.\n\n\nsqrt\nCompute the square root of self.\n\n\ntan\nCompute the tangent of self.\n\n\nzeroifnull\nReturn zero if an expression is NULL.\n\n\n\n\nabs\nexpr.types.numeric.NumericValue.abs(self)\nReturn the absolute value of self.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 2, -3, 4]})\n&gt;&gt;&gt; t.values.abs()\n┏━━━━━━━━━━━━━┓\n┃ Abs(values) ┃\n┡━━━━━━━━━━━━━┩\n│ int64       │\n├─────────────┤\n│           1 │\n│           2 │\n│           3 │\n│           4 │\n└─────────────┘\n\n\n\nacos\nexpr.types.numeric.NumericValue.acos(self)\nCompute the arc cosine of self.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.acos()\n┏━━━━━━━━━━━━━━┓\n┃ Acos(values) ┃\n┡━━━━━━━━━━━━━━┩\n│ float64      │\n├──────────────┤\n│     3.141593 │\n│     1.570796 │\n│     0.000000 │\n└──────────────┘\n\n\n\nasin\nexpr.types.numeric.NumericValue.asin(self)\nCompute the arc sine of self.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.asin()\n┏━━━━━━━━━━━━━━┓\n┃ Asin(values) ┃\n┡━━━━━━━━━━━━━━┩\n│ float64      │\n├──────────────┤\n│    -1.570796 │\n│     0.000000 │\n│     1.570796 │\n└──────────────┘\n\n\n\natan\nexpr.types.numeric.NumericValue.atan(self)\nCompute the arc tangent of self.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.atan()\n┏━━━━━━━━━━━━━━┓\n┃ Atan(values) ┃\n┡━━━━━━━━━━━━━━┩\n│ float64      │\n├──────────────┤\n│    -0.785398 │\n│     0.000000 │\n│     0.785398 │\n└──────────────┘\n\n\n\natan2\nexpr.types.numeric.NumericValue.atan2(self, other)\nCompute the two-argument version of arc tangent.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.atan2(0)\n┏━━━━━━━━━━━━━━━━━━┓\n┃ Atan2(values, 0) ┃\n┡━━━━━━━━━━━━━━━━━━┩\n│ float64          │\n├──────────────────┤\n│        -1.570796 │\n│         0.000000 │\n│         1.570796 │\n└──────────────────┘\n\n\n\nceil\nexpr.types.numeric.NumericValue.ceil(self)\nReturn the ceiling of self.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 1.1, 2, 2.1, 3.3]})\n&gt;&gt;&gt; t.values.ceil()\n┏━━━━━━━━━━━━━━┓\n┃ Ceil(values) ┃\n┡━━━━━━━━━━━━━━┩\n│ int64        │\n├──────────────┤\n│            1 │\n│            2 │\n│            2 │\n│            3 │\n│            4 │\n└──────────────┘\n\n\n\nclip\nexpr.types.numeric.NumericValue.clip(self, lower=None, upper=None)\nTrim values outside of lower and upper bounds.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlower\nibis.expr.types.numeric.NumericValue | None\nLower bound\nNone\n\n\nupper\nibis.expr.types.numeric.NumericValue | None\nUpper bound\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericValue\nClipped input\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": range(8)})\n&gt;&gt;&gt; t.values.clip(lower=3, upper=6)\n┏━━━━━━━━━━━━━━━━━━━━┓\n┃ Clip(values, 3, 6) ┃\n┡━━━━━━━━━━━━━━━━━━━━┩\n│ int64              │\n├────────────────────┤\n│                  3 │\n│                  3 │\n│                  3 │\n│                  3 │\n│                  4 │\n│                  5 │\n│                  6 │\n│                  6 │\n└────────────────────┘\n\n\n\ncos\nexpr.types.numeric.NumericValue.cos(self)\nCompute the cosine of self.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.cos()\n┏━━━━━━━━━━━━━┓\n┃ Cos(values) ┃\n┡━━━━━━━━━━━━━┩\n│ float64     │\n├─────────────┤\n│    0.540302 │\n│    1.000000 │\n│    0.540302 │\n└─────────────┘\n\n\n\ncot\nexpr.types.numeric.NumericValue.cot(self)\nCompute the cotangent of self.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.cot()\n┏━━━━━━━━━━━━━┓\n┃ Cot(values) ┃\n┡━━━━━━━━━━━━━┩\n│ float64     │\n├─────────────┤\n│   -0.642093 │\n│         inf │\n│    0.642093 │\n└─────────────┘\n\n\n\ndegrees\nexpr.types.numeric.NumericValue.degrees(self)\nCompute the degrees of self radians.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; from math import pi\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [0, pi / 2, pi, 3 * pi / 2, 2 * pi]})\n&gt;&gt;&gt; t.values.degrees()\n┏━━━━━━━━━━━━━━━━━┓\n┃ Degrees(values) ┃\n┡━━━━━━━━━━━━━━━━━┩\n│ float64         │\n├─────────────────┤\n│             0.0 │\n│            90.0 │\n│           180.0 │\n│           270.0 │\n│           360.0 │\n└─────────────────┘\n\n\n\nexp\nexpr.types.numeric.NumericValue.exp(self)\nCompute \\(e^\\texttt{self}\\).\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericValue\n\\(e^\\texttt{self}\\)\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": range(4)})\n&gt;&gt;&gt; t.values.exp()\n┏━━━━━━━━━━━━━┓\n┃ Exp(values) ┃\n┡━━━━━━━━━━━━━┩\n│ float64     │\n├─────────────┤\n│    1.000000 │\n│    2.718282 │\n│    7.389056 │\n│   20.085537 │\n└─────────────┘\n\n\n\nfloor\nexpr.types.numeric.NumericValue.floor(self)\nReturn the floor of an expression.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 1.1, 2, 2.1, 3.3]})\n&gt;&gt;&gt; t.values.floor()\n┏━━━━━━━━━━━━━━━┓\n┃ Floor(values) ┃\n┡━━━━━━━━━━━━━━━┩\n│ int64         │\n├───────────────┤\n│             1 │\n│             1 │\n│             2 │\n│             2 │\n│             3 │\n└───────────────┘\n\n\n\nln\nexpr.types.numeric.NumericValue.ln(self)\nCompute \\(\\ln\\left(\\texttt{self}\\right)\\).\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 2.718281828, 3]})\n&gt;&gt;&gt; t.values.ln()\n┏━━━━━━━━━━━━┓\n┃ Ln(values) ┃\n┡━━━━━━━━━━━━┩\n│ float64    │\n├────────────┤\n│   0.000000 │\n│   1.000000 │\n│   1.098612 │\n└────────────┘\n\n\n\nlog\nexpr.types.numeric.NumericValue.log(self, base=None)\nCompute \\(\\log_{\\texttt{base}}\\left(\\texttt{self}\\right)\\).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbase\nibis.expr.types.numeric.NumericValue | None\nThe base of the logarithm. If None, base e is used.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericValue\nLogarithm of arg with base base\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; from math import e\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [e, e**2, e**3]})\n&gt;&gt;&gt; t.values.log()\n┏━━━━━━━━━━━━━┓\n┃ Log(values) ┃\n┡━━━━━━━━━━━━━┩\n│ float64     │\n├─────────────┤\n│         1.0 │\n│         2.0 │\n│         3.0 │\n└─────────────┘\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [10, 100, 1000]})\n&gt;&gt;&gt; t.values.log(base=10)\n┏━━━━━━━━━━━━━━━━━┓\n┃ Log(values, 10) ┃\n┡━━━━━━━━━━━━━━━━━┩\n│ float64         │\n├─────────────────┤\n│             1.0 │\n│             2.0 │\n│             3.0 │\n└─────────────────┘\n\n\n\nlog10\nexpr.types.numeric.NumericValue.log10(self)\nCompute \\(\\log_{10}\\left(\\texttt{self}\\right)\\).\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 10, 100]})\n&gt;&gt;&gt; t.values.log10()\n┏━━━━━━━━━━━━━━━┓\n┃ Log10(values) ┃\n┡━━━━━━━━━━━━━━━┩\n│ float64       │\n├───────────────┤\n│           0.0 │\n│           1.0 │\n│           2.0 │\n└───────────────┘\n\n\n\nlog2\nexpr.types.numeric.NumericValue.log2(self)\nCompute \\(\\log_{2}\\left(\\texttt{self}\\right)\\).\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 2, 4, 8]})\n&gt;&gt;&gt; t.values.log2()\n┏━━━━━━━━━━━━━━┓\n┃ Log2(values) ┃\n┡━━━━━━━━━━━━━━┩\n│ float64      │\n├──────────────┤\n│          0.0 │\n│          1.0 │\n│          2.0 │\n│          3.0 │\n└──────────────┘\n\n\n\nnegate\nexpr.types.numeric.NumericValue.negate(self)\nNegate a numeric expression.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericValue\nA numeric value expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.negate()\n┏━━━━━━━━━━━━━━━━┓\n┃ Negate(values) ┃\n┡━━━━━━━━━━━━━━━━┩\n│ int64          │\n├────────────────┤\n│              1 │\n│              0 │\n│             -1 │\n└────────────────┘\n\n\n\nnullifzero\nexpr.types.numeric.NumericValue.nullifzero(self)\nReturn NULL if an expression is zero.\n\n\npoint\nexpr.types.numeric.NumericValue.point(self, right)\nReturn a point constructed from the coordinate values.\nConstant coordinates result in construction of a POINT literal or column.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nint | float | ibis.expr.types.numeric.NumericValue\nY coordinate\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nPointValue\nPoints\n\n\n\n\n\n\nradians\nexpr.types.numeric.NumericValue.radians(self)\nCompute radians from self degrees.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [0, 90, 180, 270, 360]})\n&gt;&gt;&gt; t.values.radians()\n┏━━━━━━━━━━━━━━━━━┓\n┃ Radians(values) ┃\n┡━━━━━━━━━━━━━━━━━┩\n│ float64         │\n├─────────────────┤\n│        0.000000 │\n│        1.570796 │\n│        3.141593 │\n│        4.712389 │\n│        6.283185 │\n└─────────────────┘\n\n\n\nround\nexpr.types.numeric.NumericValue.round(self, digits=None)\nRound values to an indicated number of decimal places.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndigits\nint | ibis.expr.types.numeric.IntegerValue | None\nThe number of digits to round to. Here’s how the digits parameter affects the expression output type: | digits | self.type() | Output | | :———–: | :———–: | :——-: | | None or 0 | decimal | decimal | | Nonzero | decimal | decimal | | None or 0 | Floating | int64 | | Nonzero | Floating | float64 |\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericValue\nThe rounded expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1.22, 1.64, 2.15, 2.54]})\n&gt;&gt;&gt; t\n┏━━━━━━━━━┓\n┃ values  ┃\n┡━━━━━━━━━┩\n│ float64 │\n├─────────┤\n│    1.22 │\n│    1.64 │\n│    2.15 │\n│    2.54 │\n└─────────┘\n&gt;&gt;&gt; t.values.round()\n┏━━━━━━━━━━━━━━━┓\n┃ Round(values) ┃\n┡━━━━━━━━━━━━━━━┩\n│ int64         │\n├───────────────┤\n│             1 │\n│             2 │\n│             2 │\n│             3 │\n└───────────────┘\n&gt;&gt;&gt; t.values.round(digits=1)\n┏━━━━━━━━━━━━━━━━━━┓\n┃ Round(values, 1) ┃\n┡━━━━━━━━━━━━━━━━━━┩\n│ float64          │\n├──────────────────┤\n│              1.2 │\n│              1.6 │\n│              2.2 │\n│              2.5 │\n└──────────────────┘\n\n\n\nsign\nexpr.types.numeric.NumericValue.sign(self)\nReturn the sign of the input.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 2, -3, 4]})\n&gt;&gt;&gt; t.values.sign()\n┏━━━━━━━━━━━━━━┓\n┃ Sign(values) ┃\n┡━━━━━━━━━━━━━━┩\n│ int64        │\n├──────────────┤\n│           -1 │\n│            1 │\n│           -1 │\n│            1 │\n└──────────────┘\n\n\n\nsin\nexpr.types.numeric.NumericValue.sin(self)\nCompute the sine of self.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.sin()\n┏━━━━━━━━━━━━━┓\n┃ Sin(values) ┃\n┡━━━━━━━━━━━━━┩\n│ float64     │\n├─────────────┤\n│   -0.841471 │\n│    0.000000 │\n│    0.841471 │\n└─────────────┘\n\n\n\nsqrt\nexpr.types.numeric.NumericValue.sqrt(self)\nCompute the square root of self.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 4, 9, 16]})\n&gt;&gt;&gt; t.values.sqrt()\n┏━━━━━━━━━━━━━━┓\n┃ Sqrt(values) ┃\n┡━━━━━━━━━━━━━━┩\n│ float64      │\n├──────────────┤\n│          1.0 │\n│          2.0 │\n│          3.0 │\n│          4.0 │\n└──────────────┘\n\n\n\ntan\nexpr.types.numeric.NumericValue.tan(self)\nCompute the tangent of self.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.tan()\n┏━━━━━━━━━━━━━┓\n┃ Tan(values) ┃\n┡━━━━━━━━━━━━━┩\n│ float64     │\n├─────────────┤\n│   -1.557408 │\n│    0.000000 │\n│    1.557408 │\n└─────────────┘\n\n\n\nzeroifnull\nexpr.types.numeric.NumericValue.zeroifnull(self)\nReturn zero if an expression is NULL."
  },
  {
    "objectID": "reference/expression-numeric.html#methods-1",
    "href": "reference/expression-numeric.html#methods-1",
    "title": "Numeric and Boolean Expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nbucket\nCompute a discrete binning of a numeric array.\n\n\ncorr\nReturn the correlation of two numeric columns.\n\n\ncov\nReturn the covariance of two numeric columns.\n\n\ncummean\nReturn the cumulative mean of the input.\n\n\ncumsum\nReturn the cumulative sum of the input.\n\n\nhistogram\nCompute a histogram with fixed width bins.\n\n\nmean\nReturn the mean of a numeric column.\n\n\nmedian\nReturn the median of the column.\n\n\nquantile\nReturn value at the given quantile.\n\n\nstd\nReturn the standard deviation of a numeric column.\n\n\nsum\nReturn the sum of a numeric column.\n\n\nvar\nReturn the variance of a numeric column.\n\n\n\n\nbucket\nexpr.types.numeric.NumericColumn.bucket(self, buckets, closed='left', close_extreme=True, include_under=False, include_over=False)\nCompute a discrete binning of a numeric array.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbuckets\ntyping.Sequence[int]\nList of buckets\nrequired\n\n\nclosed\ntyping.Literal[‘left’, ‘right’]\nWhich side of each interval is closed. For example: python buckets = [0, 100, 200] closed = \"left\"  # 100 falls in 2nd bucket closed = \"right\"  # 100 falls in 1st bucket\n'left'\n\n\nclose_extreme\nbool\nWhether the extreme values fall in the last bucket\nTrue\n\n\ninclude_over\nbool\nInclude values greater than the last bucket in the last bucket\nFalse\n\n\ninclude_under\nbool\nInclude values less than the first bucket in the first bucket\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerColumn\nA categorical column expression\n\n\n\n\n\n\ncorr\nexpr.types.numeric.NumericColumn.corr(self, right, where=None, how='sample')\nReturn the correlation of two numeric columns.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nNumericColumn\nNumeric column\nrequired\n\n\nwhere\nibis.BooleanValue | None\nFilter\nNone\n\n\nhow\ntyping.Literal[‘sample’, ‘pop’]\nPopulation or sample correlation\n'sample'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericScalar\nThe correlation of left and right\n\n\n\n\n\n\ncov\nexpr.types.numeric.NumericColumn.cov(self, right, where=None, how='sample')\nReturn the covariance of two numeric columns.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nNumericColumn\nNumeric column\nrequired\n\n\nwhere\nibis.BooleanValue | None\nFilter\nNone\n\n\nhow\ntyping.Literal[‘sample’, ‘pop’]\nPopulation or sample covariance\n'sample'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericScalar\nThe covariance of self and right\n\n\n\n\n\n\ncummean\nexpr.types.numeric.NumericColumn.cummean(self)\nReturn the cumulative mean of the input.\n\n\ncumsum\nexpr.types.numeric.NumericColumn.cumsum(self)\nReturn the cumulative sum of the input.\n\n\nhistogram\nexpr.types.numeric.NumericColumn.histogram(self, nbins=None, binwidth=None, base=None, eps=1e-13)\nCompute a histogram with fixed width bins.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnbins\nint | None\nIf supplied, will be used to compute the binwidth\nNone\n\n\nbinwidth\nfloat | None\nIf not supplied, computed from the data (actual max and min values)\nNone\n\n\nbase\nfloat | None\nThe value of the first histogram bin. Defaults to the minimum value of column.\nNone\n\n\neps\nfloat\nAllowed floating point epsilon for histogram base\n1e-13\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nColumn\nBucketed column\n\n\n\n\n\n\nmean\nexpr.types.numeric.NumericColumn.mean(self, where=None)\nReturn the mean of a numeric column.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nibis.BooleanValue | None\nFilter\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericScalar\nThe mean of the input expression\n\n\n\n\n\n\nmedian\nexpr.types.numeric.NumericColumn.median(self, where=None)\nReturn the median of the column.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nibis.BooleanValue | None\nOptional boolean expression. If given, only the values where where evaluates to true will be considered for the median.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericScalar\nMedian of the column\n\n\n\n\n\n\nquantile\nexpr.types.numeric.NumericColumn.quantile(self, quantile, interpolation=None, where=None)\nReturn value at the given quantile.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquantile\ntyping.Sequence[ibis.expr.types.numeric.NumericValue | float]\n0 &lt;= quantile &lt;= 1, the quantile(s) to compute\nrequired\n\n\ninterpolation\ntyping.Literal[‘linear’, ‘lower’, ‘higher’, ‘midpoint’, ‘nearest’] | None\n!!! warning “This parameter is backend dependent and may have no effect” This parameter specifies the interpolation method to use, when the desired quantile lies between two data points i and j: * linear: i + (j - i) * fraction, where fraction is the fractional part of the index surrounded by i and j. * lower: i. * higher: j. * nearest: i or j whichever is nearest. * midpoint: (i + j) / 2.\nNone\n\n\nwhere\nibis.BooleanValue | None\nBoolean filter for input values\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericScalar\nQuantile of the input\n\n\n\n\n\n\nstd\nexpr.types.numeric.NumericColumn.std(self, where=None, how='sample')\nReturn the standard deviation of a numeric column.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nibis.BooleanValue | None\nFilter\nNone\n\n\nhow\ntyping.Literal[‘sample’, ‘pop’]\nSample or population standard deviation\n'sample'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericScalar\nStandard deviation of arg\n\n\n\n\n\n\nsum\nexpr.types.numeric.NumericColumn.sum(self, where=None)\nReturn the sum of a numeric column.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nibis.BooleanValue | None\nFilter\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericScalar\nThe sum of the input expression\n\n\n\n\n\n\nvar\nexpr.types.numeric.NumericColumn.var(self, where=None, how='sample')\nReturn the variance of a numeric column.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nibis.BooleanValue | None\nFilter\nNone\n\n\nhow\ntyping.Literal[‘sample’, ‘pop’]\nSample or population variance\n'sample'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericScalar\nStandard deviation of arg"
  },
  {
    "objectID": "reference/expression-numeric.html#methods-2",
    "href": "reference/expression-numeric.html#methods-2",
    "title": "Numeric and Boolean Expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nconvert_base\nConvert an integer from one base to another.\n\n\nlabel\nLabel a set of integer values with strings.\n\n\nto_interval\nConvert an integer to an interval.\n\n\nto_timestamp\nConvert an integral UNIX timestamp to a timestamp expression.\n\n\n\n\nconvert_base\nexpr.types.numeric.IntegerValue.convert_base(self, from_base, to_base)\nConvert an integer from one base to another.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfrom_base\nIntegerValue\nNumeric base of expression\nrequired\n\n\nto_base\nIntegerValue\nNew base\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nConverted expression\n\n\n\n\n\n\nlabel\nexpr.types.numeric.IntegerValue.label(self, labels, nulls=None)\nLabel a set of integer values with strings.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlabels\ntyping.Iterable[str]\nAn iterable of string labels. Each integer value in self will be mapped to a value in labels.\nrequired\n\n\nnulls\nstr | None\nString label to use for NULL values\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nself labeled with labels\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [0, 1, 0, 2]})\n&gt;&gt;&gt; t.select(t.a, labeled=t.a.label([\"a\", \"b\", \"c\"]))\n┏━━━━━━━┳━━━━━━━━━┓\n┃ a     ┃ labeled ┃\n┡━━━━━━━╇━━━━━━━━━┩\n│ int64 │ string  │\n├───────┼─────────┤\n│     0 │ a       │\n│     1 │ b       │\n│     0 │ a       │\n│     2 │ c       │\n└───────┴─────────┘\n\n\n\nto_interval\nexpr.types.numeric.IntegerValue.to_interval(self, unit='s')\nConvert an integer to an interval.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nunit\ntyping.Literal[‘Y’, ‘M’, ‘W’, ‘D’, ‘h’, ‘m’, ‘s’, ‘ms’, ‘us’, ‘ns’]\nUnit for the resulting interval\n's'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntervalValue\nAn interval in units of unit\n\n\n\n\n\n\nto_timestamp\nexpr.types.numeric.IntegerValue.to_timestamp(self, unit='s')\nConvert an integral UNIX timestamp to a timestamp expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nunit\ntyping.Literal[‘s’, ‘ms’, ‘us’]\nThe resolution of arg\n's'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTimestampValue\nself converted to a timestamp"
  },
  {
    "objectID": "reference/expression-numeric.html#methods-3",
    "href": "reference/expression-numeric.html#methods-3",
    "title": "Numeric and Boolean Expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nbit_and\nAggregate the column using the bitwise and operator.\n\n\nbit_or\nAggregate the column using the bitwise or operator.\n\n\nbit_xor\nAggregate the column using the bitwise exclusive or operator.\n\n\n\n\nbit_and\nexpr.types.numeric.IntegerColumn.bit_and(self, where=None)\nAggregate the column using the bitwise and operator.\n\n\nbit_or\nexpr.types.numeric.IntegerColumn.bit_or(self, where=None)\nAggregate the column using the bitwise or operator.\n\n\nbit_xor\nexpr.types.numeric.IntegerColumn.bit_xor(self, where=None)\nAggregate the column using the bitwise exclusive or operator."
  },
  {
    "objectID": "reference/expression-numeric.html#methods-4",
    "href": "reference/expression-numeric.html#methods-4",
    "title": "Numeric and Boolean Expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nisinf\nReturn whether the value is infinity.\n\n\nisnan\nReturn whether the value is NaN.\n\n\n\n\nisinf\nexpr.types.numeric.FloatingValue.isinf(self)\nReturn whether the value is infinity.\n\n\nisnan\nexpr.types.numeric.FloatingValue.isnan(self)\nReturn whether the value is NaN."
  },
  {
    "objectID": "reference/expression-numeric.html#methods-5",
    "href": "reference/expression-numeric.html#methods-5",
    "title": "Numeric and Boolean Expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nifelse\nConstruct a ternary conditional expression.\n\n\n\n\nifelse\nexpr.types.logical.BooleanValue.ifelse(self, true_expr, false_expr)\nConstruct a ternary conditional expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntrue_expr\nibis.Value\nExpression to return if self evaluates to True\nrequired\n\n\nfalse_expr\nibis.Value\nExpression to return if self evaluates to False or NULL\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nValue\nThe value of true_expr if arg is True else false_expr\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"is_person\": [True, False, True, None]})\n&gt;&gt;&gt; t.is_person.ifelse(\"yes\", \"no\")\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Where(is_person, 'yes', 'no') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                        │\n├───────────────────────────────┤\n│ yes                           │\n│ no                            │\n│ yes                           │\n│ no                            │\n└───────────────────────────────┘"
  },
  {
    "objectID": "reference/config.ContextAdjustment.html",
    "href": "reference/config.ContextAdjustment.html",
    "title": "config.ContextAdjustment",
    "section": "",
    "text": "config.ContextAdjustment()\nOptions related to time context adjustment.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntime_col\nstr\nName of the timestamp column for execution with a timecontext. See ibis/expr/timecontext.py for details."
  },
  {
    "objectID": "reference/config.ContextAdjustment.html#attributes",
    "href": "reference/config.ContextAdjustment.html#attributes",
    "title": "config.ContextAdjustment",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ntime_col\nstr\nName of the timestamp column for execution with a timecontext. See ibis/expr/timecontext.py for details."
  },
  {
    "objectID": "reference/config.SQL.html",
    "href": "reference/config.SQL.html",
    "title": "config.SQL",
    "section": "",
    "text": "config.SQL()\nSQL-related options.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndefault_limit\nint | None\nNumber of rows to be retrieved for a table expression without an explicit limit. [None][None] means no limit.\n\n\ndefault_dialect\nstr\nDialect to use for printing SQL when the backend cannot be determined."
  },
  {
    "objectID": "reference/config.SQL.html#attributes",
    "href": "reference/config.SQL.html#attributes",
    "title": "config.SQL",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ndefault_limit\nint | None\nNumber of rows to be retrieved for a table expression without an explicit limit. [None][None] means no limit.\n\n\ndefault_dialect\nstr\nDialect to use for printing SQL when the backend cannot be determined."
  },
  {
    "objectID": "reference/expression-tables.html",
    "href": "reference/expression-tables.html",
    "title": "Table Expressions",
    "section": "",
    "text": "Table expressions form the basis for most Ibis expressions."
  },
  {
    "objectID": "reference/expression-tables.html#attributes",
    "href": "reference/expression-tables.html#attributes",
    "title": "Table Expressions",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\ncolumns\nThe list of columns in this table."
  },
  {
    "objectID": "reference/expression-tables.html#methods",
    "href": "reference/expression-tables.html#methods",
    "title": "Table Expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\naggregate\nAggregate a table with a given set of reductions grouping by by.\n\n\nalias\nCreate a table expression with a specific name alias.\n\n\nas_table\nPromote the expression to a table.\n\n\nasof_join\nPerform an “as-of” join between left and right.\n\n\ncache\nCache the provided expression.\n\n\ncast\nCast the columns of a table.\n\n\ncount\nCompute the number of rows in the table.\n\n\ncross_join\nCompute the cross join of a sequence of tables.\n\n\ndifference\nCompute the set difference of multiple table expressions.\n\n\ndistinct\nReturn a Table with duplicate rows removed.\n\n\ndrop\nRemove fields from a table.\n\n\ndropna\nRemove rows with null values from the table.\n\n\nfillna\nFill null values in a table expression.\n\n\nfilter\nSelect rows from table based on predicates.\n\n\ngroup_by\nCreate a grouped table expression.\n\n\nhead\nSelect the first n rows of a table.\n\n\ninfo\nReturn summary information about a table.\n\n\nintersect\nCompute the set intersection of multiple table expressions.\n\n\njoin\nPerform a join between two tables.\n\n\nlimit\nSelect n rows from self starting at offset.\n\n\nmutate\nAdd columns to a table expression.\n\n\norder_by\nSort a table by one or more expressions.\n\n\npivot_longer\nTransform a table from wider to longer.\n\n\npivot_wider\nPivot a table to a wider format.\n\n\nrelabel\nRename columns in the table.\n\n\nrowid\nA unique integer per row.\n\n\nschema\nReturn the schema for this table.\n\n\nselect\nCompute a new table expression using exprs and named_exprs.\n\n\nsql\nRun a SQL query against a table expression.\n\n\nto_array\nView a single column table as an array.\n\n\nto_pandas\nConvert a table expression to a pandas DataFrame.\n\n\ntry_cast\nCast the columns of a table.\n\n\nunion\nCompute the set union of multiple table expressions.\n\n\nunpack\nProject the struct fields of each of columns into self.\n\n\nview\nCreate a new table expression distinct from the current one.\n\n\n\n\naggregate\nexpr.types.relations.Table.aggregate(self, metrics=None, by=None, having=None, **kwargs)\nAggregate a table with a given set of reductions grouping by by.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmetrics\ntyping.Sequence[ibis.Scalar] | None\nAggregate expressions. These can be any scalar-producing expression, including aggregation functions like sum or literal values like ibis.literal(1).\nNone\n\n\nby\ntyping.Sequence[ibis.Value] | None\nGrouping expressions.\nNone\n\n\nhaving\ntyping.Sequence[ibis.BooleanValue] | None\nPost-aggregation filters. The shape requirements are the same metrics, but the output type for having is boolean. !!! warning “Expressions like x is None return bool and will not generate a SQL comparison to NULL”\nNone\n\n\nkwargs\nibis.Value\nNamed aggregate expressions\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nAn aggregate table expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"fruit\": [\"apple\", \"apple\", \"banana\", \"orange\"], \"price\": [0.5, 0.5, 0.25, 0.33]})\n&gt;&gt;&gt; t\n┏━━━━━━━━┳━━━━━━━━━┓\n┃ fruit  ┃ price   ┃\n┡━━━━━━━━╇━━━━━━━━━┩\n│ string │ float64 │\n├────────┼─────────┤\n│ apple  │    0.50 │\n│ apple  │    0.50 │\n│ banana │    0.25 │\n│ orange │    0.33 │\n└────────┴─────────┘\n&gt;&gt;&gt; t.aggregate(by=[\"fruit\"], total_cost=_.price.sum(), avg_cost=_.price.mean(), having=_.price.sum() &lt; 0.5)\n┏━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ fruit  ┃ total_cost ┃ avg_cost ┃\n┡━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string │ float64    │ float64  │\n├────────┼────────────┼──────────┤\n│ banana │       0.25 │     0.25 │\n│ orange │       0.33 │     0.33 │\n└────────┴────────────┴──────────┘\n\n\n\nalias\nexpr.types.relations.Table.alias(self, alias)\nCreate a table expression with a specific name alias.\nThis method is useful for exposing an ibis expression to the underlying backend for use in the [Table.sql][ibis.expr.types.relations.Table.sql] method.\n!!! note “.alias will create a temporary view”\n`.alias` creates a temporary view in the database.\n\nThis side effect will be removed in a future version of ibis and\n**is not part of the public API**.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nalias\nstr\nName of the child expression\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nAn table expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; expr = t.alias(\"pingüinos\").sql('SELECT * FROM \"pingüinos\" LIMIT 5')\n&gt;&gt;&gt; expr\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ … │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │ … │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │ … │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │ … │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │ … │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │ … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴───┘\n\n\n\nas_table\nexpr.types.relations.Table.as_table(self)\nPromote the expression to a table.\nThis method is a no-op for table expressions.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA table expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; t = ibis.table(dict(a=\"int\"), name=\"t\")\n&gt;&gt;&gt; s = t.as_table()\n&gt;&gt;&gt; t is s\nTrue\n\n\n\nasof_join\nexpr.types.relations.Table.asof_join(left, right, predicates=(), by=(), tolerance=None, *, lname='', rname='{name}_right')\nPerform an “as-of” join between left and right.\nSimilar to a left join except that the match is done on nearest key rather than equal keys.\nOptionally, match keys with by before joining with predicates.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nleft\nTable\nTable expression\nrequired\n\n\nright\nTable\nTable expression\nrequired\n\n\npredicates\nstr | ibis.BooleanColumn | typing.Sequence[str | ibis.BooleanColumn]\nJoin expressions\n()\n\n\nby\nstr | ibis.Column | typing.Sequence[str | ibis.Column]\ncolumn to group by before joining\n()\n\n\ntolerance\nstr | ibis.IntervalScalar | None\nAmount of time to look behind when joining\nNone\n\n\nlname\nstr\nA format string to use to rename overlapping columns in the left table (e.g. \"left_{name}\").\n''\n\n\nrname\nstr\nA format string to use to rename overlapping columns in the right table (e.g. \"right_{name}\").\n'{name}_right'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\ncache\nexpr.types.relations.Table.cache(self)\nCache the provided expression.\nAll subsequent operations on the returned expression will be performed on the cached data. Use the with statement to limit the lifetime of a cached table.\nThis method is idempotent: calling it multiple times in succession will return the same value as the first call.\n!!! note “This method eagerly evaluates the expression prior to caching”\nSubsequent evaluations will not recompute the expression so method\nchaining will not incur the overhead of caching more than once.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nCached table\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; cached_penguins = t.mutate(computation=\"Heavy Computation\").cache()\n&gt;&gt;&gt; cached_penguins\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ … │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │ … │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │ … │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │ … │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │ … │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │ … │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │ … │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │ … │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │ … │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │ … │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │ … │\n│ …       │ …         │              … │             … │                 … │ … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴───┘\nExplicit cache cleanup\n&gt;&gt;&gt; with t.mutate(computation=\"Heavy Computation\").cache() as cached_penguins:\n...     cached_penguins\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ … │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │ … │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │ … │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │ … │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │ … │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │ … │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │ … │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │ … │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │ … │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │ … │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │ … │\n│ …       │ …         │              … │             … │                 … │ … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴───┘\n\n\n\ncast\nexpr.types.relations.Table.cast(self, schema)\nCast the columns of a table.\n!!! note “If you need to cast columns to a single type, use selectors.”\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nschema\nSupportsSchema\nMapping, schema or iterable of pairs to use for casting\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nCasted table\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.schema()\nibis.Schema {\n  species            string\n  island             string\n  bill_length_mm     float64\n  bill_depth_mm      float64\n  flipper_length_mm  int64\n  body_mass_g        int64\n  sex                string\n  year               int64\n}\n&gt;&gt;&gt; cols = [\"body_mass_g\", \"bill_length_mm\"]\n&gt;&gt;&gt; t[cols].head()\n┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ body_mass_g ┃ bill_length_mm ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ int64       │ float64        │\n├─────────────┼────────────────┤\n│        3750 │           39.1 │\n│        3800 │           39.5 │\n│        3250 │           40.3 │\n│        NULL │            nan │\n│        3450 │           36.7 │\n└─────────────┴────────────────┘\nColumns not present in the input schema will be passed through unchanged\n&gt;&gt;&gt; t.columns\n['species', 'island', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex', 'year']\n&gt;&gt;&gt; expr = t.cast({\"body_mass_g\": \"float64\", \"bill_length_mm\": \"int\"})\n&gt;&gt;&gt; expr.select(*cols).head()\n┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ body_mass_g ┃ bill_length_mm ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ float64     │ int64          │\n├─────────────┼────────────────┤\n│      3750.0 │             39 │\n│      3800.0 │             40 │\n│      3250.0 │             40 │\n│         nan │           NULL │\n│      3450.0 │             37 │\n└─────────────┴────────────────┘\nColumns that are in the input schema but not in the table raise an error\n&gt;&gt;&gt; t.cast({\"foo\": \"string\"})\nTraceback (most recent call last):\n    ...\nibis.common.exceptions.IbisError: Cast schema has fields that are not in the table: ['foo']\n\n\n\ncount\nexpr.types.relations.Table.count(self, where=None)\nCompute the number of rows in the table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nibis.BooleanValue | None\nOptional boolean expression to filter rows when counting.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerScalar\nNumber of rows in the table\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [\"foo\", \"bar\", \"baz\"]})\n&gt;&gt;&gt; t\n┏━━━━━━━━┓\n┃ a      ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ foo    │\n│ bar    │\n│ baz    │\n└────────┘\n&gt;&gt;&gt; t.count()\n3\n&gt;&gt;&gt; t.count(t.a != \"foo\")\n2\n&gt;&gt;&gt; type(t.count())\n&lt;class 'ibis.expr.types.numeric.IntegerScalar'&gt;\n\n\n\ncross_join\nexpr.types.relations.Table.cross_join(left, right, *rest, lname='', rname='{name}_right')\nCompute the cross join of a sequence of tables.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nleft\nTable\nLeft table\nrequired\n\n\nright\nTable\nRight table\nrequired\n\n\nrest\nTable\nAdditional tables to cross join\n()\n\n\nlname\nstr\nA format string to use to rename overlapping columns in the left table (e.g. \"left_{name}\").\n''\n\n\nrname\nstr\nA format string to use to rename overlapping columns in the right table (e.g. \"right_{name}\").\n'{name}_right'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nCross join of left, right and rest\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.count()\n344\n&gt;&gt;&gt; agg = t.drop(\"year\").agg(s.across(s.numeric(), _.mean()))\n&gt;&gt;&gt; expr = t.cross_join(agg)\n&gt;&gt;&gt; expr\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ … │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │ … │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │ … │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │ … │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │ … │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │ … │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │ … │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │ … │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │ … │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │ … │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │ … │\n│ …       │ …         │              … │             … │                 … │ … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴───┘\n&gt;&gt;&gt; expr.columns\n['species',\n 'island',\n 'bill_length_mm',\n 'bill_depth_mm',\n 'flipper_length_mm',\n 'body_mass_g',\n 'sex',\n 'year',\n 'bill_length_mm_right',\n 'bill_depth_mm_right',\n 'flipper_length_mm_right',\n 'body_mass_g_right']\n&gt;&gt;&gt; expr.count()\n344\n\n\n\ndifference\nexpr.types.relations.Table.difference(self, table, *rest, distinct=True)\nCompute the set difference of multiple table expressions.\nThe input tables must have identical schemas.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable\nTable\nA table expression\nrequired\n\n\n*rest\nTable\nAdditional table expressions\n()\n\n\ndistinct\nbool\nOnly diff distinct rows not occurring in the calling table\nTrue\n\n\n\n\n\nSee Also\n[ibis.difference][ibis.difference]\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe rows present in self that are not present in tables.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t1 = ibis.memtable({\"a\": [1, 2]})\n&gt;&gt;&gt; t1\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     2 │\n└───────┘\n&gt;&gt;&gt; t2 = ibis.memtable({\"a\": [2, 3]})\n&gt;&gt;&gt; t2\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     2 │\n│     3 │\n└───────┘\n&gt;&gt;&gt; t1.difference(t2)\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n└───────┘\n\n\n\ndistinct\nexpr.types.relations.Table.distinct(self, *, on=None, keep='first')\nReturn a Table with duplicate rows removed.\nSimilar to pandas.DataFrame.drop_duplicates().\n!!! note “Some backends do not support keep='last'”\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\non\nstr | typing.Iterable[str] | ibis.Selector | None\nOnly consider certain columns for identifying duplicates. By default deduplicate all of the columns.\nNone\n\n\nkeep\ntyping.Literal[‘first’, ‘last’] | None\nDetermines which duplicates to keep. - \"first\": Drop duplicates except for the first occurrence. - \"last\": Drop duplicates except for the last occurrence. - None: Drop all duplicates\n'first'\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.examples as ex\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ex.penguins.fetch()\n&gt;&gt;&gt; t\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ … │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │ … │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │ … │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │ … │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │ … │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │ … │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │ … │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │ … │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │ … │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │ … │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │ … │\n│ …       │ …         │              … │             … │                 … │ … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴───┘\nCompute the distinct rows of a subset of columns\n&gt;&gt;&gt; t[[\"species\", \"island\"]].distinct()\n┏━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ species   ┃ island    ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string    │ string    │\n├───────────┼───────────┤\n│ Adelie    │ Torgersen │\n│ Adelie    │ Biscoe    │\n│ Adelie    │ Dream     │\n│ Gentoo    │ Biscoe    │\n│ Chinstrap │ Dream     │\n└───────────┴───────────┘\nDrop all duplicate rows except the first\n&gt;&gt;&gt; t.distinct(on=[\"species\", \"island\"], keep=\"first\")\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━┓\n┃ species   ┃ island    ┃ bill_length_mm ┃ bill_depth_… ┃ flipper_length_mm ┃  ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━┩\n│ string    │ string    │ float64        │ float64      │ int64             │  │\n├───────────┼───────────┼────────────────┼──────────────┼───────────────────┼──┤\n│ Adelie    │ Torgersen │           39.1 │         18.7 │               181 │  │\n│ Adelie    │ Biscoe    │           37.8 │         18.3 │               174 │  │\n│ Adelie    │ Dream     │           39.5 │         16.7 │               178 │  │\n│ Gentoo    │ Biscoe    │           46.1 │         13.2 │               211 │  │\n│ Chinstrap │ Dream     │           46.5 │         17.9 │               192 │  │\n└───────────┴───────────┴────────────────┴──────────────┴───────────────────┴──┘\nDrop all duplicate rows except the last\n&gt;&gt;&gt; t.distinct(on=[\"species\", \"island\"], keep=\"last\")\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━┓\n┃ species   ┃ island    ┃ bill_length_mm ┃ bill_depth_… ┃ flipper_length_mm ┃  ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━┩\n│ string    │ string    │ float64        │ float64      │ int64             │  │\n├───────────┼───────────┼────────────────┼──────────────┼───────────────────┼──┤\n│ Adelie    │ Torgersen │           43.1 │         19.2 │               197 │  │\n│ Adelie    │ Biscoe    │           42.7 │         18.3 │               196 │  │\n│ Adelie    │ Dream     │           41.5 │         18.5 │               201 │  │\n│ Gentoo    │ Biscoe    │           49.9 │         16.1 │               213 │  │\n│ Chinstrap │ Dream     │           50.2 │         18.7 │               198 │  │\n└───────────┴───────────┴────────────────┴──────────────┴───────────────────┴──┘\nDrop all duplicated rows\n&gt;&gt;&gt; expr = t.distinct(on=[\"species\", \"island\", \"year\", \"bill_length_mm\"], keep=None)\n&gt;&gt;&gt; expr.count()\n273\n&gt;&gt;&gt; t.count()\n344\nYou can pass [selectors][ibis.selectors] to on\n&gt;&gt;&gt; t.distinct(on=~s.numeric())\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ … │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │ … │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │ … │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │ … │\n│ Adelie  │ Biscoe    │           37.8 │          18.3 │               174 │ … │\n│ Adelie  │ Biscoe    │           37.7 │          18.7 │               180 │ … │\n│ Adelie  │ Dream     │           39.5 │          16.7 │               178 │ … │\n│ Adelie  │ Dream     │           37.2 │          18.1 │               178 │ … │\n│ Adelie  │ Dream     │           37.5 │          18.9 │               179 │ … │\n│ Gentoo  │ Biscoe    │           46.1 │          13.2 │               211 │ … │\n│ Gentoo  │ Biscoe    │           50.0 │          16.3 │               230 │ … │\n│ …       │ …         │              … │             … │                 … │ … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴───┘\nThe only valid values of keep are \"first\", \"last\" and [`None][None]\n&gt;&gt;&gt; t.distinct(on=\"species\", keep=\"second\")\nTraceback (most recent call last):\n  ...\nibis.common.exceptions.IbisError: Invalid value for keep: 'second' ...\n\n\n\ndrop\nexpr.types.relations.Table.drop(self, *fields)\nRemove fields from a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfields\nstr | ibis.selectors.Selector\nFields to drop. Strings and selectors are accepted.\n()\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA table with all columns matching fields removed.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ … │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │ … │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │ … │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │ … │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │ … │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │ … │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │ … │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │ … │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │ … │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │ … │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │ … │\n│ …       │ …         │              … │             … │                 … │ … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴───┘\nDrop one or more columns\n&gt;&gt;&gt; t.drop(\"species\").head()\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string    │ float64        │ float64       │ int64             │ … │\n├───────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Torgersen │           39.1 │          18.7 │               181 │ … │\n│ Torgersen │           39.5 │          17.4 │               186 │ … │\n│ Torgersen │           40.3 │          18.0 │               195 │ … │\n│ Torgersen │            nan │           nan │              NULL │ … │\n│ Torgersen │           36.7 │          19.3 │               193 │ … │\n└───────────┴────────────────┴───────────────┴───────────────────┴───┘\n&gt;&gt;&gt; t.drop(\"species\", \"bill_length_mm\").head()\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━┓\n┃ island    ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ … ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━┩\n│ string    │ float64       │ int64             │ int64       │ string │ … │\n├───────────┼───────────────┼───────────────────┼─────────────┼────────┼───┤\n│ Torgersen │          18.7 │               181 │        3750 │ male   │ … │\n│ Torgersen │          17.4 │               186 │        3800 │ female │ … │\n│ Torgersen │          18.0 │               195 │        3250 │ female │ … │\n│ Torgersen │           nan │              NULL │        NULL │ NULL   │ … │\n│ Torgersen │          19.3 │               193 │        3450 │ female │ … │\n└───────────┴───────────────┴───────────────────┴─────────────┴────────┴───┘\nDrop with selectors, mix and match\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t.drop(\"species\", s.startswith(\"bill_\")).head()\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ island    ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string    │ int64             │ int64       │ string │ int64 │\n├───────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Torgersen │               181 │        3750 │ male   │  2007 │\n│ Torgersen │               186 │        3800 │ female │  2007 │\n│ Torgersen │               195 │        3250 │ female │  2007 │\n│ Torgersen │              NULL │        NULL │ NULL   │  2007 │\n│ Torgersen │               193 │        3450 │ female │  2007 │\n└───────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\ndropna\nexpr.types.relations.Table.dropna(self, subset=None, how='any')\nRemove rows with null values from the table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsubset\ntyping.Sequence[str] | str | None\nColumns names to consider when dropping nulls. By default all columns are considered.\nNone\n\n\nhow\ntyping.Literal[‘any’, ‘all’]\nDetermine whether a row is removed if there is at least one null value in the row ('any'), or if all row values are null ('all').\n'any'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ … │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │ … │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │ … │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │ … │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │ … │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │ … │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │ … │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │ … │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │ … │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │ … │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │ … │\n│ …       │ …         │              … │             … │                 … │ … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴───┘\n&gt;&gt;&gt; t.count()\n344\n&gt;&gt;&gt; t.dropna([\"bill_length_mm\", \"body_mass_g\"]).count()\n342\n&gt;&gt;&gt; t.dropna(how=\"all\").count()  # no rows where all columns are null\n344\n\n\n\nfillna\nexpr.types.relations.Table.fillna(self, replacements)\nFill null values in a table expression.\n!!! note “There is potential lack of type stability with the fillna API”\nFor example, different library versions may impact whether a given\nbackend promotes integer replacement values to floats.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nreplacements\nibis.Scalar | typing.Mapping[str, ibis.Scalar]\nValue with which to fill nulls. If replacements is a mapping, the keys are column names that map to their replacement value. If passed as a scalar all columns are filled with that value.\nrequired\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.sex\n┏━━━━━━━━┓\n┃ sex    ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ male   │\n│ female │\n│ female │\n│ NULL   │\n│ female │\n│ male   │\n│ female │\n│ male   │\n│ NULL   │\n│ NULL   │\n│ …      │\n└────────┘\n&gt;&gt;&gt; t.fillna({\"sex\": \"unrecorded\"}).sex\n┏━━━━━━━━━━━━┓\n┃ sex        ┃\n┡━━━━━━━━━━━━┩\n│ string     │\n├────────────┤\n│ male       │\n│ female     │\n│ female     │\n│ unrecorded │\n│ female     │\n│ male       │\n│ female     │\n│ male       │\n│ unrecorded │\n│ unrecorded │\n│ …          │\n└────────────┘\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nfilter\nexpr.types.relations.Table.filter(self, predicates)\nSelect rows from table based on predicates.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npredicates\nibis.BooleanValue | typing.Sequence[ibis.BooleanValue] | ibis.selectors.IfAnyAll\nBoolean value expressions used to select rows in table.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nFiltered table expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ … │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │ … │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │ … │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │ … │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │ … │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │ … │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │ … │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │ … │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │ … │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │ … │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │ … │\n│ …       │ …         │              … │             … │                 … │ … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴───┘\n&gt;&gt;&gt; t.filter([t.species == \"Adelie\", t.body_mass_g &gt; 3500]).sex.value_counts().dropna(\"sex\")\n┏━━━━━━━━┳━━━━━━━━━━━┓\n┃ sex    ┃ sex_count ┃\n┡━━━━━━━━╇━━━━━━━━━━━┩\n│ string │ int64     │\n├────────┼───────────┤\n│ male   │        68 │\n│ female │        22 │\n└────────┴───────────┘\n\n\n\ngroup_by\nexpr.types.relations.Table.group_by(self, by=None, **key_exprs)\nCreate a grouped table expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nby\nstr | ibis.Value | typing.Iterable[str] | typing.Iterable[ibis.Value] | None\nGrouping expressions\nNone\n\n\nkey_exprs\nstr | ibis.Value | typing.Iterable[str] | typing.Iterable[ibis.Value]\nNamed grouping expressions\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGroupedTable\nA grouped table expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"fruit\": [\"apple\", \"apple\", \"banana\", \"orange\"], \"price\": [0.5, 0.5, 0.25, 0.33]})\n&gt;&gt;&gt; t\n┏━━━━━━━━┳━━━━━━━━━┓\n┃ fruit  ┃ price   ┃\n┡━━━━━━━━╇━━━━━━━━━┩\n│ string │ float64 │\n├────────┼─────────┤\n│ apple  │    0.50 │\n│ apple  │    0.50 │\n│ banana │    0.25 │\n│ orange │    0.33 │\n└────────┴─────────┘\n&gt;&gt;&gt; t.group_by(\"fruit\").agg(total_cost=_.price.sum(), avg_cost=_.price.mean())\n┏━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ fruit  ┃ total_cost ┃ avg_cost ┃\n┡━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string │ float64    │ float64  │\n├────────┼────────────┼──────────┤\n│ apple  │       1.00 │     0.50 │\n│ banana │       0.25 │     0.25 │\n│ orange │       0.33 │     0.33 │\n└────────┴────────────┴──────────┘\n\n\n\nhead\nexpr.types.relations.Table.head(self, n=5)\nSelect the first n rows of a table.\n!!! note “The result set is not deterministic without a call to [order_by][ibis.expr.types.relations.Table.order_by].”\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nNumber of rows to include\n5\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nself limited to n rows\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [1, 1, 2], \"b\": [\"c\", \"a\", \"a\"]})\n&gt;&gt;&gt; t\n┏━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ b      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ c      │\n│     1 │ a      │\n│     2 │ a      │\n└───────┴────────┘\n&gt;&gt;&gt; t.head(2)\n┏━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ b      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ c      │\n│     1 │ a      │\n└───────┴────────┘\n\n\nSee Also\n[Table.limit][ibis.expr.types.relations.Table.limit] [Table.order_by][ibis.expr.types.relations.Table.order_by]\n\n\n\ninfo\nexpr.types.relations.Table.info(self)\nReturn summary information about a table.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nSummary of self\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.info()\n┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━┓\n┃ name              ┃ type    ┃ nullable ┃ nulls ┃ non_nulls ┃ null_frac ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━┩\n│ string            │ string  │ boolean  │ int64 │ int64     │ float64   │ … │\n├───────────────────┼─────────┼──────────┼───────┼───────────┼───────────┼───┤\n│ species           │ string  │ True     │     0 │       344 │  0.000000 │ … │\n│ island            │ string  │ True     │     0 │       344 │  0.000000 │ … │\n│ bill_length_mm    │ float64 │ True     │     2 │       342 │  0.005814 │ … │\n│ bill_depth_mm     │ float64 │ True     │     2 │       342 │  0.005814 │ … │\n│ flipper_length_mm │ int64   │ True     │     2 │       342 │  0.005814 │ … │\n│ body_mass_g       │ int64   │ True     │     2 │       342 │  0.005814 │ … │\n│ sex               │ string  │ True     │    11 │       333 │  0.031977 │ … │\n│ year              │ int64   │ True     │     0 │       344 │  0.000000 │ … │\n└───────────────────┴─────────┴──────────┴───────┴───────────┴───────────┴───┘\n\n\n\nintersect\nexpr.types.relations.Table.intersect(self, table, *rest, distinct=True)\nCompute the set intersection of multiple table expressions.\nThe input tables must have identical schemas.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable\nTable\nA table expression\nrequired\n\n\n*rest\nTable\nAdditional table expressions\n()\n\n\ndistinct\nbool\nOnly return distinct rows\nTrue\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA new table containing the intersection of all input tables.\n\n\n\n\n\nSee Also\n[ibis.intersect][ibis.intersect]\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t1 = ibis.memtable({\"a\": [1, 2]})\n&gt;&gt;&gt; t1\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     2 │\n└───────┘\n&gt;&gt;&gt; t2 = ibis.memtable({\"a\": [2, 3]})\n&gt;&gt;&gt; t2\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     2 │\n│     3 │\n└───────┘\n&gt;&gt;&gt; t1.intersect(t2)\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     2 │\n└───────┘\n\n\n\njoin\nexpr.types.relations.Table.join(left, right, predicates=(), how='inner', *, lname='', rname='{name}_right')\nPerform a join between two tables.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nleft\nTable\nLeft table to join\nrequired\n\n\nright\nTable\nRight table to join\nrequired\n\n\npredicates\nstr | typing.Sequence[str | tuple[str | ibis.Column, str | ibis.Column] | ibis.BooleanColumn]\nBoolean or column names to join on\n()\n\n\nhow\ntyping.Literal[‘inner’, ‘left’, ‘outer’, ‘right’, ‘semi’, ‘anti’, ‘any_inner’, ‘any_left’, ‘left_semi’]\nJoin method\n'inner'\n\n\nlname\nstr\nA format string to use to rename overlapping columns in the left table (e.g. \"left_{name}\").\n''\n\n\nrname\nstr\nA format string to use to rename overlapping columns in the right table (e.g. \"right_{name}\").\n'{name}_right'\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; import ibis.examples as ex\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; movies = ex.ml_latest_small_movies.fetch()\n&gt;&gt;&gt; movies\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ movieId ┃ title                            ┃ genres                          ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64   │ string                           │ string                          │\n├─────────┼──────────────────────────────────┼─────────────────────────────────┤\n│       1 │ Toy Story (1995)                 │ Adventure|Animation|Children|C… │\n│       2 │ Jumanji (1995)                   │ Adventure|Children|Fantasy      │\n│       3 │ Grumpier Old Men (1995)          │ Comedy|Romance                  │\n│       4 │ Waiting to Exhale (1995)         │ Comedy|Drama|Romance            │\n│       5 │ Father of the Bride Part II (19… │ Comedy                          │\n│       6 │ Heat (1995)                      │ Action|Crime|Thriller           │\n│       7 │ Sabrina (1995)                   │ Comedy|Romance                  │\n│       8 │ Tom and Huck (1995)              │ Adventure|Children              │\n│       9 │ Sudden Death (1995)              │ Action                          │\n│      10 │ GoldenEye (1995)                 │ Action|Adventure|Thriller       │\n│       … │ …                                │ …                               │\n└─────────┴──────────────────────────────────┴─────────────────────────────────┘\n&gt;&gt;&gt; links = ex.ml_latest_small_links.fetch()\n&gt;&gt;&gt; links\n┏━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┓\n┃ movieId ┃ imdbId  ┃ tmdbId ┃\n┡━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━┩\n│ int64   │ string  │ int64  │\n├─────────┼─────────┼────────┤\n│       1 │ 0114709 │    862 │\n│       2 │ 0113497 │   8844 │\n│       3 │ 0113228 │  15602 │\n│       4 │ 0114885 │  31357 │\n│       5 │ 0113041 │  11862 │\n│       6 │ 0113277 │    949 │\n│       7 │ 0114319 │  11860 │\n│       8 │ 0112302 │  45325 │\n│       9 │ 0114576 │   9091 │\n│      10 │ 0113189 │    710 │\n│       … │ …       │      … │\n└─────────┴─────────┴────────┘\nImplicit inner equality join on the shared movieId column\n&gt;&gt;&gt; linked = movies.join(links, \"movieId\", how=\"inner\")\n&gt;&gt;&gt; linked.head()\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┓\n┃ movieId ┃ title                  ┃ genres                 ┃ imdbId  ┃ tmdbId ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━┩\n│ int64   │ string                 │ string                 │ string  │ int64  │\n├─────────┼────────────────────────┼────────────────────────┼─────────┼────────┤\n│       1 │ Toy Story (1995)       │ Adventure|Animation|C… │ 0114709 │    862 │\n│       2 │ Jumanji (1995)         │ Adventure|Children|Fa… │ 0113497 │   8844 │\n│       3 │ Grumpier Old Men (199… │ Comedy|Romance         │ 0113228 │  15602 │\n│       4 │ Waiting to Exhale (19… │ Comedy|Drama|Romance   │ 0114885 │  31357 │\n│       5 │ Father of the Bride P… │ Comedy                 │ 0113041 │  11862 │\n└─────────┴────────────────────────┴────────────────────────┴─────────┴────────┘\nExplicit equality join using the default how value of \"inner\"\n&gt;&gt;&gt; linked = movies.join(links, movies.movieId == links.movieId)\n&gt;&gt;&gt; linked.head()\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┓\n┃ movieId ┃ title                  ┃ genres                 ┃ imdbId  ┃ tmdbId ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━┩\n│ int64   │ string                 │ string                 │ string  │ int64  │\n├─────────┼────────────────────────┼────────────────────────┼─────────┼────────┤\n│       1 │ Toy Story (1995)       │ Adventure|Animation|C… │ 0114709 │    862 │\n│       2 │ Jumanji (1995)         │ Adventure|Children|Fa… │ 0113497 │   8844 │\n│       3 │ Grumpier Old Men (199… │ Comedy|Romance         │ 0113228 │  15602 │\n│       4 │ Waiting to Exhale (19… │ Comedy|Drama|Romance   │ 0114885 │  31357 │\n│       5 │ Father of the Bride P… │ Comedy                 │ 0113041 │  11862 │\n└─────────┴────────────────────────┴────────────────────────┴─────────┴────────┘\n\n\n\nlimit\nexpr.types.relations.Table.limit(self, n, offset=0)\nSelect n rows from self starting at offset.\n!!! note “The result set is not deterministic without a call to [order_by][ibis.expr.types.relations.Table.order_by].”\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nNumber of rows to include\nrequired\n\n\noffset\nint\nNumber of rows to skip first\n0\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe first n rows of self starting at offset\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [1, 1, 2], \"b\": [\"c\", \"a\", \"a\"]})\n&gt;&gt;&gt; t\n┏━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ b      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ c      │\n│     1 │ a      │\n│     2 │ a      │\n└───────┴────────┘\n&gt;&gt;&gt; t.limit(2)\n┏━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ b      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ c      │\n│     1 │ a      │\n└───────┴────────┘\n\n\nSee Also\n[Table.order_by][ibis.expr.types.relations.Table.order_by]\n\n\n\nmutate\nexpr.types.relations.Table.mutate(self, exprs=None, **mutations)\nAdd columns to a table expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexprs\ntyping.Sequence[ibis.Expr] | None\nList of named expressions to add as columns\nNone\n\n\nmutations\nibis.Value\nNamed expressions using keyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression with additional columns\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch().select(\"species\", \"year\", \"bill_length_mm\")\n&gt;&gt;&gt; t\n┏━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ species ┃ year  ┃ bill_length_mm ┃\n┡━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string  │ int64 │ float64        │\n├─────────┼───────┼────────────────┤\n│ Adelie  │  2007 │           39.1 │\n│ Adelie  │  2007 │           39.5 │\n│ Adelie  │  2007 │           40.3 │\n│ Adelie  │  2007 │            nan │\n│ Adelie  │  2007 │           36.7 │\n│ Adelie  │  2007 │           39.3 │\n│ Adelie  │  2007 │           38.9 │\n│ Adelie  │  2007 │           39.2 │\n│ Adelie  │  2007 │           34.1 │\n│ Adelie  │  2007 │           42.0 │\n│ …       │     … │              … │\n└─────────┴───────┴────────────────┘\nAdd a new column from a per-element expression\n&gt;&gt;&gt; t.mutate(next_year=_.year + 1).head()\n┏━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ species ┃ year  ┃ bill_length_mm ┃ next_year ┃\n┡━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string  │ int64 │ float64        │ int64     │\n├─────────┼───────┼────────────────┼───────────┤\n│ Adelie  │  2007 │           39.1 │      2008 │\n│ Adelie  │  2007 │           39.5 │      2008 │\n│ Adelie  │  2007 │           40.3 │      2008 │\n│ Adelie  │  2007 │            nan │      2008 │\n│ Adelie  │  2007 │           36.7 │      2008 │\n└─────────┴───────┴────────────────┴───────────┘\nAdd a new column based on an aggregation. Note the automatic broadcasting.\n&gt;&gt;&gt; t.select(\"species\", bill_demean=_.bill_length_mm - _.bill_length_mm.mean()).head()\n┏━━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ species ┃ bill_demean ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━┩\n│ string  │ float64     │\n├─────────┼─────────────┤\n│ Adelie  │    -4.82193 │\n│ Adelie  │    -4.42193 │\n│ Adelie  │    -3.62193 │\n│ Adelie  │         nan │\n│ Adelie  │    -7.22193 │\n└─────────┴─────────────┘\nMutate across multiple columns\n&gt;&gt;&gt; t.mutate(s.across(s.numeric() & ~s.c(\"year\"), _ - _.mean())).head()\n┏━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ species ┃ year  ┃ bill_length_mm ┃\n┡━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string  │ int64 │ float64        │\n├─────────┼───────┼────────────────┤\n│ Adelie  │  2007 │       -4.82193 │\n│ Adelie  │  2007 │       -4.42193 │\n│ Adelie  │  2007 │       -3.62193 │\n│ Adelie  │  2007 │            nan │\n│ Adelie  │  2007 │       -7.22193 │\n└─────────┴───────┴────────────────┘\n\n\n\norder_by\nexpr.types.relations.Table.order_by(self, by)\nSort a table by one or more expressions.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nby\nstr | ibis.Column | tuple[str | ibis.Column, bool] | typing.Sequence[str] | typing.Sequence[ibis.Column] | typing.Sequence[tuple[str | ibis.Column, bool]] | None\nExpressions to sort the table by.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nSorted table\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [1, 2, 3], \"b\": [\"c\", \"b\", \"a\"], \"c\": [4, 6, 5]})\n&gt;&gt;&gt; t\n┏━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ a     ┃ b      ┃ c     ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ int64 │ string │ int64 │\n├───────┼────────┼───────┤\n│     1 │ c      │     4 │\n│     2 │ b      │     6 │\n│     3 │ a      │     5 │\n└───────┴────────┴───────┘\n&gt;&gt;&gt; t.order_by(\"b\")\n┏━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ a     ┃ b      ┃ c     ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ int64 │ string │ int64 │\n├───────┼────────┼───────┤\n│     3 │ a      │     5 │\n│     2 │ b      │     6 │\n│     1 │ c      │     4 │\n└───────┴────────┴───────┘\n&gt;&gt;&gt; t.order_by(ibis.desc(\"c\"))\n┏━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ a     ┃ b      ┃ c     ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ int64 │ string │ int64 │\n├───────┼────────┼───────┤\n│     2 │ b      │     6 │\n│     3 │ a      │     5 │\n│     1 │ c      │     4 │\n└───────┴────────┴───────┘\n\n\n\npivot_longer\nexpr.types.relations.Table.pivot_longer(self, col, *, names_to='name', names_pattern='(.+)', names_transform=None, values_to='value', values_transform=None)\nTransform a table from wider to longer.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncol\nstr | ibis.Selector\nString column name or selector.\nrequired\n\n\nnames_to\nstr | typing.Iterable[str]\nA string or iterable of strings indicating how to name the new pivoted columns.\n'name'\n\n\nnames_pattern\nstr | re.Pattern\nPattern to use to extract column names from the input. By default the entire column name is extracted.\n'(.+)'\n\n\nnames_transform\ntyping.Callable[[str], ibis.Value] | typing.Mapping[str, typing.Callable[[str], ibis.Value]] | None\nFunction or mapping of a name in names_to to a function to transform a column name to a value.\nNone\n\n\nvalues_to\nstr\nName of the pivoted value column.\n'value'\n\n\nvalues_transform\ntyping.Callable[[ibis.Value], ibis.Value] | ibis.expr.deferred.Deferred | None\nApply a function to the value column. This can be a lambda or deferred expression.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nPivoted table\n\n\n\n\n\nExamples\nBasic usage\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; relig_income = ibis.examples.relig_income_raw.fetch()\n&gt;&gt;&gt; relig_income\n┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━┓\n┃ religion                ┃ &lt;$10k ┃ $10-20k ┃ $20-30k ┃ $30-40k ┃ $40-50k ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━┩\n│ string                  │ int64 │ int64   │ int64   │ int64   │ int64   │ … │\n├─────────────────────────┼───────┼─────────┼─────────┼─────────┼─────────┼───┤\n│ Agnostic                │    27 │      34 │      60 │      81 │      76 │ … │\n│ Atheist                 │    12 │      27 │      37 │      52 │      35 │ … │\n│ Buddhist                │    27 │      21 │      30 │      34 │      33 │ … │\n│ Catholic                │   418 │     617 │     732 │     670 │     638 │ … │\n│ Don’t know/refused      │    15 │      14 │      15 │      11 │      10 │ … │\n│ Evangelical Prot        │   575 │     869 │    1064 │     982 │     881 │ … │\n│ Hindu                   │     1 │       9 │       7 │       9 │      11 │ … │\n│ Historically Black Prot │   228 │     244 │     236 │     238 │     197 │ … │\n│ Jehovah's Witness       │    20 │      27 │      24 │      24 │      21 │ … │\n│ Jewish                  │    19 │      19 │      25 │      25 │      30 │ … │\n│ …                       │     … │       … │       … │       … │       … │ … │\n└─────────────────────────┴───────┴─────────┴─────────┴─────────┴─────────┴───┘\nHere we convert column names not matching the selector for the religion column and convert those names into values\n&gt;&gt;&gt; relig_income.pivot_longer(~s.c(\"religion\"), names_to=\"income\", values_to=\"count\")\n┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┓\n┃ religion ┃ income             ┃ count ┃\n┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━┩\n│ string   │ string             │ int64 │\n├──────────┼────────────────────┼───────┤\n│ Agnostic │ &lt;$10k              │    27 │\n│ Agnostic │ $10-20k            │    34 │\n│ Agnostic │ $20-30k            │    60 │\n│ Agnostic │ $30-40k            │    81 │\n│ Agnostic │ $40-50k            │    76 │\n│ Agnostic │ $50-75k            │   137 │\n│ Agnostic │ $75-100k           │   122 │\n│ Agnostic │ $100-150k          │   109 │\n│ Agnostic │ &gt;150k              │    84 │\n│ Agnostic │ Don't know/refused │    96 │\n│ …        │ …                  │     … │\n└──────────┴────────────────────┴───────┘\nSimilarly for a different example dataset, we convert names to values but using a different selector and the default values_to value.\n&gt;&gt;&gt; world_bank_pop = ibis.examples.world_bank_pop_raw.fetch()\n&gt;&gt;&gt; world_bank_pop.head()\n┏━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━┓\n┃ country ┃ indicator   ┃ 2000         ┃ 2001         ┃ 2002         ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string      │ float64      │ float64      │ float64      │ … │\n├─────────┼─────────────┼──────────────┼──────────────┼──────────────┼───┤\n│ ABW     │ SP.URB.TOTL │ 4.244400e+04 │ 4.304800e+04 │ 4.367000e+04 │ … │\n│ ABW     │ SP.URB.GROW │ 1.182632e+00 │ 1.413021e+00 │ 1.434560e+00 │ … │\n│ ABW     │ SP.POP.TOTL │ 9.085300e+04 │ 9.289800e+04 │ 9.499200e+04 │ … │\n│ ABW     │ SP.POP.GROW │ 2.055027e+00 │ 2.225930e+00 │ 2.229056e+00 │ … │\n│ AFG     │ SP.URB.TOTL │ 4.436299e+06 │ 4.648055e+06 │ 4.892951e+06 │ … │\n└─────────┴─────────────┴──────────────┴──────────────┴──────────────┴───┘\n&gt;&gt;&gt; world_bank_pop.pivot_longer(s.matches(r\"\\d{4}\"), names_to=\"year\").head()\n┏━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┓\n┃ country ┃ indicator   ┃ year   ┃ value   ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━┩\n│ string  │ string      │ string │ float64 │\n├─────────┼─────────────┼────────┼─────────┤\n│ ABW     │ SP.URB.TOTL │ 2000   │ 42444.0 │\n│ ABW     │ SP.URB.TOTL │ 2001   │ 43048.0 │\n│ ABW     │ SP.URB.TOTL │ 2002   │ 43670.0 │\n│ ABW     │ SP.URB.TOTL │ 2003   │ 44246.0 │\n│ ABW     │ SP.URB.TOTL │ 2004   │ 44669.0 │\n└─────────┴─────────────┴────────┴─────────┘\npivot_longer has some preprocessing capabiltiies like stripping a prefix and applying a function to column names\n&gt;&gt;&gt; billboard = ibis.examples.billboard.fetch()\n&gt;&gt;&gt; billboard\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━┓\n┃ artist         ┃ track                   ┃ date_entered ┃ wk1   ┃ wk2   ┃ … ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━┩\n│ string         │ string                  │ date         │ int64 │ int64 │ … │\n├────────────────┼─────────────────────────┼──────────────┼───────┼───────┼───┤\n│ 2 Pac          │ Baby Don't Cry (Keep... │ 2000-02-26   │    87 │    82 │ … │\n│ 2Ge+her        │ The Hardest Part Of ... │ 2000-09-02   │    91 │    87 │ … │\n│ 3 Doors Down   │ Kryptonite              │ 2000-04-08   │    81 │    70 │ … │\n│ 3 Doors Down   │ Loser                   │ 2000-10-21   │    76 │    76 │ … │\n│ 504 Boyz       │ Wobble Wobble           │ 2000-04-15   │    57 │    34 │ … │\n│ 98^0           │ Give Me Just One Nig... │ 2000-08-19   │    51 │    39 │ … │\n│ A*Teens        │ Dancing Queen           │ 2000-07-08   │    97 │    97 │ … │\n│ Aaliyah        │ I Don't Wanna           │ 2000-01-29   │    84 │    62 │ … │\n│ Aaliyah        │ Try Again               │ 2000-03-18   │    59 │    53 │ … │\n│ Adams, Yolanda │ Open My Heart           │ 2000-08-26   │    76 │    76 │ … │\n│ …              │ …                       │ …            │     … │     … │ … │\n└────────────────┴─────────────────────────┴──────────────┴───────┴───────┴───┘\n&gt;&gt;&gt; billboard.pivot_longer(\n...     s.startswith(\"wk\"),\n...     names_to=\"week\",\n...     names_pattern=r\"wk(.+)\",\n...     names_transform=int,\n...     values_to=\"rank\",\n...     values_transform=_.cast(\"int\"),\n... ).dropna(\"rank\")\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━┓\n┃ artist  ┃ track                   ┃ date_entered ┃ week ┃ rank  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━┩\n│ string  │ string                  │ date         │ int8 │ int64 │\n├─────────┼─────────────────────────┼──────────────┼──────┼───────┤\n│ 2 Pac   │ Baby Don't Cry (Keep... │ 2000-02-26   │    1 │    87 │\n│ 2 Pac   │ Baby Don't Cry (Keep... │ 2000-02-26   │    2 │    82 │\n│ 2 Pac   │ Baby Don't Cry (Keep... │ 2000-02-26   │    3 │    72 │\n│ 2 Pac   │ Baby Don't Cry (Keep... │ 2000-02-26   │    4 │    77 │\n│ 2 Pac   │ Baby Don't Cry (Keep... │ 2000-02-26   │    5 │    87 │\n│ 2 Pac   │ Baby Don't Cry (Keep... │ 2000-02-26   │    6 │    94 │\n│ 2 Pac   │ Baby Don't Cry (Keep... │ 2000-02-26   │    7 │    99 │\n│ 2Ge+her │ The Hardest Part Of ... │ 2000-09-02   │    1 │    91 │\n│ 2Ge+her │ The Hardest Part Of ... │ 2000-09-02   │    2 │    87 │\n│ 2Ge+her │ The Hardest Part Of ... │ 2000-09-02   │    3 │    92 │\n│ …       │ …                       │ …            │    … │     … │\n└─────────┴─────────────────────────┴──────────────┴──────┴───────┘\nYou can use regular expression capture groups to extract multiple variables stored in column names\n&gt;&gt;&gt; who = ibis.examples.who.fetch()\n&gt;&gt;&gt; who\n┏━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━┓\n┃ country     ┃ iso2   ┃ iso3   ┃ year  ┃ new_sp_m014 ┃ new_sp_m1524 ┃ … ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━┩\n│ string      │ string │ string │ int64 │ int64       │ int64        │ … │\n├─────────────┼────────┼────────┼───────┼─────────────┼──────────────┼───┤\n│ Afghanistan │ AF     │ AFG    │  1980 │        NULL │         NULL │ … │\n│ Afghanistan │ AF     │ AFG    │  1981 │        NULL │         NULL │ … │\n│ Afghanistan │ AF     │ AFG    │  1982 │        NULL │         NULL │ … │\n│ Afghanistan │ AF     │ AFG    │  1983 │        NULL │         NULL │ … │\n│ Afghanistan │ AF     │ AFG    │  1984 │        NULL │         NULL │ … │\n│ Afghanistan │ AF     │ AFG    │  1985 │        NULL │         NULL │ … │\n│ Afghanistan │ AF     │ AFG    │  1986 │        NULL │         NULL │ … │\n│ Afghanistan │ AF     │ AFG    │  1987 │        NULL │         NULL │ … │\n│ Afghanistan │ AF     │ AFG    │  1988 │        NULL │         NULL │ … │\n│ Afghanistan │ AF     │ AFG    │  1989 │        NULL │         NULL │ … │\n│ …           │ …      │ …      │     … │           … │            … │ … │\n└─────────────┴────────┴────────┴───────┴─────────────┴──────────────┴───┘\n&gt;&gt;&gt; len(who.columns)\n60\n&gt;&gt;&gt; who.pivot_longer(\n...     s.r[\"new_sp_m014\":\"newrel_f65\"],\n...     names_to=[\"diagnosis\", \"gender\", \"age\"],\n...     names_pattern=\"new_?(.*)_(.)(.*)\",\n...     values_to=\"count\",\n... )\n┏━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ country     ┃ iso2   ┃ iso3   ┃ year  ┃ diagnosis ┃ gender ┃ age    ┃ count ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string      │ string │ string │ int64 │ string    │ string │ string │ int64 │\n├─────────────┼────────┼────────┼───────┼───────────┼────────┼────────┼───────┤\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ m      │ 014    │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ m      │ 1524   │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ m      │ 2534   │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ m      │ 3544   │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ m      │ 4554   │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ m      │ 5564   │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ m      │ 65     │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ f      │ 014    │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ f      │ 1524   │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ f      │ 2534   │  NULL │\n│ …           │ …      │ …      │     … │ …         │ …      │ …      │     … │\n└─────────────┴────────┴────────┴───────┴───────────┴────────┴────────┴───────┘\nnames_transform is flexible, and can be:\n1. A mapping of one or more names in `names_to` to callable\n2. A callable that will be applied to every name\nLet’s recode gender and age to numeric values using a mapping\n&gt;&gt;&gt; who.pivot_longer(\n...     s.r[\"new_sp_m014\":\"newrel_f65\"],\n...     names_to=[\"diagnosis\", \"gender\", \"age\"],\n...     names_pattern=\"new_?(.*)_(.)(.*)\",\n...     names_transform=dict(\n...         gender={\"m\": 1, \"f\": 2}.get,\n...         age=dict(zip([\"014\", \"1524\", \"2534\", \"3544\", \"4554\", \"5564\", \"65\"], range(7))).get,\n...     ),\n...     values_to=\"count\",\n... )\n┏━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━┳━━━━━━━┓\n┃ country     ┃ iso2   ┃ iso3   ┃ year  ┃ diagnosis ┃ gender ┃ age  ┃ count ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━╇━━━━━━━┩\n│ string      │ string │ string │ int64 │ string    │ int8   │ int8 │ int64 │\n├─────────────┼────────┼────────┼───────┼───────────┼────────┼──────┼───────┤\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      1 │    0 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      1 │    1 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      1 │    2 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      1 │    3 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      1 │    4 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      1 │    5 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      1 │    6 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      2 │    0 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      2 │    1 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      2 │    2 │  NULL │\n│ …           │ …      │ …      │     … │ …         │      … │    … │     … │\n└─────────────┴────────┴────────┴───────┴───────────┴────────┴──────┴───────┘\nThe number of match groups in names_pattern must match the length of names_to\n&gt;&gt;&gt; who.pivot_longer(\n...     s.r[\"new_sp_m014\":\"newrel_f65\"],\n...     names_to=[\"diagnosis\", \"gender\", \"age\"],\n...     names_pattern=\"new_?(.*)_.(.*)\",\n... )\nTraceback (most recent call last):\n  ...\nibis.common.exceptions.IbisInputError: Number of match groups in `names_pattern` ...\nnames_transform must be a mapping or callable\n&gt;&gt;&gt; who.pivot_longer(s.r[\"new_sp_m014\":\"newrel_f65\"], names_transform=\"upper\")\nTraceback (most recent call last):\n  ...\nibis.common.exceptions.IbisTypeError: ... Got &lt;class 'str'&gt;\n\n\n\npivot_wider\nexpr.types.relations.Table.pivot_wider(self, *, id_cols=None, names_from='name', names_prefix='', names_sep='_', names_sort=False, names=None, values_from='value', values_fill=None, values_agg='arbitrary')\nPivot a table to a wider format.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nid_cols\nibis.Selector | None\nA set of columns that uniquely identify each observation.\nNone\n\n\nnames_from\nstr | typing.Iterable[str] | ibis.Selector\nAn argument describing which column or columns to use to get the name of the output columns.\n'name'\n\n\nnames_prefix\nstr\nString added to the start of every column name.\n''\n\n\nnames_sep\nstr\nIf names_from or values_from contains multiple columns, this argument will be used to join their values together into a single string to use as a column name.\n'_'\n\n\nnames_sort\nbool\nIf [True][True] columns are sorted. If [False][False] column names are ordered by appearance.\nFalse\n\n\nnames\ntyping.Iterable[str] | None\nAn explicit sequence of values to look for in columns matching names_from. * When this value is None, the values will be computed from names_from. * When this value is not None, each element’s length must match the length of names_from. See examples below for more detail.\nNone\n\n\nvalues_from\nstr | typing.Iterable[str] | ibis.Selector\nAn argument describing which column or columns to get the cell values from.\n'value'\n\n\nvalues_fill\nint | float | str | ibis.Scalar | None\nA scalar value that specifies what each value should be filled with when missing.\nNone\n\n\nvalues_agg\nstr | typing.Callable[[ibis.Value], ibis.Scalar] | ibis.expr.deferred.Deferred\nA function applied to the value in each cell in the output.\n'arbitrary'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nWider pivoted table\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\nBasic usage\n&gt;&gt;&gt; fish_encounters = ibis.examples.fish_encounters.fetch()\n&gt;&gt;&gt; fish_encounters\n┏━━━━━━━┳━━━━━━━━━┳━━━━━━━┓\n┃ fish  ┃ station ┃ seen  ┃\n┡━━━━━━━╇━━━━━━━━━╇━━━━━━━┩\n│ int64 │ string  │ int64 │\n├───────┼─────────┼───────┤\n│  4842 │ Release │     1 │\n│  4842 │ I80_1   │     1 │\n│  4842 │ Lisbon  │     1 │\n│  4842 │ Rstr    │     1 │\n│  4842 │ Base_TD │     1 │\n│  4842 │ BCE     │     1 │\n│  4842 │ BCW     │     1 │\n│  4842 │ BCE2    │     1 │\n│  4842 │ BCW2    │     1 │\n│  4842 │ MAE     │     1 │\n│     … │ …       │     … │\n└───────┴─────────┴───────┘\n&gt;&gt;&gt; fish_encounters.pivot_wider(names_from=\"station\", values_from=\"seen\")\n┏━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━┓\n┃ fish  ┃ Release ┃ I80_1 ┃ Lisbon ┃ Rstr  ┃ Base_TD ┃ BCE   ┃ BCW   ┃ … ┃\n┡━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━┩\n│ int64 │ int64   │ int64 │ int64  │ int64 │ int64   │ int64 │ int64 │ … │\n├───────┼─────────┼───────┼────────┼───────┼─────────┼───────┼───────┼───┤\n│  4842 │       1 │     1 │      1 │     1 │       1 │     1 │     1 │ … │\n│  4843 │       1 │     1 │      1 │     1 │       1 │     1 │     1 │ … │\n│  4844 │       1 │     1 │      1 │     1 │       1 │     1 │     1 │ … │\n│  4845 │       1 │     1 │      1 │     1 │       1 │  NULL │  NULL │ … │\n│  4847 │       1 │     1 │      1 │  NULL │    NULL │  NULL │  NULL │ … │\n│  4848 │       1 │     1 │      1 │     1 │    NULL │  NULL │  NULL │ … │\n│  4849 │       1 │     1 │   NULL │  NULL │    NULL │  NULL │  NULL │ … │\n│  4850 │       1 │     1 │   NULL │     1 │       1 │     1 │     1 │ … │\n│  4851 │       1 │     1 │   NULL │  NULL │    NULL │  NULL │  NULL │ … │\n│  4854 │       1 │     1 │   NULL │  NULL │    NULL │  NULL │  NULL │ … │\n│     … │       … │     … │      … │     … │       … │     … │     … │ … │\n└───────┴─────────┴───────┴────────┴───────┴─────────┴───────┴───────┴───┘\nFill missing pivoted values using values_fill\n&gt;&gt;&gt; fish_encounters.pivot_wider(names_from=\"station\", values_from=\"seen\", values_fill=0)\n┏━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━┓\n┃ fish  ┃ Release ┃ I80_1 ┃ Lisbon ┃ Rstr  ┃ Base_TD ┃ BCE   ┃ BCW   ┃ … ┃\n┡━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━┩\n│ int64 │ int64   │ int64 │ int64  │ int64 │ int64   │ int64 │ int64 │ … │\n├───────┼─────────┼───────┼────────┼───────┼─────────┼───────┼───────┼───┤\n│  4842 │       1 │     1 │      1 │     1 │       1 │     1 │     1 │ … │\n│  4843 │       1 │     1 │      1 │     1 │       1 │     1 │     1 │ … │\n│  4844 │       1 │     1 │      1 │     1 │       1 │     1 │     1 │ … │\n│  4845 │       1 │     1 │      1 │     1 │       1 │     0 │     0 │ … │\n│  4847 │       1 │     1 │      1 │     0 │       0 │     0 │     0 │ … │\n│  4848 │       1 │     1 │      1 │     1 │       0 │     0 │     0 │ … │\n│  4849 │       1 │     1 │      0 │     0 │       0 │     0 │     0 │ … │\n│  4850 │       1 │     1 │      0 │     1 │       1 │     1 │     1 │ … │\n│  4851 │       1 │     1 │      0 │     0 │       0 │     0 │     0 │ … │\n│  4854 │       1 │     1 │      0 │     0 │       0 │     0 │     0 │ … │\n│     … │       … │     … │      … │     … │       … │     … │     … │ … │\n└───────┴─────────┴───────┴────────┴───────┴─────────┴───────┴───────┴───┘\nCompute multiple values columns\n&gt;&gt;&gt; us_rent_income = ibis.examples.us_rent_income.fetch()\n&gt;&gt;&gt; us_rent_income\n┏━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━┓\n┃ geoid  ┃ name       ┃ variable ┃ estimate ┃ moe   ┃\n┡━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━┩\n│ string │ string     │ string   │ int64    │ int64 │\n├────────┼────────────┼──────────┼──────────┼───────┤\n│ 01     │ Alabama    │ income   │    24476 │   136 │\n│ 01     │ Alabama    │ rent     │      747 │     3 │\n│ 02     │ Alaska     │ income   │    32940 │   508 │\n│ 02     │ Alaska     │ rent     │     1200 │    13 │\n│ 04     │ Arizona    │ income   │    27517 │   148 │\n│ 04     │ Arizona    │ rent     │      972 │     4 │\n│ 05     │ Arkansas   │ income   │    23789 │   165 │\n│ 05     │ Arkansas   │ rent     │      709 │     5 │\n│ 06     │ California │ income   │    29454 │   109 │\n│ 06     │ California │ rent     │     1358 │     3 │\n│ …      │ …          │ …        │        … │     … │\n└────────┴────────────┴──────────┴──────────┴───────┘\n&gt;&gt;&gt; us_rent_income.pivot_wider(names_from=\"variable\", values_from=[\"estimate\", \"moe\"])\n┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━┓\n┃ geoid  ┃ name                 ┃ estimate_income ┃ moe_income ┃ … ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━┩\n│ string │ string               │ int64           │ int64      │ … │\n├────────┼──────────────────────┼─────────────────┼────────────┼───┤\n│ 01     │ Alabama              │           24476 │        136 │ … │\n│ 02     │ Alaska               │           32940 │        508 │ … │\n│ 04     │ Arizona              │           27517 │        148 │ … │\n│ 05     │ Arkansas             │           23789 │        165 │ … │\n│ 06     │ California           │           29454 │        109 │ … │\n│ 08     │ Colorado             │           32401 │        109 │ … │\n│ 09     │ Connecticut          │           35326 │        195 │ … │\n│ 10     │ Delaware             │           31560 │        247 │ … │\n│ 11     │ District of Columbia │           43198 │        681 │ … │\n│ 12     │ Florida              │           25952 │         70 │ … │\n│ …      │ …                    │               … │          … │ … │\n└────────┴──────────────────────┴─────────────────┴────────────┴───┘\nThe column name separator can be changed using the names_sep parameter\n&gt;&gt;&gt; us_rent_income.pivot_wider(\n...     names_from=\"variable\",\n...     names_sep=\".\",\n...     values_from=s.c(\"estimate\", \"moe\"),\n... )\n┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━┓\n┃ geoid  ┃ name                 ┃ estimate.income ┃ moe.income ┃ … ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━┩\n│ string │ string               │ int64           │ int64      │ … │\n├────────┼──────────────────────┼─────────────────┼────────────┼───┤\n│ 01     │ Alabama              │           24476 │        136 │ … │\n│ 02     │ Alaska               │           32940 │        508 │ … │\n│ 04     │ Arizona              │           27517 │        148 │ … │\n│ 05     │ Arkansas             │           23789 │        165 │ … │\n│ 06     │ California           │           29454 │        109 │ … │\n│ 08     │ Colorado             │           32401 │        109 │ … │\n│ 09     │ Connecticut          │           35326 │        195 │ … │\n│ 10     │ Delaware             │           31560 │        247 │ … │\n│ 11     │ District of Columbia │           43198 │        681 │ … │\n│ 12     │ Florida              │           25952 │         70 │ … │\n│ …      │ …                    │               … │          … │ … │\n└────────┴──────────────────────┴─────────────────┴────────────┴───┘\nSupply an alternative function to summarize values\n&gt;&gt;&gt; warpbreaks = ibis.examples.warpbreaks.fetch().select(\"wool\", \"tension\", \"breaks\")\n&gt;&gt;&gt; warpbreaks\n┏━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┓\n┃ wool   ┃ tension ┃ breaks ┃\n┡━━━━━━━━╇━━━━━━━━━╇━━━━━━━━┩\n│ string │ string  │ int64  │\n├────────┼─────────┼────────┤\n│ A      │ L       │     26 │\n│ A      │ L       │     30 │\n│ A      │ L       │     54 │\n│ A      │ L       │     25 │\n│ A      │ L       │     70 │\n│ A      │ L       │     52 │\n│ A      │ L       │     51 │\n│ A      │ L       │     26 │\n│ A      │ L       │     67 │\n│ A      │ M       │     18 │\n│ …      │ …       │      … │\n└────────┴─────────┴────────┘\n&gt;&gt;&gt; warpbreaks.pivot_wider(names_from=\"wool\", values_from=\"breaks\", values_agg=\"mean\")\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ tension ┃ A         ┃ B         ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string  │ float64   │ float64   │\n├─────────┼───────────┼───────────┤\n│ L       │ 44.555556 │ 28.222222 │\n│ M       │ 24.000000 │ 28.777778 │\n│ H       │ 24.555556 │ 18.777778 │\n└─────────┴───────────┴───────────┘\nPassing Deferred objects to values_agg is supported\n&gt;&gt;&gt; warpbreaks.pivot_wider(\n...     names_from=\"tension\",\n...     values_from=\"breaks\",\n...     values_agg=_.sum(),\n... )\n┏━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃ wool   ┃ L     ┃ M     ┃ H     ┃\n┡━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│ string │ int64 │ int64 │ int64 │\n├────────┼───────┼───────┼───────┤\n│ A      │   401 │   216 │   221 │\n│ B      │   254 │   259 │   169 │\n└────────┴───────┴───────┴───────┘\nUse a custom aggregate function\n&gt;&gt;&gt; warpbreaks.pivot_wider(\n...     names_from=\"wool\",\n...     values_from=\"breaks\",\n...     values_agg=lambda col: col.std() / col.mean(),\n... )\n┏━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n┃ tension ┃ A        ┃ B        ┃\n┡━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n│ string  │ float64  │ float64  │\n├─────────┼──────────┼──────────┤\n│ L       │ 0.406183 │ 0.349325 │\n│ M       │ 0.360844 │ 0.327719 │\n│ H       │ 0.418344 │ 0.260590 │\n└─────────┴──────────┴──────────┘\nGenerate some random data, setting the random seed for reproducibility\n&gt;&gt;&gt; import random\n&gt;&gt;&gt; random.seed(0)\n&gt;&gt;&gt; raw = ibis.memtable(\n...     [\n...         dict(\n...             product=product,\n...             country=country,\n...             year=year,\n...             production=random.random(),\n...         )\n...         for product in \"AB\"\n...         for country in [\"AI\", \"EI\"]\n...         for year in range(2000, 2015)\n...     ]\n... )\n&gt;&gt;&gt; production = raw.filter(\n...     ((_.product == \"A\") & (_.country == \"AI\")) | (_.product == \"B\")\n... )\n&gt;&gt;&gt; production\n┏━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━┓\n┃ product ┃ country ┃ year  ┃ production ┃\n┡━━━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━┩\n│ string  │ string  │ int64 │ float64    │\n├─────────┼─────────┼───────┼────────────┤\n│ B       │ AI      │  2000 │   0.477010 │\n│ B       │ AI      │  2001 │   0.865310 │\n│ B       │ AI      │  2002 │   0.260492 │\n│ B       │ AI      │  2003 │   0.805028 │\n│ B       │ AI      │  2004 │   0.548699 │\n│ B       │ AI      │  2005 │   0.014042 │\n│ B       │ AI      │  2006 │   0.719705 │\n│ B       │ AI      │  2007 │   0.398824 │\n│ B       │ AI      │  2008 │   0.824845 │\n│ B       │ AI      │  2009 │   0.668153 │\n│ …       │ …       │     … │          … │\n└─────────┴─────────┴───────┴────────────┘\nPivoting with multiple name columns\n&gt;&gt;&gt; production.pivot_wider(\n...     names_from=[\"product\", \"country\"],\n...     values_from=\"production\",\n... )\n┏━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n┃ year  ┃ B_AI     ┃ B_EI     ┃ A_AI     ┃\n┡━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n│ int64 │ float64  │ float64  │ float64  │\n├───────┼──────────┼──────────┼──────────┤\n│  2000 │ 0.477010 │ 0.870471 │ 0.844422 │\n│  2001 │ 0.865310 │ 0.191067 │ 0.757954 │\n│  2002 │ 0.260492 │ 0.567511 │ 0.420572 │\n│  2003 │ 0.805028 │ 0.238616 │ 0.258917 │\n│  2004 │ 0.548699 │ 0.967540 │ 0.511275 │\n│  2005 │ 0.014042 │ 0.803179 │ 0.404934 │\n│  2006 │ 0.719705 │ 0.447970 │ 0.783799 │\n│  2007 │ 0.398824 │ 0.080446 │ 0.303313 │\n│  2008 │ 0.824845 │ 0.320055 │ 0.476597 │\n│  2009 │ 0.668153 │ 0.507941 │ 0.583382 │\n│     … │        … │        … │        … │\n└───────┴──────────┴──────────┴──────────┘\nSelect a subset of names. This call incurs no computation when constructing the expression.\n&gt;&gt;&gt; production.pivot_wider(\n...     names_from=[\"product\", \"country\"],\n...     names=[(\"A\", \"AI\"), (\"B\", \"AI\")],\n...     values_from=\"production\",\n... )\n┏━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n┃ year  ┃ A_AI     ┃ B_AI     ┃\n┡━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n│ int64 │ float64  │ float64  │\n├───────┼──────────┼──────────┤\n│  2000 │ 0.844422 │ 0.477010 │\n│  2001 │ 0.757954 │ 0.865310 │\n│  2002 │ 0.420572 │ 0.260492 │\n│  2003 │ 0.258917 │ 0.805028 │\n│  2004 │ 0.511275 │ 0.548699 │\n│  2005 │ 0.404934 │ 0.014042 │\n│  2006 │ 0.783799 │ 0.719705 │\n│  2007 │ 0.303313 │ 0.398824 │\n│  2008 │ 0.476597 │ 0.824845 │\n│  2009 │ 0.583382 │ 0.668153 │\n│     … │        … │        … │\n└───────┴──────────┴──────────┘\nSort the new columns’ names\n&gt;&gt;&gt; production.pivot_wider(\n...     names_from=[\"product\", \"country\"],\n...     values_from=\"production\",\n...     names_sort=True,\n... )\n┏━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n┃ year  ┃ A_AI     ┃ B_AI     ┃ B_EI     ┃\n┡━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n│ int64 │ float64  │ float64  │ float64  │\n├───────┼──────────┼──────────┼──────────┤\n│  2000 │ 0.844422 │ 0.477010 │ 0.870471 │\n│  2001 │ 0.757954 │ 0.865310 │ 0.191067 │\n│  2002 │ 0.420572 │ 0.260492 │ 0.567511 │\n│  2003 │ 0.258917 │ 0.805028 │ 0.238616 │\n│  2004 │ 0.511275 │ 0.548699 │ 0.967540 │\n│  2005 │ 0.404934 │ 0.014042 │ 0.803179 │\n│  2006 │ 0.783799 │ 0.719705 │ 0.447970 │\n│  2007 │ 0.303313 │ 0.398824 │ 0.080446 │\n│  2008 │ 0.476597 │ 0.824845 │ 0.320055 │\n│  2009 │ 0.583382 │ 0.668153 │ 0.507941 │\n│     … │        … │        … │        … │\n└───────┴──────────┴──────────┴──────────┘\n\n\n\nrelabel\nexpr.types.relations.Table.relabel(self, substitutions)\nRename columns in the table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsubstitutions\ntyping.Mapping[str, str] | typing.Callable[[str], str | None] | str | typing.Literal[‘snake_case’, ‘ALL_CAPS’]\nA mapping, function, or format string mapping old to new column names. If a column isn’t in the mapping (or if the callable returns None) it is left with its original name. May also pass a format string to rename all columns, like \"prefix_{name}\". Also accepts the literal string \"snake_case\" or \"ALL_CAPS\" which will relabel all columns to use a snake_case or \"ALL_CAPS\" naming convention.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA relabeled table expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; first3 = s.r[:3]  # first 3 columns\n&gt;&gt;&gt; t = ibis.examples.penguins_raw_raw.fetch().select(first3)\n&gt;&gt;&gt; t\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ studyName ┃ Sample Number ┃ Species                             ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ int64         │ string                              │\n├───────────┼───────────────┼─────────────────────────────────────┤\n│ PAL0708   │             1 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │             2 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │             3 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │             4 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │             5 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │             6 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │             7 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │             8 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │             9 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │            10 │ Adelie Penguin (Pygoscelis adeliae) │\n│ …         │             … │ …                                   │\n└───────────┴───────────────┴─────────────────────────────────────┘\nRelabel column names using a mapping from old name to new name\n&gt;&gt;&gt; t.relabel({\"studyName\": \"study_name\"}).head(1)\n┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ study_name ┃ Sample Number ┃ Species                             ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string     │ int64         │ string                              │\n├────────────┼───────────────┼─────────────────────────────────────┤\n│ PAL0708    │             1 │ Adelie Penguin (Pygoscelis adeliae) │\n└────────────┴───────────────┴─────────────────────────────────────┘\nRelabel column names using a snake_case convention\n&gt;&gt;&gt; t.relabel(\"snake_case\").head(1)\n┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ study_name ┃ sample_number ┃ species                             ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string     │ int64         │ string                              │\n├────────────┼───────────────┼─────────────────────────────────────┤\n│ PAL0708    │             1 │ Adelie Penguin (Pygoscelis adeliae) │\n└────────────┴───────────────┴─────────────────────────────────────┘\nRelabel column names using a ALL_CAPS convention\n&gt;&gt;&gt; t.relabel(\"ALL_CAPS\").head(1)\n┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ STUDY_NAME ┃ SAMPLE_NUMBER ┃ SPECIES                             ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string     │ int64         │ string                              │\n├────────────┼───────────────┼─────────────────────────────────────┤\n│ PAL0708    │             1 │ Adelie Penguin (Pygoscelis adeliae) │\n└────────────┴───────────────┴─────────────────────────────────────┘\nRelabel columns using a format string\n&gt;&gt;&gt; t.relabel(\"p_{name}\").head(1)\n┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ p_studyName ┃ p_Sample Number ┃ p_Species                           ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string      │ int64           │ string                              │\n├─────────────┼─────────────────┼─────────────────────────────────────┤\n│ PAL0708     │               1 │ Adelie Penguin (Pygoscelis adeliae) │\n└─────────────┴─────────────────┴─────────────────────────────────────┘\nRelabel column names using a callable\n&gt;&gt;&gt; t.relabel(str.upper).head(1)\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ STUDYNAME ┃ SAMPLE NUMBER ┃ SPECIES                             ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ int64         │ string                              │\n├───────────┼───────────────┼─────────────────────────────────────┤\n│ PAL0708   │             1 │ Adelie Penguin (Pygoscelis adeliae) │\n└───────────┴───────────────┴─────────────────────────────────────┘\n\n\n\nrowid\nexpr.types.relations.Table.rowid(self)\nA unique integer per row.\n!!! note “This operation is only valid on physical tables”\nAny further meaning behind this expression is backend dependent.\nGenerally this corresponds to some index into the database storage\n(for example, sqlite or duckdb's `rowid`).\nFor a monotonically increasing row number, see ibis.row_number.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerColumn\nAn integer column\n\n\n\n\n\n\nschema\nexpr.types.relations.Table.schema(self)\nReturn the schema for this table.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nSchema\nThe table’s schema.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.schema()\nibis.Schema {\n  species            string\n  island             string\n  bill_length_mm     float64\n  bill_depth_mm      float64\n  flipper_length_mm  int64\n  body_mass_g        int64\n  sex                string\n  year               int64\n}\n\n\n\nselect\nexpr.types.relations.Table.select(self, *exprs, **named_exprs)\nCompute a new table expression using exprs and named_exprs.\nPassing an aggregate function to this method will broadcast the aggregate’s value over the number of rows in the table and automatically constructs a window function expression. See the examples section for more details.\nFor backwards compatibility the keyword argument exprs is reserved and cannot be used to name an expression. This behavior will be removed in v4.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexprs\nibis.Value | str | typing.Iterable[ibis.Value | str]\nColumn expression, string, or list of column expressions and strings.\n()\n\n\nnamed_exprs\nibis.Value | str\nColumn expressions\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ … │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │ … │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │ … │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │ … │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │ … │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │ … │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │ … │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │ … │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │ … │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │ … │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │ … │\n│ …       │ …         │              … │             … │                 … │ … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴───┘\nSimple projection\n&gt;&gt;&gt; t.select(\"island\", \"bill_length_mm\").head()\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ island    ┃ bill_length_mm ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string    │ float64        │\n├───────────┼────────────────┤\n│ Torgersen │           39.1 │\n│ Torgersen │           39.5 │\n│ Torgersen │           40.3 │\n│ Torgersen │            nan │\n│ Torgersen │           36.7 │\n└───────────┴────────────────┘\nProjection by zero-indexed column position\n&gt;&gt;&gt; t.select(0, 4).head()\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ flipper_length_mm ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ string  │ int64             │\n├─────────┼───────────────────┤\n│ Adelie  │               181 │\n│ Adelie  │               186 │\n│ Adelie  │               195 │\n│ Adelie  │              NULL │\n│ Adelie  │               193 │\n└─────────┴───────────────────┘\nProjection with renaming and compute in one call\n&gt;&gt;&gt; t.select(next_year=t.year + 1).head()\n┏━━━━━━━━━━━┓\n┃ next_year ┃\n┡━━━━━━━━━━━┩\n│ int64     │\n├───────────┤\n│      2008 │\n│      2008 │\n│      2008 │\n│      2008 │\n│      2008 │\n└───────────┘\nProjection with aggregation expressions\n&gt;&gt;&gt; t.select(\"island\", bill_mean=t.bill_length_mm.mean()).head()\n┏━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ island    ┃ bill_mean ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string    │ float64   │\n├───────────┼───────────┤\n│ Torgersen │  43.92193 │\n│ Torgersen │  43.92193 │\n│ Torgersen │  43.92193 │\n│ Torgersen │  43.92193 │\n│ Torgersen │  43.92193 │\n└───────────┴───────────┘\nProjection with a selector\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t.select(s.numeric() & ~s.c(\"year\")).head()\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n│ float64        │ float64       │ int64             │ int64       │\n├────────────────┼───────────────┼───────────────────┼─────────────┤\n│           39.1 │          18.7 │               181 │        3750 │\n│           39.5 │          17.4 │               186 │        3800 │\n│           40.3 │          18.0 │               195 │        3250 │\n│            nan │           nan │              NULL │        NULL │\n│           36.7 │          19.3 │               193 │        3450 │\n└────────────────┴───────────────┴───────────────────┴─────────────┘\nProjection + aggregation across multiple columns\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; t.select(s.across(s.numeric() & ~s.c(\"year\"), _.mean())).head()\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n│ float64        │ float64       │ float64           │ float64     │\n├────────────────┼───────────────┼───────────────────┼─────────────┤\n│       43.92193 │      17.15117 │        200.915205 │ 4201.754386 │\n│       43.92193 │      17.15117 │        200.915205 │ 4201.754386 │\n│       43.92193 │      17.15117 │        200.915205 │ 4201.754386 │\n│       43.92193 │      17.15117 │        200.915205 │ 4201.754386 │\n│       43.92193 │      17.15117 │        200.915205 │ 4201.754386 │\n└────────────────┴───────────────┴───────────────────┴─────────────┘\n\n\n\nsql\nexpr.types.relations.Table.sql(self, query, dialect=None)\nRun a SQL query against a table expression.\nSee [Table.alias][ibis.expr.types.relations.Table.alias] for details on using named table expressions in a SQL string.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr\nQuery string\nrequired\n\n\ndialect\nstr | None\nOptional string indicating the dialect of query. The default value of None will use the backend’s native dialect.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nAn opaque table expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch(table_name=\"penguins\")\n&gt;&gt;&gt; expr = t.sql(\"SELECT island, mean(bill_length_mm) FROM penguins GROUP BY 1 ORDER BY 2 DESC\")\n&gt;&gt;&gt; expr\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n┃ island    ┃ mean(bill_length_mm) ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ float64              │\n├───────────┼──────────────────────┤\n│ Biscoe    │            45.257485 │\n│ Dream     │            44.167742 │\n│ Torgersen │            38.950980 │\n└───────────┴──────────────────────┘\n\n\n\nto_array\nexpr.types.relations.Table.to_array(self)\nView a single column table as an array.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nA single column view of a table\n\n\n\n\n\n\nto_pandas\nexpr.types.relations.Table.to_pandas(self, **kwargs)\nConvert a table expression to a pandas DataFrame.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkwargs\n\nSame as keyword arguments to [execute][ibis.expr.types.core.Expr.execute]\n{}\n\n\n\n\n\n\ntry_cast\nexpr.types.relations.Table.try_cast(self, schema)\nCast the columns of a table.\nIf the cast fails for a row, the value is returned as NULL or NaN depending on backend behavior.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nschema\nSupportsSchema\nMapping, schema or iterable of pairs to use for casting\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nCasted table\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [\"1\", \"2\", \"3\"], \"b\": [\"2.2\", \"3.3\", \"book\"]})\n&gt;&gt;&gt; t.try_cast({\"a\": \"int\", \"b\": \"float\"})\n┏━━━━━━━┳━━━━━━━━━┓\n┃ a     ┃ b       ┃\n┡━━━━━━━╇━━━━━━━━━┩\n│ int64 │ float64 │\n├───────┼─────────┤\n│     1 │     2.2 │\n│     2 │     3.3 │\n│     3 │     nan │\n└───────┴─────────┘\n\n\n\nunion\nexpr.types.relations.Table.union(self, table, *rest, distinct=False)\nCompute the set union of multiple table expressions.\nThe input tables must have identical schemas.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable\nTable\nA table expression\nrequired\n\n\n*rest\nTable\nAdditional table expressions\n()\n\n\ndistinct\nbool\nOnly return distinct rows\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA new table containing the union of all input tables.\n\n\n\n\n\nSee Also\n[ibis.union][ibis.union]\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t1 = ibis.memtable({\"a\": [1, 2]})\n&gt;&gt;&gt; t1\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     2 │\n└───────┘\n&gt;&gt;&gt; t2 = ibis.memtable({\"a\": [2, 3]})\n&gt;&gt;&gt; t2\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     2 │\n│     3 │\n└───────┘\n&gt;&gt;&gt; t1.union(t2)  # union all by default\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     2 │\n│     2 │\n│     3 │\n└───────┘\n&gt;&gt;&gt; t1.union(t2, distinct=True).order_by(\"a\")\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     2 │\n│     3 │\n└───────┘\n\n\n\nunpack\nexpr.types.relations.Table.unpack(self, *columns)\nProject the struct fields of each of columns into self.\nExisting fields are retained in the projection.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumns\nstr\nString column names to project into self.\n()\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe child table with struct fields of each of columns projected.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; lines = '''\n...     {\"name\": \"a\", \"pos\": {\"lat\": 10.1, \"lon\": 30.3}}\n...     {\"name\": \"b\", \"pos\": {\"lat\": 10.2, \"lon\": 30.2}}\n...     {\"name\": \"c\", \"pos\": {\"lat\": 10.3, \"lon\": 30.1}}\n... '''\n&gt;&gt;&gt; with open(\"/tmp/lines.json\", \"w\") as f:\n...     _ = f.write(lines)\n&gt;&gt;&gt; t = ibis.read_json(\"/tmp/lines.json\")\n&gt;&gt;&gt; t\n┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ name   ┃ pos                                ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string │ struct&lt;lat: float64, lon: float64&gt; │\n├────────┼────────────────────────────────────┤\n│ a      │ {'lat': 10.1, 'lon': 30.3}         │\n│ b      │ {'lat': 10.2, 'lon': 30.2}         │\n│ c      │ {'lat': 10.3, 'lon': 30.1}         │\n└────────┴────────────────────────────────────┘\n&gt;&gt;&gt; t.unpack(\"pos\")\n┏━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┓\n┃ name   ┃ lat     ┃ lon     ┃\n┡━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━┩\n│ string │ float64 │ float64 │\n├────────┼─────────┼─────────┤\n│ a      │    10.1 │    30.3 │\n│ b      │    10.2 │    30.2 │\n│ c      │    10.3 │    30.1 │\n└────────┴─────────┴─────────┘\n\n\nSee Also\n[StructValue.lift][ibis.expr.types.structs.StructValue.lift]\n\n\n\nview\nexpr.types.relations.Table.view(self)\nCreate a new table expression distinct from the current one.\nUse this API for any self-referencing operations like a self-join.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression"
  },
  {
    "objectID": "reference/expression-tables.html#methods-1",
    "href": "reference/expression-tables.html#methods-1",
    "title": "Table Expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\naggregate\nCompute aggregates over a group by.\n\n\ncount\nComputing the number of rows per group.\n\n\nhaving\nAdd a post-aggregation result filter expr.\n\n\nmutate\nReturn a table projection with window functions applied.\n\n\norder_by\nSort a grouped table expression by expr.\n\n\nover\nApply a window over the input expressions.\n\n\nselect\nProject new columns out of the grouped table.\n\n\n\n\naggregate\nexpr.types.relations.GroupedTable.aggregate(self, metrics=None, **kwds)\nCompute aggregates over a group by.\n\n\ncount\nexpr.types.relations.GroupedTable.count(self)\nComputing the number of rows per group.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe aggregated table\n\n\n\n\n\n\nhaving\nexpr.types.relations.GroupedTable.having(self, expr)\nAdd a post-aggregation result filter expr.\n!!! warning “Expressions like x is None return bool and will not generate a SQL comparison to NULL”\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.BooleanScalar\nAn expression that filters based on an aggregate value.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGroupedTable\nA grouped table expression\n\n\n\n\n\n\nmutate\nexpr.types.relations.GroupedTable.mutate(self, *exprs, **kwexprs)\nReturn a table projection with window functions applied.\nAny arguments can be functions.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexprs\nibis.Value | typing.Sequence[ibis.Value]\nList of expressions\n()\n\n\nkwexprs\nibis.Value\nExpressions\n{}\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; t = ibis.table([\n...     ('foo', 'string'),\n...     ('bar', 'string'),\n...     ('baz', 'double'),\n... ], name='t')\n&gt;&gt;&gt; t\nUnboundTable: t\n  foo string\n  bar string\n  baz float64\n&gt;&gt;&gt; expr = (t.group_by('foo')\n...          .order_by(ibis.desc('bar'))\n...          .mutate(qux=lambda x: x.baz.lag(), qux2=t.baz.lead()))\n&gt;&gt;&gt; print(expr)\nr0 := UnboundTable: t\n  foo string\n  bar string\n  baz float64\nSelection[r0]\n  selections:\n    r0\n    qux:  WindowFunction(...)\n    qux2: WindowFunction(...)\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA table expression with window functions applied\n\n\n\n\n\n\norder_by\nexpr.types.relations.GroupedTable.order_by(self, expr)\n\nNotes\nThis API call is ignored in aggregations.\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.Value | typing.Iterable[ibis.Value]\nExpressions to order the results by\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGroupedTable\nA sorted grouped GroupedTable\n\n\n\n\n\n\nover\nexpr.types.relations.GroupedTable.over(self, window=None, *, rows=None, range=None, group_by=None, order_by=None)\nApply a window over the input expressions.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwindow\n\nWindow to add to the input\nNone\n\n\nrows\n\nWhether to use the ROWS window clause\nNone\n\n\nrange\n\nWhether to use the RANGE window clause\nNone\n\n\ngroup_by\n\nGrouping key\nNone\n\n\norder_by\n\nOrdering key\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGroupedTable\nA new grouped table expression\n\n\n\n\n\n\nselect\nexpr.types.relations.GroupedTable.select(self, *exprs, **kwexprs)\n\nSee Also\n[GroupedTable.mutate][ibis.expr.types.groupby.GroupedTable.mutate]"
  },
  {
    "objectID": "reference/expr.datatypes.core.html",
    "href": "reference/expr.datatypes.core.html",
    "title": "expr.datatypes.core",
    "section": "",
    "text": "expr.datatypes.core\n\n\n\n\n\nName\nDescription\n\n\n\n\nArray\nArray values.\n\n\nBinary\nA type representing a sequence of bytes.\n\n\nBoolean\n[True][True] or [False][False] values.\n\n\nBounds\nThe lower and upper bound of a fixed-size value.\n\n\nDataType\nBase class for all data types.\n\n\nDate\nDate values.\n\n\nDecimal\nFixed-precision decimal values.\n\n\nFloat16\n16-bit floating point numbers.\n\n\nFloat32\n32-bit floating point numbers.\n\n\nFloat64\n64-bit floating point numbers.\n\n\nFloating\nFloating point values.\n\n\nGeoSpatial\nGeospatial values.\n\n\nINET\nIP addresses.\n\n\nInt16\nSigned 16-bit integers.\n\n\nInt32\nSigned 32-bit integers.\n\n\nInt64\nSigned 64-bit integers.\n\n\nInt8\nSigned 8-bit integers.\n\n\nInteger\nInteger values.\n\n\nInterval\nInterval values.\n\n\nJSON\nJSON values.\n\n\nLineString\nA sequence of 2 or more points.\n\n\nMACADDR\nMedia Access Control (MAC) address of a network interface.\n\n\nMap\nAssociative array values.\n\n\nMultiLineString\nA set of one or more line strings.\n\n\nMultiPoint\nA set of one or more points.\n\n\nMultiPolygon\nA set of one or more polygons.\n\n\nNull\nNull values.\n\n\nNumeric\nNumeric types.\n\n\nParametric\nTypes that can be parameterized.\n\n\nPoint\nA point described by two coordinates.\n\n\nPolygon\nA set of one or more closed line strings.\n\n\nPrimitive\nValues with known size.\n\n\nSignedInteger\nSigned integer values.\n\n\nString\nA type representing a string.\n\n\nStruct\nStructured values.\n\n\nTemporal\nData types related to time.\n\n\nTime\nTime values.\n\n\nTimestamp\nTimestamp values.\n\n\nUInt16\nUnsigned 16-bit integers.\n\n\nUInt32\nUnsigned 32-bit integers.\n\n\nUInt64\nUnsigned 64-bit integers.\n\n\nUInt8\nUnsigned 8-bit integers.\n\n\nUUID\nA 128-bit number used to identify information in computer systems.\n\n\nUnknown\nAn unknown type.\n\n\nUnsignedInteger\nUnsigned integer values.\n\n\nVariadic\nValues with unknown size.\n\n\n\n\n\nexpr.datatypes.core.Array()\nArray values.\n\n\n\nexpr.datatypes.core.Binary()\n\n\nSome databases treat strings and blobs of equally, and some do not.\nFor example, Impala doesn’t make a distinction between string and binary types but PostgreSQL has a TEXT type and a BYTEA type which are distinct types that have different behavior.\n\n\n\n\nexpr.datatypes.core.Boolean()\n[True][True] or [False][False] values.\n\n\n\nexpr.datatypes.core.Bounds()\nThe lower and upper bound of a fixed-size value.\n\n\n\nexpr.datatypes.core.DataType()\nBase class for all data types.\n[DataType][ibis.expr.datatypes.DataType] instances are immutable.\n\n\n\n\n\nName\nDescription\n\n\n\n\nname\nReturn the name of the data type.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_dask\nReturn the equivalent ibis datatype.\n\n\nfrom_numpy\nReturn the equivalent ibis datatype.\n\n\nfrom_pandas\nReturn the equivalent ibis datatype.\n\n\nfrom_pyarrow\nReturn the equivalent ibis datatype.\n\n\nto_dask\nReturn the equivalent dask datatype.\n\n\nto_numpy\nReturn the equivalent numpy datatype.\n\n\nto_pandas\nReturn the equivalent pandas datatype.\n\n\nto_pyarrow\nReturn the equivalent pyarrow datatype.\n\n\n\n\n\nexpr.datatypes.core.DataType.from_dask(cls, dask_type, nullable=True)\nReturn the equivalent ibis datatype.\n\n\n\nexpr.datatypes.core.DataType.from_numpy(cls, numpy_type, nullable=True)\nReturn the equivalent ibis datatype.\n\n\n\nexpr.datatypes.core.DataType.from_pandas(cls, pandas_type, nullable=True)\nReturn the equivalent ibis datatype.\n\n\n\nexpr.datatypes.core.DataType.from_pyarrow(cls, arrow_type, nullable=True)\nReturn the equivalent ibis datatype.\n\n\n\nexpr.datatypes.core.DataType.to_dask(self)\nReturn the equivalent dask datatype.\n\n\n\nexpr.datatypes.core.DataType.to_numpy(self)\nReturn the equivalent numpy datatype.\n\n\n\nexpr.datatypes.core.DataType.to_pandas(self)\nReturn the equivalent pandas datatype.\n\n\n\nexpr.datatypes.core.DataType.to_pyarrow(self)\nReturn the equivalent pyarrow datatype.\n\n\n\n\n\nexpr.datatypes.core.Date()\nDate values.\n\n\n\nexpr.datatypes.core.Decimal(self, precision=None, scale=None, **kwargs)\nFixed-precision decimal values.\n\n\n\n\n\nName\nDescription\n\n\n\n\nlargest\nReturn the largest type of decimal.\n\n\nprecision\nThe number of decimal places values of this type can hold.\n\n\nscale\nThe number of values after the decimal point.\n\n\n\n\n\n\n\nexpr.datatypes.core.Float16()\n16-bit floating point numbers.\n\n\n\nexpr.datatypes.core.Float32()\n32-bit floating point numbers.\n\n\n\nexpr.datatypes.core.Float64()\n64-bit floating point numbers.\n\n\n\nexpr.datatypes.core.Floating()\nFloating point values.\n\n\n\n\n\nName\nDescription\n\n\n\n\nlargest\nReturn the largest type of floating point values.\n\n\n\n\n\n\n\nexpr.datatypes.core.GeoSpatial()\nGeospatial values.\n\n\n\n\n\nName\nDescription\n\n\n\n\ngeotype\nThe specific geospatial type.\n\n\nsrid\nThe spatial reference identifier.\n\n\n\n\n\n\n\nexpr.datatypes.core.INET()\nIP addresses.\n\n\n\nexpr.datatypes.core.Int16()\nSigned 16-bit integers.\n\n\n\nexpr.datatypes.core.Int32()\nSigned 32-bit integers.\n\n\n\nexpr.datatypes.core.Int64()\nSigned 64-bit integers.\n\n\n\nexpr.datatypes.core.Int8()\nSigned 8-bit integers.\n\n\n\nexpr.datatypes.core.Integer()\nInteger values.\n\n\n\n\n\nName\nDescription\n\n\n\n\nnbytes\nReturn the number of bytes used to store values of this type.\n\n\n\n\n\n\n\nexpr.datatypes.core.Interval()\nInterval values.\n\n\n\n\n\nName\nDescription\n\n\n\n\nresolution\nThe interval unit’s name.\n\n\nunit\nThe time unit of the interval.\n\n\n\n\n\n\n\nexpr.datatypes.core.JSON()\nJSON values.\n\n\n\nexpr.datatypes.core.LineString()\nA sequence of 2 or more points.\n\n\n\nexpr.datatypes.core.MACADDR()\nMedia Access Control (MAC) address of a network interface.\n\n\n\nexpr.datatypes.core.Map()\nAssociative array values.\n\n\n\nexpr.datatypes.core.MultiLineString()\nA set of one or more line strings.\n\n\n\nexpr.datatypes.core.MultiPoint()\nA set of one or more points.\n\n\n\nexpr.datatypes.core.MultiPolygon()\nA set of one or more polygons.\n\n\n\nexpr.datatypes.core.Null()\nNull values.\n\n\n\nexpr.datatypes.core.Numeric()\nNumeric types.\n\n\n\nexpr.datatypes.core.Parametric()\nTypes that can be parameterized.\n\n\n\nexpr.datatypes.core.Point()\nA point described by two coordinates.\n\n\n\nexpr.datatypes.core.Polygon()\nA set of one or more closed line strings.\nThe first line string represents the shape (external ring) and the rest represent holes in that shape (internal rings).\n\n\n\nexpr.datatypes.core.Primitive()\nValues with known size.\n\n\n\nexpr.datatypes.core.SignedInteger()\nSigned integer values.\n\n\n\n\n\nName\nDescription\n\n\n\n\nlargest\nReturn the largest type of signed integer.\n\n\n\n\n\n\n\nexpr.datatypes.core.String()\n\n\nBecause of differences in the way different backends handle strings, we cannot assume that strings are UTF-8 encoded.\n\n\n\n\nexpr.datatypes.core.Struct()\nStructured values.\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_tuples\nConstruct a Struct type from pairs.\n\n\nnames\nReturn the names of the struct’s fields.\n\n\ntypes\nReturn the types of the struct’s fields.\n\n\n\n\n\nexpr.datatypes.core.Struct.from_tuples(cls, pairs, nullable=True)\nConstruct a Struct type from pairs.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npairs\ntyping.Iterable[tuple[ibis.expr.datatypes.str, ibis.expr.datatypes.str | ibis.expr.datatypes.core.DataType]]\nAn iterable of pairs of field name and type\nrequired\n\n\nnullable\nbool\nWhether the type is nullable\nTrue\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nStruct\nStruct data type instance\n\n\n\n\n\n\n\nexpr.datatypes.core.Struct.names(self)\nReturn the names of the struct’s fields.\n\n\n\nexpr.datatypes.core.Struct.types(self)\nReturn the types of the struct’s fields.\n\n\n\n\n\nexpr.datatypes.core.Temporal()\nData types related to time.\n\n\n\nexpr.datatypes.core.Time()\nTime values.\n\n\n\nexpr.datatypes.core.Timestamp()\nTimestamp values.\n\n\n\n\n\nName\nDescription\n\n\n\n\nscale\nThe scale of the timestamp if known.\n\n\ntimezone\nThe timezone of values of this type.\n\n\nunit\nReturn the unit of the timestamp.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_unit\nReturn a timestamp type with the given unit and timezone.\n\n\n\n\n\nexpr.datatypes.core.Timestamp.from_unit(cls, unit, timezone=None, nullable=True)\nReturn a timestamp type with the given unit and timezone.\n\n\n\n\n\nexpr.datatypes.core.UInt16()\nUnsigned 16-bit integers.\n\n\n\nexpr.datatypes.core.UInt32()\nUnsigned 32-bit integers.\n\n\n\nexpr.datatypes.core.UInt64()\nUnsigned 64-bit integers.\n\n\n\nexpr.datatypes.core.UInt8()\nUnsigned 8-bit integers.\n\n\n\nexpr.datatypes.core.UUID()\nA 128-bit number used to identify information in computer systems.\n\n\n\nexpr.datatypes.core.Unknown()\nAn unknown type.\n\n\n\nexpr.datatypes.core.UnsignedInteger()\nUnsigned integer values.\n\n\n\n\n\nName\nDescription\n\n\n\n\nlargest\nReturn the largest type of unsigned integer.\n\n\n\n\n\n\n\nexpr.datatypes.core.Variadic()\nValues with unknown size.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndtype\nConstruct an ibis datatype from a python type.\n\n\n\n\n\nexpr.datatypes.core.dtype(value, nullable=True)\nConstruct an ibis datatype from a python type."
  },
  {
    "objectID": "reference/expr.datatypes.core.html#classes",
    "href": "reference/expr.datatypes.core.html#classes",
    "title": "expr.datatypes.core",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nArray\nArray values.\n\n\nBinary\nA type representing a sequence of bytes.\n\n\nBoolean\n[True][True] or [False][False] values.\n\n\nBounds\nThe lower and upper bound of a fixed-size value.\n\n\nDataType\nBase class for all data types.\n\n\nDate\nDate values.\n\n\nDecimal\nFixed-precision decimal values.\n\n\nFloat16\n16-bit floating point numbers.\n\n\nFloat32\n32-bit floating point numbers.\n\n\nFloat64\n64-bit floating point numbers.\n\n\nFloating\nFloating point values.\n\n\nGeoSpatial\nGeospatial values.\n\n\nINET\nIP addresses.\n\n\nInt16\nSigned 16-bit integers.\n\n\nInt32\nSigned 32-bit integers.\n\n\nInt64\nSigned 64-bit integers.\n\n\nInt8\nSigned 8-bit integers.\n\n\nInteger\nInteger values.\n\n\nInterval\nInterval values.\n\n\nJSON\nJSON values.\n\n\nLineString\nA sequence of 2 or more points.\n\n\nMACADDR\nMedia Access Control (MAC) address of a network interface.\n\n\nMap\nAssociative array values.\n\n\nMultiLineString\nA set of one or more line strings.\n\n\nMultiPoint\nA set of one or more points.\n\n\nMultiPolygon\nA set of one or more polygons.\n\n\nNull\nNull values.\n\n\nNumeric\nNumeric types.\n\n\nParametric\nTypes that can be parameterized.\n\n\nPoint\nA point described by two coordinates.\n\n\nPolygon\nA set of one or more closed line strings.\n\n\nPrimitive\nValues with known size.\n\n\nSignedInteger\nSigned integer values.\n\n\nString\nA type representing a string.\n\n\nStruct\nStructured values.\n\n\nTemporal\nData types related to time.\n\n\nTime\nTime values.\n\n\nTimestamp\nTimestamp values.\n\n\nUInt16\nUnsigned 16-bit integers.\n\n\nUInt32\nUnsigned 32-bit integers.\n\n\nUInt64\nUnsigned 64-bit integers.\n\n\nUInt8\nUnsigned 8-bit integers.\n\n\nUUID\nA 128-bit number used to identify information in computer systems.\n\n\nUnknown\nAn unknown type.\n\n\nUnsignedInteger\nUnsigned integer values.\n\n\nVariadic\nValues with unknown size.\n\n\n\n\n\nexpr.datatypes.core.Array()\nArray values.\n\n\n\nexpr.datatypes.core.Binary()\n\n\nSome databases treat strings and blobs of equally, and some do not.\nFor example, Impala doesn’t make a distinction between string and binary types but PostgreSQL has a TEXT type and a BYTEA type which are distinct types that have different behavior.\n\n\n\n\nexpr.datatypes.core.Boolean()\n[True][True] or [False][False] values.\n\n\n\nexpr.datatypes.core.Bounds()\nThe lower and upper bound of a fixed-size value.\n\n\n\nexpr.datatypes.core.DataType()\nBase class for all data types.\n[DataType][ibis.expr.datatypes.DataType] instances are immutable.\n\n\n\n\n\nName\nDescription\n\n\n\n\nname\nReturn the name of the data type.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_dask\nReturn the equivalent ibis datatype.\n\n\nfrom_numpy\nReturn the equivalent ibis datatype.\n\n\nfrom_pandas\nReturn the equivalent ibis datatype.\n\n\nfrom_pyarrow\nReturn the equivalent ibis datatype.\n\n\nto_dask\nReturn the equivalent dask datatype.\n\n\nto_numpy\nReturn the equivalent numpy datatype.\n\n\nto_pandas\nReturn the equivalent pandas datatype.\n\n\nto_pyarrow\nReturn the equivalent pyarrow datatype.\n\n\n\n\n\nexpr.datatypes.core.DataType.from_dask(cls, dask_type, nullable=True)\nReturn the equivalent ibis datatype.\n\n\n\nexpr.datatypes.core.DataType.from_numpy(cls, numpy_type, nullable=True)\nReturn the equivalent ibis datatype.\n\n\n\nexpr.datatypes.core.DataType.from_pandas(cls, pandas_type, nullable=True)\nReturn the equivalent ibis datatype.\n\n\n\nexpr.datatypes.core.DataType.from_pyarrow(cls, arrow_type, nullable=True)\nReturn the equivalent ibis datatype.\n\n\n\nexpr.datatypes.core.DataType.to_dask(self)\nReturn the equivalent dask datatype.\n\n\n\nexpr.datatypes.core.DataType.to_numpy(self)\nReturn the equivalent numpy datatype.\n\n\n\nexpr.datatypes.core.DataType.to_pandas(self)\nReturn the equivalent pandas datatype.\n\n\n\nexpr.datatypes.core.DataType.to_pyarrow(self)\nReturn the equivalent pyarrow datatype.\n\n\n\n\n\nexpr.datatypes.core.Date()\nDate values.\n\n\n\nexpr.datatypes.core.Decimal(self, precision=None, scale=None, **kwargs)\nFixed-precision decimal values.\n\n\n\n\n\nName\nDescription\n\n\n\n\nlargest\nReturn the largest type of decimal.\n\n\nprecision\nThe number of decimal places values of this type can hold.\n\n\nscale\nThe number of values after the decimal point.\n\n\n\n\n\n\n\nexpr.datatypes.core.Float16()\n16-bit floating point numbers.\n\n\n\nexpr.datatypes.core.Float32()\n32-bit floating point numbers.\n\n\n\nexpr.datatypes.core.Float64()\n64-bit floating point numbers.\n\n\n\nexpr.datatypes.core.Floating()\nFloating point values.\n\n\n\n\n\nName\nDescription\n\n\n\n\nlargest\nReturn the largest type of floating point values.\n\n\n\n\n\n\n\nexpr.datatypes.core.GeoSpatial()\nGeospatial values.\n\n\n\n\n\nName\nDescription\n\n\n\n\ngeotype\nThe specific geospatial type.\n\n\nsrid\nThe spatial reference identifier.\n\n\n\n\n\n\n\nexpr.datatypes.core.INET()\nIP addresses.\n\n\n\nexpr.datatypes.core.Int16()\nSigned 16-bit integers.\n\n\n\nexpr.datatypes.core.Int32()\nSigned 32-bit integers.\n\n\n\nexpr.datatypes.core.Int64()\nSigned 64-bit integers.\n\n\n\nexpr.datatypes.core.Int8()\nSigned 8-bit integers.\n\n\n\nexpr.datatypes.core.Integer()\nInteger values.\n\n\n\n\n\nName\nDescription\n\n\n\n\nnbytes\nReturn the number of bytes used to store values of this type.\n\n\n\n\n\n\n\nexpr.datatypes.core.Interval()\nInterval values.\n\n\n\n\n\nName\nDescription\n\n\n\n\nresolution\nThe interval unit’s name.\n\n\nunit\nThe time unit of the interval.\n\n\n\n\n\n\n\nexpr.datatypes.core.JSON()\nJSON values.\n\n\n\nexpr.datatypes.core.LineString()\nA sequence of 2 or more points.\n\n\n\nexpr.datatypes.core.MACADDR()\nMedia Access Control (MAC) address of a network interface.\n\n\n\nexpr.datatypes.core.Map()\nAssociative array values.\n\n\n\nexpr.datatypes.core.MultiLineString()\nA set of one or more line strings.\n\n\n\nexpr.datatypes.core.MultiPoint()\nA set of one or more points.\n\n\n\nexpr.datatypes.core.MultiPolygon()\nA set of one or more polygons.\n\n\n\nexpr.datatypes.core.Null()\nNull values.\n\n\n\nexpr.datatypes.core.Numeric()\nNumeric types.\n\n\n\nexpr.datatypes.core.Parametric()\nTypes that can be parameterized.\n\n\n\nexpr.datatypes.core.Point()\nA point described by two coordinates.\n\n\n\nexpr.datatypes.core.Polygon()\nA set of one or more closed line strings.\nThe first line string represents the shape (external ring) and the rest represent holes in that shape (internal rings).\n\n\n\nexpr.datatypes.core.Primitive()\nValues with known size.\n\n\n\nexpr.datatypes.core.SignedInteger()\nSigned integer values.\n\n\n\n\n\nName\nDescription\n\n\n\n\nlargest\nReturn the largest type of signed integer.\n\n\n\n\n\n\n\nexpr.datatypes.core.String()\n\n\nBecause of differences in the way different backends handle strings, we cannot assume that strings are UTF-8 encoded.\n\n\n\n\nexpr.datatypes.core.Struct()\nStructured values.\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_tuples\nConstruct a Struct type from pairs.\n\n\nnames\nReturn the names of the struct’s fields.\n\n\ntypes\nReturn the types of the struct’s fields.\n\n\n\n\n\nexpr.datatypes.core.Struct.from_tuples(cls, pairs, nullable=True)\nConstruct a Struct type from pairs.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npairs\ntyping.Iterable[tuple[ibis.expr.datatypes.str, ibis.expr.datatypes.str | ibis.expr.datatypes.core.DataType]]\nAn iterable of pairs of field name and type\nrequired\n\n\nnullable\nbool\nWhether the type is nullable\nTrue\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nStruct\nStruct data type instance\n\n\n\n\n\n\n\nexpr.datatypes.core.Struct.names(self)\nReturn the names of the struct’s fields.\n\n\n\nexpr.datatypes.core.Struct.types(self)\nReturn the types of the struct’s fields.\n\n\n\n\n\nexpr.datatypes.core.Temporal()\nData types related to time.\n\n\n\nexpr.datatypes.core.Time()\nTime values.\n\n\n\nexpr.datatypes.core.Timestamp()\nTimestamp values.\n\n\n\n\n\nName\nDescription\n\n\n\n\nscale\nThe scale of the timestamp if known.\n\n\ntimezone\nThe timezone of values of this type.\n\n\nunit\nReturn the unit of the timestamp.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfrom_unit\nReturn a timestamp type with the given unit and timezone.\n\n\n\n\n\nexpr.datatypes.core.Timestamp.from_unit(cls, unit, timezone=None, nullable=True)\nReturn a timestamp type with the given unit and timezone.\n\n\n\n\n\nexpr.datatypes.core.UInt16()\nUnsigned 16-bit integers.\n\n\n\nexpr.datatypes.core.UInt32()\nUnsigned 32-bit integers.\n\n\n\nexpr.datatypes.core.UInt64()\nUnsigned 64-bit integers.\n\n\n\nexpr.datatypes.core.UInt8()\nUnsigned 8-bit integers.\n\n\n\nexpr.datatypes.core.UUID()\nA 128-bit number used to identify information in computer systems.\n\n\n\nexpr.datatypes.core.Unknown()\nAn unknown type.\n\n\n\nexpr.datatypes.core.UnsignedInteger()\nUnsigned integer values.\n\n\n\n\n\nName\nDescription\n\n\n\n\nlargest\nReturn the largest type of unsigned integer.\n\n\n\n\n\n\n\nexpr.datatypes.core.Variadic()\nValues with unknown size."
  },
  {
    "objectID": "reference/expr.datatypes.core.html#functions",
    "href": "reference/expr.datatypes.core.html#functions",
    "title": "expr.datatypes.core",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndtype\nConstruct an ibis datatype from a python type.\n\n\n\n\n\nexpr.datatypes.core.dtype(value, nullable=True)\nConstruct an ibis datatype from a python type."
  },
  {
    "objectID": "reference/selectors.html",
    "href": "reference/selectors.html",
    "title": "selectors",
    "section": "",
    "text": "selectors\nConvenient column selectors.\n!!! tip “Check out the blog post on selectors for examples!”\n\n\nColumn selectors are convenience functions for selecting columns that share some property.\n\n\n\nFor example, a common task is to be able to select all numeric columns for a subsequent computation.\nWithout selectors this becomes quite verbose and tedious to write:\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; t = ibis.table(...)  # doctest: +SKIP\n&gt;&gt;&gt; t.select([t[c] for c in t.columns if t[c].type().is_numeric()])  # doctest: +SKIP\nCompare that to the [numeric][ibis.selectors.numeric] selector:\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t.select(s.numeric())  # doctest: +SKIP\nWhen there are multiple properties to check it gets worse:\n&gt;&gt;&gt; t.select(  # doctest: +SKIP\n...     [\n...         t[c] for c in t.columns\n...         if t[c].type().is_numeric()\n...         if (\"a\" in c or \"cd\" in c)\n...     ]\n... )\nUsing a composition of selectors this is much less tiresome:\n&gt;&gt;&gt; t.select(s.numeric() & s.contains((\"a\", \"cd\")))  # doctest: +SKIP\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nSelector\nA column selector.\n\n\n\n\n\nselectors.Selector()\nA column selector.\n\n\n\n\n\nName\nDescription\n\n\n\n\nexpand\nExpand table into a sequence of value expressions.\n\n\n\n\n\nselectors.Selector.expand(self, table)\nExpand table into a sequence of value expressions.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable\nibis.Table\nAn ibis table expression\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Sequence[Value]\nA sequence of value expressions\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nacross\nApply data transformations across multiple columns.\n\n\nall\nReturn every column from a table.\n\n\nall_of\nInclude columns satisfying all of predicates.\n\n\nany_of\nInclude columns satisfying any of predicates.\n\n\nc\nSelect specific column names.\n\n\ncontains\nReturn columns whose name contains needles.\n\n\nendswith\nSelect columns whose name ends with one of suffixes.\n\n\nfirst\nReturn the first column of a table.\n\n\nif_all\nReturn the conjunction of predicate applied on all selector columns.\n\n\nif_any\nReturn the disjunction of predicate applied on all selector columns.\n\n\nlast\nReturn the last column of a table.\n\n\nmatches\nReturn columns whose name matches the regular expression regex.\n\n\nnumeric\nReturn numeric columns.\n\n\nof_type\nSelect columns of type dtype.\n\n\nstartswith\nSelect columns whose name starts with one of prefixes.\n\n\nwhere\nSelect columns that satisfy predicate.\n\n\n\n\n\nselectors.across(selector, func, names=None)\nApply data transformations across multiple columns.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nselector\nibis.selectors.Selector | typing.Iterable[str] | str\nAn expression that selects columns on which the transformation function will be applied, an iterable of str column names or a single str column name.\nrequired\n\n\nfunc\nibis.expr.deferred.Deferred | typing.Callable[[ibis.Value], ibis.Value] | typing.Mapping[str | None, ibis.expr.deferred.Deferred | typing.Callable[[ibis.Value], ibis.Value]]\nA function (or dictionary of functions) to use to transform the data.\nrequired\n\n\nnames\nstr | typing.Callable[[str, str | None], str] | None\nA lambda function or a format string to name the columns created by the transformation function.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nAcross\nAn Across selector object\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; from ibis import _, selectors as s\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.select(s.startswith(\"bill\")).mutate(\n...     s.across(\n...         s.numeric(),\n...         dict(centered =_ - _.mean()),\n...         names = \"{fn}_{col}\"\n...     )\n... )\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ bill_length_mm ┃ bill_depth_mm ┃ centered_bill_length_mm ┃ … ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ float64        │ float64       │ float64                 │ … │\n├────────────────┼───────────────┼─────────────────────────┼───┤\n│           39.1 │          18.7 │                -4.82193 │ … │\n│           39.5 │          17.4 │                -4.42193 │ … │\n│           40.3 │          18.0 │                -3.62193 │ … │\n│            nan │           nan │                     nan │ … │\n│           36.7 │          19.3 │                -7.22193 │ … │\n│           39.3 │          20.6 │                -4.62193 │ … │\n│           38.9 │          17.8 │                -5.02193 │ … │\n│           39.2 │          19.6 │                -4.72193 │ … │\n│           34.1 │          18.1 │                -9.82193 │ … │\n│           42.0 │          20.2 │                -1.92193 │ … │\n│              … │             … │                       … │ … │\n└────────────────┴───────────────┴─────────────────────────┴───┘\n\n\n\n\nselectors.all()\nReturn every column from a table.\n\n\n\nselectors.all_of(*predicates)\nInclude columns satisfying all of predicates.\n\n\n\nselectors.any_of(*predicates)\nInclude columns satisfying any of predicates.\n\n\n\nselectors.c(*names)\nSelect specific column names.\n\n\n\nselectors.contains(needles, how=any)\nReturn columns whose name contains needles.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nneedles\nstr | tuple[str, …]\nOne or more strings to search for in column names\nrequired\n\n\nhow\ntyping.Callable[[typing.Iterable[bool]], bool]\nA boolean reduction to allow the configuration of how needles are summarized.\nany\n\n\n\n\n\n\nSelect columns that contain either \"a\" or \"b\"\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(dict(a=\"int64\", b=\"string\", c=\"float\", d=\"array&lt;int16&gt;\", ab=\"struct&lt;x: int&gt;\"))\n&gt;&gt;&gt; expr = t.select(s.contains((\"a\", \"b\")))\n&gt;&gt;&gt; expr.columns\n['a', 'b', 'ab']\nSelect columns that contain all of \"a\" and \"b\", that is, both \"a\" and \"b\" must be in each column’s name to match.\n&gt;&gt;&gt; expr = t.select(s.contains((\"a\", \"b\"), how=all))\n&gt;&gt;&gt; expr.columns\n['ab']\n\n\n\n[matches][ibis.selectors.matches]\n\n\n\n\nselectors.endswith(suffixes)\nSelect columns whose name ends with one of suffixes.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsuffixes\nstr | tuple[str, …]\nSuffixes to compare column names against\nrequired\n\n\n\n\n\n\n[startswith][ibis.selectors.startswith]\n\n\n\n\nselectors.first()\nReturn the first column of a table.\n\n\n\nselectors.if_all(selector, predicate)\nReturn the conjunction of predicate applied on all selector columns.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nselector\nSelector\nA column selector\nrequired\n\n\npredicate\nibis.expr.deferred.Deferred | typing.Callable\nA callable or deferred object defining a predicate to apply to each column from selector.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from ibis import selectors as s, _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; penguins = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; cols = s.across(s.endswith(\"_mm\"), (_ - _.mean()) / _.std())\n&gt;&gt;&gt; expr = penguins.mutate(cols).filter(s.if_all(s.endswith(\"_mm\"), _.abs() &gt; 1))\n&gt;&gt;&gt; expr_by_hand = penguins.mutate(cols).filter(\n...     (_.bill_length_mm.abs() &gt; 1)\n...     & (_.bill_depth_mm.abs() &gt; 1)\n...     & (_.flipper_length_mm.abs() &gt; 1)\n... )\n&gt;&gt;&gt; expr.equals(expr_by_hand)\nTrue\n&gt;&gt;&gt; expr\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ … │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Dream     │      -1.157951 │      1.088129 │         -1.416272 │ … │\n│ Adelie  │ Torgersen │      -1.231217 │      1.138768 │         -1.202926 │ … │\n│ Gentoo  │ Biscoe    │       1.149917 │     -1.443781 │          1.214987 │ … │\n│ Gentoo  │ Biscoe    │       1.040019 │     -1.089314 │          1.072757 │ … │\n│ Gentoo  │ Biscoe    │       1.131601 │     -1.089314 │          1.712792 │ … │\n│ Gentoo  │ Biscoe    │       1.241499 │     -1.089314 │          1.570562 │ … │\n│ Gentoo  │ Biscoe    │       1.351398 │     -1.494420 │          1.214987 │ … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴───┘\n\n\n\n\nselectors.if_any(selector, predicate)\nReturn the disjunction of predicate applied on all selector columns.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nselector\nSelector\nA column selector\nrequired\n\n\npredicate\nibis.expr.deferred.Deferred | typing.Callable\nA callable or deferred object defining a predicate to apply to each column from selector.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from ibis import selectors as s, _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; penguins = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; cols = s.across(s.endswith(\"_mm\"), (_ - _.mean()) / _.std())\n&gt;&gt;&gt; expr = penguins.mutate(cols).filter(s.if_any(s.endswith(\"_mm\"), _.abs() &gt; 2))\n&gt;&gt;&gt; expr_by_hand = penguins.mutate(cols).filter(\n...     (_.bill_length_mm.abs() &gt; 2)\n...     | (_.bill_depth_mm.abs() &gt; 2)\n...     | (_.flipper_length_mm.abs() &gt; 2)\n... )\n&gt;&gt;&gt; expr.equals(expr_by_hand)\nTrue\n&gt;&gt;&gt; expr\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string │ float64        │ float64       │ float64           │ … │\n├─────────┼────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Biscoe │      -1.103002 │      0.733662 │         -2.056307 │ … │\n│ Gentoo  │ Biscoe │       1.113285 │     -0.431017 │          2.068368 │ … │\n│ Gentoo  │ Biscoe │       2.871660 │     -0.076550 │          2.068368 │ … │\n│ Gentoo  │ Biscoe │       1.900890 │     -0.734846 │          2.139483 │ … │\n│ Gentoo  │ Biscoe │       1.076652 │     -0.177826 │          2.068368 │ … │\n│ Gentoo  │ Biscoe │       0.856855 │     -0.582932 │          2.068368 │ … │\n│ Gentoo  │ Biscoe │       1.497929 │     -0.076550 │          2.068368 │ … │\n│ Gentoo  │ Biscoe │       1.388031 │     -0.431017 │          2.068368 │ … │\n│ Gentoo  │ Biscoe │       2.047422 │     -0.582932 │          2.068368 │ … │\n│ Adelie  │ Dream  │      -2.165354 │     -0.836123 │         -0.918466 │ … │\n│ …       │ …      │              … │             … │                 … │ … │\n└─────────┴────────┴────────────────┴───────────────┴───────────────────┴───┘\n\n\n\n\nselectors.last()\nReturn the last column of a table.\n\n\n\nselectors.matches(regex)\nReturn columns whose name matches the regular expression regex.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nregex\nstr | re.Pattern\nA string or re.Pattern object\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(dict(ab=\"string\", abd=\"int\", be=\"array&lt;string&gt;\"))\n&gt;&gt;&gt; expr = t.select(s.matches(r\"ab+\"))\n&gt;&gt;&gt; expr.columns\n['ab', 'abd']\n\n\n\n[contains][ibis.selectors.contains]\n\n\n\n\nselectors.numeric()\nReturn numeric columns.\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(dict(a=\"int\", b=\"string\", c=\"array&lt;string&gt;\"), name=\"t\")\n&gt;&gt;&gt; t\nUnboundTable: t\n  a int64\n  b string\n  c array&lt;string&gt;\n&gt;&gt;&gt; expr = t.select(s.numeric())  # `a` has integer type, so it's numeric\n&gt;&gt;&gt; expr.columns\n['a']\n\n\n\n[of_type][ibis.selectors.of_type]\n\n\n\n\nselectors.of_type(dtype)\nSelect columns of type dtype.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndtype\nibis.DataType | str | type[ibis.DataType]\nDataType instance, str or DataType class\nrequired\n\n\n\n\n\n\nSelect according to a specific DataType instance\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(dict(name=\"string\", siblings=\"array&lt;string&gt;\", parents=\"array&lt;int64&gt;\"))\n&gt;&gt;&gt; expr = t.select(s.of_type(dt.Array(dt.string)))\n&gt;&gt;&gt; expr.columns\n['siblings']\nStrings are also accepted\n&gt;&gt;&gt; expr = t.select(s.of_type(\"array&lt;string&gt;\"))\n&gt;&gt;&gt; expr.columns\n['siblings']\nAbstract/unparametrized types may also be specified by their string name (e.g. “integer” for any integer type), or by passing in a DataType class instead. The following options are equivalent.\n&gt;&gt;&gt; expr1 = t.select(s.of_type(\"array\"))\n&gt;&gt;&gt; expr2 = t.select(s.of_type(dt.Array))\n&gt;&gt;&gt; expr1.equals(expr2)\nTrue\n&gt;&gt;&gt; expr2.columns\n['siblings', 'parents']\n\n\n\n[numeric][ibis.selectors.numeric]\n\n\n\n\nselectors.startswith(prefixes)\nSelect columns whose name starts with one of prefixes.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprefixes\nstr | tuple[str, …]\nPrefixes to compare column names against\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(dict(apples=\"int\", oranges=\"float\", bananas=\"bool\"), name=\"t\")\n&gt;&gt;&gt; expr = t.select(s.startswith((\"a\", \"b\")))\n&gt;&gt;&gt; expr.columns\n['apples', 'bananas']\n\n\n\n[endswith][ibis.selectors.endswith]\n\n\n\n\nselectors.where(predicate)\nSelect columns that satisfy predicate.\nUse this selector when one of the other selectors does not meet your needs.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npredicate\ntyping.Callable[[ibis.Value], bool]\nA callable that accepts an ibis value expression and returns a bool\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(dict(a=\"float32\"), name=\"t\")\n&gt;&gt;&gt; expr = t.select(s.where(lambda col: col.get_name() == \"a\"))\n&gt;&gt;&gt; expr.columns\n['a']"
  },
  {
    "objectID": "reference/selectors.html#rationale",
    "href": "reference/selectors.html#rationale",
    "title": "selectors",
    "section": "",
    "text": "Column selectors are convenience functions for selecting columns that share some property."
  },
  {
    "objectID": "reference/selectors.html#discussion",
    "href": "reference/selectors.html#discussion",
    "title": "selectors",
    "section": "",
    "text": "For example, a common task is to be able to select all numeric columns for a subsequent computation.\nWithout selectors this becomes quite verbose and tedious to write:\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; t = ibis.table(...)  # doctest: +SKIP\n&gt;&gt;&gt; t.select([t[c] for c in t.columns if t[c].type().is_numeric()])  # doctest: +SKIP\nCompare that to the [numeric][ibis.selectors.numeric] selector:\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t.select(s.numeric())  # doctest: +SKIP\nWhen there are multiple properties to check it gets worse:\n&gt;&gt;&gt; t.select(  # doctest: +SKIP\n...     [\n...         t[c] for c in t.columns\n...         if t[c].type().is_numeric()\n...         if (\"a\" in c or \"cd\" in c)\n...     ]\n... )\nUsing a composition of selectors this is much less tiresome:\n&gt;&gt;&gt; t.select(s.numeric() & s.contains((\"a\", \"cd\")))  # doctest: +SKIP"
  },
  {
    "objectID": "reference/selectors.html#classes",
    "href": "reference/selectors.html#classes",
    "title": "selectors",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nSelector\nA column selector.\n\n\n\n\n\nselectors.Selector()\nA column selector.\n\n\n\n\n\nName\nDescription\n\n\n\n\nexpand\nExpand table into a sequence of value expressions.\n\n\n\n\n\nselectors.Selector.expand(self, table)\nExpand table into a sequence of value expressions.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable\nibis.Table\nAn ibis table expression\nrequired\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Sequence[Value]\nA sequence of value expressions"
  },
  {
    "objectID": "reference/selectors.html#functions",
    "href": "reference/selectors.html#functions",
    "title": "selectors",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nacross\nApply data transformations across multiple columns.\n\n\nall\nReturn every column from a table.\n\n\nall_of\nInclude columns satisfying all of predicates.\n\n\nany_of\nInclude columns satisfying any of predicates.\n\n\nc\nSelect specific column names.\n\n\ncontains\nReturn columns whose name contains needles.\n\n\nendswith\nSelect columns whose name ends with one of suffixes.\n\n\nfirst\nReturn the first column of a table.\n\n\nif_all\nReturn the conjunction of predicate applied on all selector columns.\n\n\nif_any\nReturn the disjunction of predicate applied on all selector columns.\n\n\nlast\nReturn the last column of a table.\n\n\nmatches\nReturn columns whose name matches the regular expression regex.\n\n\nnumeric\nReturn numeric columns.\n\n\nof_type\nSelect columns of type dtype.\n\n\nstartswith\nSelect columns whose name starts with one of prefixes.\n\n\nwhere\nSelect columns that satisfy predicate.\n\n\n\n\n\nselectors.across(selector, func, names=None)\nApply data transformations across multiple columns.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nselector\nibis.selectors.Selector | typing.Iterable[str] | str\nAn expression that selects columns on which the transformation function will be applied, an iterable of str column names or a single str column name.\nrequired\n\n\nfunc\nibis.expr.deferred.Deferred | typing.Callable[[ibis.Value], ibis.Value] | typing.Mapping[str | None, ibis.expr.deferred.Deferred | typing.Callable[[ibis.Value], ibis.Value]]\nA function (or dictionary of functions) to use to transform the data.\nrequired\n\n\nnames\nstr | typing.Callable[[str, str | None], str] | None\nA lambda function or a format string to name the columns created by the transformation function.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nAcross\nAn Across selector object\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; from ibis import _, selectors as s\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.select(s.startswith(\"bill\")).mutate(\n...     s.across(\n...         s.numeric(),\n...         dict(centered =_ - _.mean()),\n...         names = \"{fn}_{col}\"\n...     )\n... )\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ bill_length_mm ┃ bill_depth_mm ┃ centered_bill_length_mm ┃ … ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ float64        │ float64       │ float64                 │ … │\n├────────────────┼───────────────┼─────────────────────────┼───┤\n│           39.1 │          18.7 │                -4.82193 │ … │\n│           39.5 │          17.4 │                -4.42193 │ … │\n│           40.3 │          18.0 │                -3.62193 │ … │\n│            nan │           nan │                     nan │ … │\n│           36.7 │          19.3 │                -7.22193 │ … │\n│           39.3 │          20.6 │                -4.62193 │ … │\n│           38.9 │          17.8 │                -5.02193 │ … │\n│           39.2 │          19.6 │                -4.72193 │ … │\n│           34.1 │          18.1 │                -9.82193 │ … │\n│           42.0 │          20.2 │                -1.92193 │ … │\n│              … │             … │                       … │ … │\n└────────────────┴───────────────┴─────────────────────────┴───┘\n\n\n\n\nselectors.all()\nReturn every column from a table.\n\n\n\nselectors.all_of(*predicates)\nInclude columns satisfying all of predicates.\n\n\n\nselectors.any_of(*predicates)\nInclude columns satisfying any of predicates.\n\n\n\nselectors.c(*names)\nSelect specific column names.\n\n\n\nselectors.contains(needles, how=any)\nReturn columns whose name contains needles.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nneedles\nstr | tuple[str, …]\nOne or more strings to search for in column names\nrequired\n\n\nhow\ntyping.Callable[[typing.Iterable[bool]], bool]\nA boolean reduction to allow the configuration of how needles are summarized.\nany\n\n\n\n\n\n\nSelect columns that contain either \"a\" or \"b\"\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(dict(a=\"int64\", b=\"string\", c=\"float\", d=\"array&lt;int16&gt;\", ab=\"struct&lt;x: int&gt;\"))\n&gt;&gt;&gt; expr = t.select(s.contains((\"a\", \"b\")))\n&gt;&gt;&gt; expr.columns\n['a', 'b', 'ab']\nSelect columns that contain all of \"a\" and \"b\", that is, both \"a\" and \"b\" must be in each column’s name to match.\n&gt;&gt;&gt; expr = t.select(s.contains((\"a\", \"b\"), how=all))\n&gt;&gt;&gt; expr.columns\n['ab']\n\n\n\n[matches][ibis.selectors.matches]\n\n\n\n\nselectors.endswith(suffixes)\nSelect columns whose name ends with one of suffixes.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsuffixes\nstr | tuple[str, …]\nSuffixes to compare column names against\nrequired\n\n\n\n\n\n\n[startswith][ibis.selectors.startswith]\n\n\n\n\nselectors.first()\nReturn the first column of a table.\n\n\n\nselectors.if_all(selector, predicate)\nReturn the conjunction of predicate applied on all selector columns.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nselector\nSelector\nA column selector\nrequired\n\n\npredicate\nibis.expr.deferred.Deferred | typing.Callable\nA callable or deferred object defining a predicate to apply to each column from selector.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from ibis import selectors as s, _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; penguins = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; cols = s.across(s.endswith(\"_mm\"), (_ - _.mean()) / _.std())\n&gt;&gt;&gt; expr = penguins.mutate(cols).filter(s.if_all(s.endswith(\"_mm\"), _.abs() &gt; 1))\n&gt;&gt;&gt; expr_by_hand = penguins.mutate(cols).filter(\n...     (_.bill_length_mm.abs() &gt; 1)\n...     & (_.bill_depth_mm.abs() &gt; 1)\n...     & (_.flipper_length_mm.abs() &gt; 1)\n... )\n&gt;&gt;&gt; expr.equals(expr_by_hand)\nTrue\n&gt;&gt;&gt; expr\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ … │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Dream     │      -1.157951 │      1.088129 │         -1.416272 │ … │\n│ Adelie  │ Torgersen │      -1.231217 │      1.138768 │         -1.202926 │ … │\n│ Gentoo  │ Biscoe    │       1.149917 │     -1.443781 │          1.214987 │ … │\n│ Gentoo  │ Biscoe    │       1.040019 │     -1.089314 │          1.072757 │ … │\n│ Gentoo  │ Biscoe    │       1.131601 │     -1.089314 │          1.712792 │ … │\n│ Gentoo  │ Biscoe    │       1.241499 │     -1.089314 │          1.570562 │ … │\n│ Gentoo  │ Biscoe    │       1.351398 │     -1.494420 │          1.214987 │ … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴───┘\n\n\n\n\nselectors.if_any(selector, predicate)\nReturn the disjunction of predicate applied on all selector columns.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nselector\nSelector\nA column selector\nrequired\n\n\npredicate\nibis.expr.deferred.Deferred | typing.Callable\nA callable or deferred object defining a predicate to apply to each column from selector.\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from ibis import selectors as s, _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; penguins = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; cols = s.across(s.endswith(\"_mm\"), (_ - _.mean()) / _.std())\n&gt;&gt;&gt; expr = penguins.mutate(cols).filter(s.if_any(s.endswith(\"_mm\"), _.abs() &gt; 2))\n&gt;&gt;&gt; expr_by_hand = penguins.mutate(cols).filter(\n...     (_.bill_length_mm.abs() &gt; 2)\n...     | (_.bill_depth_mm.abs() &gt; 2)\n...     | (_.flipper_length_mm.abs() &gt; 2)\n... )\n&gt;&gt;&gt; expr.equals(expr_by_hand)\nTrue\n&gt;&gt;&gt; expr\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ species ┃ island ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ … ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string  │ string │ float64        │ float64       │ float64           │ … │\n├─────────┼────────┼────────────────┼───────────────┼───────────────────┼───┤\n│ Adelie  │ Biscoe │      -1.103002 │      0.733662 │         -2.056307 │ … │\n│ Gentoo  │ Biscoe │       1.113285 │     -0.431017 │          2.068368 │ … │\n│ Gentoo  │ Biscoe │       2.871660 │     -0.076550 │          2.068368 │ … │\n│ Gentoo  │ Biscoe │       1.900890 │     -0.734846 │          2.139483 │ … │\n│ Gentoo  │ Biscoe │       1.076652 │     -0.177826 │          2.068368 │ … │\n│ Gentoo  │ Biscoe │       0.856855 │     -0.582932 │          2.068368 │ … │\n│ Gentoo  │ Biscoe │       1.497929 │     -0.076550 │          2.068368 │ … │\n│ Gentoo  │ Biscoe │       1.388031 │     -0.431017 │          2.068368 │ … │\n│ Gentoo  │ Biscoe │       2.047422 │     -0.582932 │          2.068368 │ … │\n│ Adelie  │ Dream  │      -2.165354 │     -0.836123 │         -0.918466 │ … │\n│ …       │ …      │              … │             … │                 … │ … │\n└─────────┴────────┴────────────────┴───────────────┴───────────────────┴───┘\n\n\n\n\nselectors.last()\nReturn the last column of a table.\n\n\n\nselectors.matches(regex)\nReturn columns whose name matches the regular expression regex.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nregex\nstr | re.Pattern\nA string or re.Pattern object\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(dict(ab=\"string\", abd=\"int\", be=\"array&lt;string&gt;\"))\n&gt;&gt;&gt; expr = t.select(s.matches(r\"ab+\"))\n&gt;&gt;&gt; expr.columns\n['ab', 'abd']\n\n\n\n[contains][ibis.selectors.contains]\n\n\n\n\nselectors.numeric()\nReturn numeric columns.\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(dict(a=\"int\", b=\"string\", c=\"array&lt;string&gt;\"), name=\"t\")\n&gt;&gt;&gt; t\nUnboundTable: t\n  a int64\n  b string\n  c array&lt;string&gt;\n&gt;&gt;&gt; expr = t.select(s.numeric())  # `a` has integer type, so it's numeric\n&gt;&gt;&gt; expr.columns\n['a']\n\n\n\n[of_type][ibis.selectors.of_type]\n\n\n\n\nselectors.of_type(dtype)\nSelect columns of type dtype.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndtype\nibis.DataType | str | type[ibis.DataType]\nDataType instance, str or DataType class\nrequired\n\n\n\n\n\n\nSelect according to a specific DataType instance\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(dict(name=\"string\", siblings=\"array&lt;string&gt;\", parents=\"array&lt;int64&gt;\"))\n&gt;&gt;&gt; expr = t.select(s.of_type(dt.Array(dt.string)))\n&gt;&gt;&gt; expr.columns\n['siblings']\nStrings are also accepted\n&gt;&gt;&gt; expr = t.select(s.of_type(\"array&lt;string&gt;\"))\n&gt;&gt;&gt; expr.columns\n['siblings']\nAbstract/unparametrized types may also be specified by their string name (e.g. “integer” for any integer type), or by passing in a DataType class instead. The following options are equivalent.\n&gt;&gt;&gt; expr1 = t.select(s.of_type(\"array\"))\n&gt;&gt;&gt; expr2 = t.select(s.of_type(dt.Array))\n&gt;&gt;&gt; expr1.equals(expr2)\nTrue\n&gt;&gt;&gt; expr2.columns\n['siblings', 'parents']\n\n\n\n[numeric][ibis.selectors.numeric]\n\n\n\n\nselectors.startswith(prefixes)\nSelect columns whose name starts with one of prefixes.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprefixes\nstr | tuple[str, …]\nPrefixes to compare column names against\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(dict(apples=\"int\", oranges=\"float\", bananas=\"bool\"), name=\"t\")\n&gt;&gt;&gt; expr = t.select(s.startswith((\"a\", \"b\")))\n&gt;&gt;&gt; expr.columns\n['apples', 'bananas']\n\n\n\n[endswith][ibis.selectors.endswith]\n\n\n\n\nselectors.where(predicate)\nSelect columns that satisfy predicate.\nUse this selector when one of the other selectors does not meet your needs.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npredicate\ntyping.Callable[[ibis.Value], bool]\nA callable that accepts an ibis value expression and returns a bool\nrequired\n\n\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(dict(a=\"float32\"), name=\"t\")\n&gt;&gt;&gt; expr = t.select(s.where(lambda col: col.get_name() == \"a\"))\n&gt;&gt;&gt; expr.columns\n['a']"
  },
  {
    "objectID": "reference/expression-generic.html",
    "href": "reference/expression-generic.html",
    "title": "Generic Expression APIs",
    "section": "",
    "text": "These expressions are available on scalars and columns of any element type."
  },
  {
    "objectID": "reference/expression-generic.html#methods",
    "href": "reference/expression-generic.html#methods",
    "title": "Generic Expression APIs",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nas_table\nPromote the expression to a table.\n\n\nasc\nSort an expression ascending.\n\n\nbetween\nCheck if this expression is between lower and upper, inclusive.\n\n\ncase\nCreate a SimpleCaseBuilder to chain multiple if-else statements.\n\n\ncases\nCreate a case expression in one shot.\n\n\ncast\nCast expression to indicated data type.\n\n\ncoalesce\nReturn the first non-null value from args.\n\n\ncollect\nAggregate this expression’s elements into an array.\n\n\ndesc\nSort an expression descending.\n\n\nfillna\nReplace any null values with the indicated fill value.\n\n\ngreatest\nCompute the largest value among the supplied arguments.\n\n\ngroup_concat\nConcatenate values using the indicated separator to produce a string.\n\n\nhash\nCompute an integer hash value.\n\n\nidentical_to\nReturn whether this expression is identical to other.\n\n\nisin\nCheck whether this expression’s values are in values.\n\n\nisnull\nReturn whether this expression is NULL.\n\n\nleast\nCompute the smallest value among the supplied arguments.\n\n\nname\nRename an expression to name.\n\n\nnotin\nCheck whether this expression’s values are not in values.\n\n\nnotnull\nReturn whether this expression is not NULL.\n\n\nnullif\nSet values to null if they equal the values null_if_expr.\n\n\nover\nConstruct a window expression.\n\n\nsubstitute\nReplace values given in values with replacement.\n\n\nto_pandas\nConvert a column expression to a pandas Series or scalar object.\n\n\ntry_cast\nTry cast expression to indicated data type.\n\n\ntype\nReturn the [DataType] of this expression.\n\n\ntypeof\nReturn the data type of the expression.\n\n\n\n\nas_table\nexpr.types.generic.Value.as_table(self)\nPromote the expression to a table.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA table expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; t = ibis.table(dict(a=\"str\"), name=\"t\")\n&gt;&gt;&gt; expr = t.a.length().name(\"len\").as_table()\n&gt;&gt;&gt; expected = t.select(len=t.a.length())\n&gt;&gt;&gt; expr.equals(expected)\nTrue\n\n\n\nasc\nexpr.types.generic.Value.asc(self)\nSort an expression ascending.\n\n\nbetween\nexpr.types.generic.Value.between(self, lower, upper)\nCheck if this expression is between lower and upper, inclusive.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlower\nValue\nLower bound\nrequired\n\n\nupper\nValue\nUpper bound\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nExpression indicating membership in the provided range\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch().limit(5)\n&gt;&gt;&gt; t.bill_length_mm.between(35, 38)\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Between(bill_length_mm, 35, 38) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                         │\n├─────────────────────────────────┤\n│ False                           │\n│ False                           │\n│ False                           │\n│ NULL                            │\n│ True                            │\n└─────────────────────────────────┘\n\n\n\ncase\nexpr.types.generic.Value.case(self)\nCreate a SimpleCaseBuilder to chain multiple if-else statements.\nAdd new search expressions with the .when() method. These must be comparable with this column expression. Conclude by calling .end()\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nSimpleCaseBuilder\nA case builder\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; t = ibis.table([('string_col', 'string')], name='t')\n&gt;&gt;&gt; expr = t.string_col\n&gt;&gt;&gt; case_expr = (expr.case()\n...              .when('a', 'an a')\n...              .when('b', 'a b')\n...              .else_('null or (not a and not b)')\n...              .end())\n&gt;&gt;&gt; case_expr\nr0 := UnboundTable: t\n  string_col string\nSimpleCase(...)\n\n\n\ncases\nexpr.types.generic.Value.cases(self, case_result_pairs, default=None)\nCreate a case expression in one shot.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncase_result_pairs\ntyping.Iterable[tuple[ibis.BooleanValue, ibis.expr.types.generic.Value]]\nConditional-result pairs\nrequired\n\n\ndefault\nibis.expr.types.generic.Value | None\nValue to return if none of the case conditions are true\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nValue expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 2, 1, 2, 3, 2, 4]})\n&gt;&gt;&gt; t\n┏━━━━━━━━┓\n┃ values ┃\n┡━━━━━━━━┩\n│ int64  │\n├────────┤\n│      1 │\n│      2 │\n│      1 │\n│      2 │\n│      3 │\n│      2 │\n│      4 │\n└────────┘\n&gt;&gt;&gt; number_letter_map = ((1, \"a\"), (2, \"b\"), (3, \"c\"))\n&gt;&gt;&gt; t.values.cases(number_letter_map, default=\"unk\").name(\"replace\")\n┏━━━━━━━━━┓\n┃ replace ┃\n┡━━━━━━━━━┩\n│ string  │\n├─────────┤\n│ a       │\n│ b       │\n│ a       │\n│ b       │\n│ c       │\n│ b       │\n│ unk     │\n└─────────┘\n\n\n\ncast\nexpr.types.generic.Value.cast(self, target_type)\nCast expression to indicated data type.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntarget_type\nibis.DataType\nType to cast to\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nCasted expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = False\n&gt;&gt;&gt; t = ibis.table(dict(a=\"int64\"), name=\"t\")\n&gt;&gt;&gt; t.a.cast(\"float\")\nr0 := UnboundTable: t\n  a int64\nCast(a, float64): Cast(r0.a, to=float64)\n\n\n\ncoalesce\nexpr.types.generic.Value.coalesce(self, *args)\nReturn the first non-null value from args.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nValue\nArguments from which to choose the first non-null value\n()\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nCoalesced expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.coalesce(None, 4, 5).name(\"x\")\nx: Coalesce(...)\n\n\n\ncollect\nexpr.types.generic.Value.collect(self, where=None)\nAggregate this expression’s elements into an array.\nThis function is called array_agg, list_agg, or list in other systems.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nibis.BooleanValue | None\nFilter to apply before aggregation\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayScalar\nCollected array\n\n\n\n\n\nExamples\nBasic collect usage\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"key\": list(\"aaabb\"), \"value\": [1, 2, 3, 4, 5]})\n&gt;&gt;&gt; t\n┏━━━━━━━━┳━━━━━━━┓\n┃ key    ┃ value ┃\n┡━━━━━━━━╇━━━━━━━┩\n│ string │ int64 │\n├────────┼───────┤\n│ a      │     1 │\n│ a      │     2 │\n│ a      │     3 │\n│ b      │     4 │\n│ b      │     5 │\n└────────┴───────┘\n&gt;&gt;&gt; t.value.collect()\n[1, 2, 3, 4, 5]\n&gt;&gt;&gt; type(t.value.collect())\n&lt;class 'ibis.expr.types.arrays.ArrayScalar'&gt;\nCollect elements per group\n&gt;&gt;&gt; t.group_by(\"key\").agg(v=lambda t: t.value.collect())\n┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n┃ key    ┃ v                    ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n│ string │ array&lt;int64&gt;         │\n├────────┼──────────────────────┤\n│ a      │ [1, 2, ... +1]       │\n│ b      │ [4, 5]               │\n└────────┴──────────────────────┘\nCollect elements per group using a filter\n&gt;&gt;&gt; t.group_by(\"key\").agg(v=lambda t: t.value.collect(where=t.value &gt; 1))\n┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n┃ key    ┃ v                    ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n│ string │ array&lt;int64&gt;         │\n├────────┼──────────────────────┤\n│ a      │ [2, 3]               │\n│ b      │ [4, 5]               │\n└────────┴──────────────────────┘\n\n\n\ndesc\nexpr.types.generic.Value.desc(self)\nSort an expression descending.\n\n\nfillna\nexpr.types.generic.Value.fillna(self, fill_value)\nReplace any null values with the indicated fill value.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfill_value\nScalar\nValue with which to replace NA values in self\nrequired\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch().limit(5)\n&gt;&gt;&gt; t.sex\n┏━━━━━━━━┓\n┃ sex    ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ male   │\n│ female │\n│ female │\n│ NULL   │\n│ female │\n└────────┘\n&gt;&gt;&gt; t.sex.fillna(\"unrecorded\").name(\"sex\")\n┏━━━━━━━━━━━━┓\n┃ sex        ┃\n┡━━━━━━━━━━━━┩\n│ string     │\n├────────────┤\n│ male       │\n│ female     │\n│ female     │\n│ unrecorded │\n│ female     │\n└────────────┘\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nself filled with fill_value where it is NA\n\n\n\n\n\n\ngreatest\nexpr.types.generic.Value.greatest(self, *args)\nCompute the largest value among the supplied arguments.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nibis.Value\nArguments to choose from\n()\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nMaximum of the passed arguments\n\n\n\n\n\n\ngroup_concat\nexpr.types.generic.Value.group_concat(self, sep=',', where=None)\nConcatenate values using the indicated separator to produce a string.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsep\nstr\nSeparator will be used to join strings\n','\n\n\nwhere\nibis.BooleanValue | None\nFilter expression\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringScalar\nConcatenated string expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch().limit(5)\n&gt;&gt;&gt; t[[\"bill_length_mm\", \"bill_depth_mm\"]]\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ bill_length_mm ┃ bill_depth_mm ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ float64        │ float64       │\n├────────────────┼───────────────┤\n│           39.1 │          18.7 │\n│           39.5 │          17.4 │\n│           40.3 │          18.0 │\n│            nan │           nan │\n│           36.7 │          19.3 │\n└────────────────┴───────────────┘\n&gt;&gt;&gt; t.bill_length_mm.group_concat()\n'39.1,39.5,40.3,36.7'\n&gt;&gt;&gt; t.bill_length_mm.group_concat(sep=\": \")\n'39.1: 39.5: 40.3: 36.7'\n&gt;&gt;&gt; t.bill_length_mm.group_concat(sep=\": \", where=t.bill_depth_mm &gt; 18)\n'39.1: 36.7'\n\n\n\nhash\nexpr.types.generic.Value.hash(self)\nCompute an integer hash value.\n!!! info “The hashing function used is backend-dependent.”\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nThe hash value of self\n\n\n\n\n\n\nidentical_to\nexpr.types.generic.Value.identical_to(self, other)\nReturn whether this expression is identical to other.\nCorresponds to IS NOT DISTINCT FROM in SQL.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nValue\nExpression to compare to\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether this expression is not distinct from other\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; one = ibis.literal(1)\n&gt;&gt;&gt; two = ibis.literal(2)\n&gt;&gt;&gt; two.identical_to(one + one)\nTrue\n\n\n\nisin\nexpr.types.generic.Value.isin(self, values)\nCheck whether this expression’s values are in values.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\nibis.expr.types.generic.Value | typing.Sequence[ibis.expr.types.generic.Value]\nValues or expression to check for membership\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nExpression indicating membership\n\n\n\n\n\nExamples\nCheck whether a column’s values are contained in a sequence\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; table = ibis.table(dict(string_col='string'), name=\"t\")\n&gt;&gt;&gt; table.string_col.isin(['foo', 'bar', 'baz'])\nr0 := UnboundTable: t\n  string_col string\nContains(string_col): Contains(...)\nCheck whether a column’s values are contained in another table’s column\n&gt;&gt;&gt; table2 = ibis.table(dict(other_string_col='string'), name=\"t2\")\n&gt;&gt;&gt; table.string_col.isin(table2.other_string_col)\nr0 := UnboundTable: t\n  string_col string\nr1 := UnboundTable: t2\n  other_string_col string\nContains(string_col, other_string_col): Contains(...)\n\n\n\nisnull\nexpr.types.generic.Value.isnull(self)\nReturn whether this expression is NULL.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch().limit(5)\n&gt;&gt;&gt; t.bill_depth_mm\n┏━━━━━━━━━━━━━━━┓\n┃ bill_depth_mm ┃\n┡━━━━━━━━━━━━━━━┩\n│ float64       │\n├───────────────┤\n│          18.7 │\n│          17.4 │\n│          18.0 │\n│           nan │\n│          19.3 │\n└───────────────┘\n&gt;&gt;&gt; t.bill_depth_mm.isnull()\n┏━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ IsNull(bill_depth_mm) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean               │\n├───────────────────────┤\n│ False                 │\n│ False                 │\n│ False                 │\n│ True                  │\n│ False                 │\n└───────────────────────┘\n\n\n\nleast\nexpr.types.generic.Value.least(self, *args)\nCompute the smallest value among the supplied arguments.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nibis.Value\nArguments to choose from\n()\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nMinimum of the passed arguments\n\n\n\n\n\n\nname\nexpr.types.generic.Value.name(self, name)\nRename an expression to name.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nThe new name of the expression\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nself with name name\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; t = ibis.table(dict(a=\"int64\"), name=\"t\")\n&gt;&gt;&gt; t.a.name(\"b\")\nr0 := UnboundTable: t\n  a int64\nb: r0.a\n\n\n\nnotin\nexpr.types.generic.Value.notin(self, values)\nCheck whether this expression’s values are not in values.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\nibis.expr.types.generic.Value | typing.Sequence[ibis.expr.types.generic.Value]\nValues or expression to check for lack of membership\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self’s values are not contained in values\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch().limit(5)\n&gt;&gt;&gt; t.bill_depth_mm\n┏━━━━━━━━━━━━━━━┓\n┃ bill_depth_mm ┃\n┡━━━━━━━━━━━━━━━┩\n│ float64       │\n├───────────────┤\n│          18.7 │\n│          17.4 │\n│          18.0 │\n│           nan │\n│          19.3 │\n└───────────────┘\n&gt;&gt;&gt; t.bill_depth_mm.notin([18.7, 18.1])\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ NotContains(bill_depth_mm) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                    │\n├────────────────────────────┤\n│ False                      │\n│ True                       │\n│ True                       │\n│ NULL                       │\n│ True                       │\n└────────────────────────────┘\n\n\n\nnotnull\nexpr.types.generic.Value.notnull(self)\nReturn whether this expression is not NULL.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch().limit(5)\n&gt;&gt;&gt; t.bill_depth_mm\n┏━━━━━━━━━━━━━━━┓\n┃ bill_depth_mm ┃\n┡━━━━━━━━━━━━━━━┩\n│ float64       │\n├───────────────┤\n│          18.7 │\n│          17.4 │\n│          18.0 │\n│           nan │\n│          19.3 │\n└───────────────┘\n&gt;&gt;&gt; t.bill_depth_mm.notnull()\n┏━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ NotNull(bill_depth_mm) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                │\n├────────────────────────┤\n│ True                   │\n│ True                   │\n│ True                   │\n│ False                  │\n│ True                   │\n└────────────────────────┘\n\n\n\nnullif\nexpr.types.generic.Value.nullif(self, null_if_expr)\nSet values to null if they equal the values null_if_expr.\nCommonly use to avoid divide-by-zero problems by replacing zero with NULL in the divisor.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnull_if_expr\nValue\nExpression indicating what values should be NULL\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nValue expression\n\n\n\n\n\n\nover\nexpr.types.generic.Value.over(self, window=None, *, rows=None, range=None, group_by=None, order_by=None)\nConstruct a window expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwindow\n\nWindow specification\nNone\n\n\nrows\n\nWhether to use the ROWS window clause\nNone\n\n\nrange\n\nWhether to use the RANGE window clause\nNone\n\n\ngroup_by\n\nGrouping key\nNone\n\n\norder_by\n\nOrdering key\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nA window function expression\n\n\n\n\n\n\nsubstitute\nexpr.types.generic.Value.substitute(self, value, replacement=None, else_=None)\nReplace values given in values with replacement.\nThis is similar to the pandas replace method.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nibis.expr.types.generic.Value | dict\nExpression or dict.\nrequired\n\n\nreplacement\nibis.expr.types.generic.Value | None\nIf an expression is passed to value, this must be passed.\nNone\n\n\nelse_\nibis.expr.types.generic.Value | None\nIf an original value does not match value, then else_ is used. The default of None means leave the original value unchanged.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nReplaced values\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.island.value_counts()\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n┃ island    ┃ island_count ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n│ string    │ int64        │\n├───────────┼──────────────┤\n│ Torgersen │           52 │\n│ Biscoe    │          168 │\n│ Dream     │          124 │\n└───────────┴──────────────┘\n&gt;&gt;&gt; t.island.substitute({\"Torgersen\": \"torg\", \"Biscoe\": \"bisc\"}).value_counts()\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ SimpleCase(island, island) ┃ SimpleCase(island, island)_count ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                     │ int64                            │\n├────────────────────────────┼──────────────────────────────────┤\n│ torg                       │                               52 │\n│ bisc                       │                              168 │\n│ Dream                      │                              124 │\n└────────────────────────────┴──────────────────────────────────┘\n\n\n\nto_pandas\nexpr.types.generic.Value.to_pandas(self, **kwargs)\nConvert a column expression to a pandas Series or scalar object.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkwargs\n\nSame as keyword arguments to [execute][ibis.expr.types.core.Expr.execute]\n{}\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch().limit(5)\n&gt;&gt;&gt; t.to_pandas()\n  species     island  bill_length_mm  ...  body_mass_g     sex  year\n0  Adelie  Torgersen            39.1  ...       3750.0    male  2007\n1  Adelie  Torgersen            39.5  ...       3800.0  female  2007\n2  Adelie  Torgersen            40.3  ...       3250.0  female  2007\n3  Adelie  Torgersen             NaN  ...          NaN    None  2007\n4  Adelie  Torgersen            36.7  ...       3450.0  female  2007\n[5 rows x 8 columns]\n\n\n\ntry_cast\nexpr.types.generic.Value.try_cast(self, target_type)\nTry cast expression to indicated data type. If the cast fails for a row, the value is returned as null or NaN depending on target_type and backend behavior.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntarget_type\nibis.DataType\nType to try cast to\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nCasted expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"numbers\": [1, 2, 3, 4], \"strings\": [\"1.0\", \"2\", \"hello\", \"world\"]})\n&gt;&gt;&gt; t\n┏━━━━━━━━━┳━━━━━━━━━┓\n┃ numbers ┃ strings ┃\n┡━━━━━━━━━╇━━━━━━━━━┩\n│ int64   │ string  │\n├─────────┼─────────┤\n│       1 │ 1.0     │\n│       2 │ 2       │\n│       3 │ hello   │\n│       4 │ world   │\n└─────────┴─────────┘\n&gt;&gt;&gt; t = t.mutate(numbers_to_strings=_.numbers.try_cast(\"string\"))\n&gt;&gt;&gt; t = t.mutate(strings_to_numbers=_.strings.try_cast(\"int\"))\n&gt;&gt;&gt; t\n┏━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ numbers ┃ strings ┃ numbers_to_strings ┃ strings_to_numbers ┃\n┡━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ int64   │ string  │ string             │ int64              │\n├─────────┼─────────┼────────────────────┼────────────────────┤\n│       1 │ 1.0     │ 1                  │                  1 │\n│       2 │ 2       │ 2                  │                  2 │\n│       3 │ hello   │ 3                  │               NULL │\n│       4 │ world   │ 4                  │               NULL │\n└─────────┴─────────┴────────────────────┴────────────────────┘\n\n\n\ntype\nexpr.types.generic.Value.type(self)\nReturn the [DataType] of this expression.\n\n\ntypeof\nexpr.types.generic.Value.typeof(self)\nReturn the data type of the expression.\nThe values of the returned strings are necessarily backend dependent.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nA string indicating the type of the value"
  },
  {
    "objectID": "reference/expression-generic.html#methods-1",
    "href": "reference/expression-generic.html#methods-1",
    "title": "Generic Expression APIs",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\napprox_median\nReturn an approximate of the median of self.\n\n\napprox_nunique\nReturn the approximate number of distinct elements in self.\n\n\narbitrary\nSelect an arbitrary value in a column.\n\n\nargmax\nReturn the value of self that maximizes key.\n\n\nargmin\nReturn the value of self that minimizes key.\n\n\ncount\nCompute the number of rows in an expression.\n\n\ncume_dist\nReturn the cumulative distribution over a window.\n\n\ncummax\nReturn the cumulative max over a window.\n\n\ncummin\nReturn the cumulative min over a window.\n\n\ndense_rank\nPosition of first element within each group of equal values.\n\n\nfirst\nReturn the first value of a column.\n\n\nlag\nReturn the row located at offset rows before the current row.\n\n\nlast\nReturn the last value of a column.\n\n\nlead\nReturn the row located at offset rows after the current row.\n\n\nmax\nReturn the maximum of a column.\n\n\nmin\nReturn the minimum of a column.\n\n\nmode\nReturn the mode of a column.\n\n\nnth\nReturn the nth value (0-indexed) over a window.\n\n\nntile\nReturn the integer number of a partitioning of the column values.\n\n\nnunique\nCompute the number of distinct rows in an expression.\n\n\npercent_rank\nReturn the relative rank of the values in the column.\n\n\nrank\nCompute position of first element within each equal-value group in sorted order.\n\n\ntopk\nReturn a “top k” expression.\n\n\nvalue_counts\nCompute a frequency table.\n\n\n\n\napprox_median\nexpr.types.generic.Column.approx_median(self, where=None)\nReturn an approximate of the median of self.\n!!! info “The result may or may not be exact”\nWhether the result is an approximation depends on the backend.\n\n!!! warning \"Do not depend on the results being exact\"\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nibis.BooleanValue | None\nFilter in values when where is True\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nAn approximation of the median of self\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.body_mass_g.approx_median()\n4030\n&gt;&gt;&gt; t.body_mass_g.approx_median(where=t.species == \"Chinstrap\")\n3700\n\n\n\napprox_nunique\nexpr.types.generic.Column.approx_nunique(self, where=None)\nReturn the approximate number of distinct elements in self.\n!!! info “The result may or may not be exact”\nWhether the result is an approximation depends on the backend.\n\n!!! warning \"Do not depend on the results being exact\"\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nibis.BooleanValue | None\nFilter in values when where is True\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nAn approximate count of the distinct elements of self\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.body_mass_g.approx_nunique()\n94\n&gt;&gt;&gt; t.body_mass_g.approx_nunique(where=t.species == \"Adelie\")\n55\n\n\n\narbitrary\nexpr.types.generic.Column.arbitrary(self, where=None, how='first')\nSelect an arbitrary value in a column.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nibis.BooleanValue | None\nA filter expression\nNone\n\n\nhow\ntyping.Literal[‘first’, ‘last’, ‘heavy’]\nThe method to use for selecting the element. * \"first\": Select the first non-NULL element * \"last\": Select the last non-NULL element * \"heavy\": Select a frequently occurring value using the heavy hitters algorithm. \"heavy\" is only supported by Clickhouse backend.\n'first'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nAn expression\n\n\n\n\n\n\nargmax\nexpr.types.generic.Column.argmax(self, key, where=None)\nReturn the value of self that maximizes key.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nibis.BooleanValue | None\nFilter in values when where is True\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nThe value of self that maximizes key\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.species.argmax(t.body_mass_g)\n'Gentoo'\n&gt;&gt;&gt; t.species.argmax(t.body_mass_g, where=t.island == \"Dream\")\n'Chinstrap'\n\n\n\nargmin\nexpr.types.generic.Column.argmin(self, key, where=None)\nReturn the value of self that minimizes key.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nibis.BooleanValue | None\nFilter in values when where is True\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nThe value of self that minimizes key\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.species.argmin(t.body_mass_g)\n'Chinstrap'\n&gt;&gt;&gt; t.species.argmin(t.body_mass_g, where=t.island == \"Biscoe\")\n'Adelie'\n\n\n\ncount\nexpr.types.generic.Column.count(self, where=None)\nCompute the number of rows in an expression.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nibis.BooleanValue | None\nFilter expression\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerScalar\nNumber of elements in an expression\n\n\n\n\n\n\ncume_dist\nexpr.types.generic.Column.cume_dist(self)\nReturn the cumulative distribution over a window.\n\n\ncummax\nexpr.types.generic.Column.cummax(self)\nReturn the cumulative max over a window.\n\n\ncummin\nexpr.types.generic.Column.cummin(self)\nReturn the cumulative min over a window.\n\n\ndense_rank\nexpr.types.generic.Column.dense_rank(self)\nPosition of first element within each group of equal values.\nValues are returned in sorted order and duplicate values are ignored.\nEquivalent to SQL’s DENSE_RANK().\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerColumn\nThe rank\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 2, 1, 2, 3, 2]})\n&gt;&gt;&gt; t.mutate(rank=t.values.dense_rank())\n┏━━━━━━━━┳━━━━━━━┓\n┃ values ┃ rank  ┃\n┡━━━━━━━━╇━━━━━━━┩\n│ int64  │ int64 │\n├────────┼───────┤\n│      1 │     0 │\n│      1 │     0 │\n│      2 │     1 │\n│      2 │     1 │\n│      2 │     1 │\n│      3 │     2 │\n└────────┴───────┘\n\n\n\nfirst\nexpr.types.generic.Column.first(self, where=None)\nReturn the first value of a column.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"chars\": [\"a\", \"b\", \"c\", \"d\"]})\n&gt;&gt;&gt; t\n┏━━━━━━━━┓\n┃ chars  ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ a      │\n│ b      │\n│ c      │\n│ d      │\n└────────┘\n&gt;&gt;&gt; t.chars.first()\n'a'\n&gt;&gt;&gt; t.chars.first(where=t.chars != 'a')\n'b'\n\n\n\nlag\nexpr.types.generic.Column.lag(self, offset=None, default=None)\nReturn the row located at offset rows before the current row.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noffset\nint | ibis.IntegerValue | None\nIndex of row to select\nNone\n\n\ndefault\nibis.expr.types.generic.Value | None\nValue used if no row exists at offset\nNone\n\n\n\n\n\n\nlast\nexpr.types.generic.Column.last(self, where=None)\nReturn the last value of a column.\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"chars\": [\"a\", \"b\", \"c\", \"d\"]})\n&gt;&gt;&gt; t\n┏━━━━━━━━┓\n┃ chars  ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ a      │\n│ b      │\n│ c      │\n│ d      │\n└────────┘\n&gt;&gt;&gt; t.chars.last()\n'd'\n&gt;&gt;&gt; t.chars.last(where=t.chars != 'd')\n'c'\n\n\n\nlead\nexpr.types.generic.Column.lead(self, offset=None, default=None)\nReturn the row located at offset rows after the current row.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noffset\nint | ibis.IntegerValue | None\nIndex of row to select\nNone\n\n\ndefault\nibis.expr.types.generic.Value | None\nValue used if no row exists at offset\nNone\n\n\n\n\n\n\nmax\nexpr.types.generic.Column.max(self, where=None)\nReturn the maximum of a column.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nibis.BooleanValue | None\nFilter in values when where is True\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nThe maximum value in self\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.body_mass_g.max()\n6300\n&gt;&gt;&gt; t.body_mass_g.max(where=t.species == \"Chinstrap\")\n4800\n\n\n\nmin\nexpr.types.generic.Column.min(self, where=None)\nReturn the minimum of a column.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nibis.BooleanValue | None\nFilter in values when where is True\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nThe minimum value in self\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.body_mass_g.min()\n2700\n&gt;&gt;&gt; t.body_mass_g.min(where=t.species == \"Adelie\")\n2850\n\n\n\nmode\nexpr.types.generic.Column.mode(self, where=None)\nReturn the mode of a column.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nibis.BooleanValue | None\nFilter in values when where is True\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nThe mode of self\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.body_mass_g.mode()\n3800\n&gt;&gt;&gt; t.body_mass_g.mode(where=(t.species == \"Gentoo\") & (t.sex == \"male\"))\n5550\n\n\n\nnth\nexpr.types.generic.Column.nth(self, n)\nReturn the nth value (0-indexed) over a window.\n.nth(0) is equivalent to .first(). Negative will result in NULL. If the value of n is greater than the number of rows in the window, NULL will be returned.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint | ibis.IntegerValue\nDesired rank value\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nColumn\nThe nth value over a window\n\n\n\n\n\n\nntile\nexpr.types.generic.Column.ntile(self, buckets)\nReturn the integer number of a partitioning of the column values.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbuckets\nint | ibis.IntegerValue\nNumber of buckets to partition into\nrequired\n\n\n\n\n\n\nnunique\nexpr.types.generic.Column.nunique(self, where=None)\nCompute the number of distinct rows in an expression.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nibis.BooleanValue | None\nFilter expression\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerScalar\nNumber of distinct elements in an expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.body_mass_g.nunique()\n94\n&gt;&gt;&gt; t.body_mass_g.nunique(where=t.species == \"Adelie\")\n55\n\n\n\npercent_rank\nexpr.types.generic.Column.percent_rank(self)\nReturn the relative rank of the values in the column.\n\n\nrank\nexpr.types.generic.Column.rank(self)\nCompute position of first element within each equal-value group in sorted order.\nEquivalent to SQL’s RANK() window function.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nInt64Column\nThe min rank\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 2, 1, 2, 3, 2]})\n&gt;&gt;&gt; t.mutate(rank=t.values.rank())\n┏━━━━━━━━┳━━━━━━━┓\n┃ values ┃ rank  ┃\n┡━━━━━━━━╇━━━━━━━┩\n│ int64  │ int64 │\n├────────┼───────┤\n│      1 │     0 │\n│      1 │     0 │\n│      2 │     2 │\n│      2 │     2 │\n│      2 │     2 │\n│      3 │     5 │\n└────────┴───────┘\n\n\n\ntopk\nexpr.types.generic.Column.topk(self, k, by=None)\nReturn a “top k” expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nk\nint\nReturn this number of rows\nrequired\n\n\nby\nibis.Value | None\nAn expression. Defaults to count.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTableExpr\nA top-k expression\n\n\n\n\n\n\nvalue_counts\nexpr.types.generic.Column.value_counts(self)\nCompute a frequency table.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nFrequency table expression\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"chars\": char} for char in \"aabcddd\")\n&gt;&gt;&gt; t\n┏━━━━━━━━┓\n┃ chars  ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ a      │\n│ a      │\n│ b      │\n│ c      │\n│ d      │\n│ d      │\n│ d      │\n└────────┘\n&gt;&gt;&gt; t.chars.value_counts()\n┏━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ chars  ┃ chars_count ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━┩\n│ string │ int64       │\n├────────┼─────────────┤\n│ a      │           2 │\n│ b      │           1 │\n│ c      │           1 │\n│ d      │           3 │\n└────────┴─────────────┘"
  },
  {
    "objectID": "reference/expression-generic.html#methods-2",
    "href": "reference/expression-generic.html#methods-2",
    "title": "Generic Expression APIs",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nas_table\nPromote the scalar expression to a table.\n\n\n\n\nas_table\nexpr.types.generic.Scalar.as_table(self)\nPromote the scalar expression to a table.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA table expression\n\n\n\n\n\nExamples\nPromote an aggregation to a table\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.expr.types as ir\n&gt;&gt;&gt; t = ibis.table(dict(a=\"str\"), name=\"t\")\n&gt;&gt;&gt; expr = t.a.length().sum().name(\"len\").as_table()\n&gt;&gt;&gt; isinstance(expr, ir.Table)\nTrue\nPromote a literal value to a table\n&gt;&gt;&gt; import ibis.expr.types as ir\n&gt;&gt;&gt; lit = ibis.literal(1).name(\"a\").as_table()\n&gt;&gt;&gt; isinstance(lit, ir.Table)\nTrue"
  },
  {
    "objectID": "reference/expression-geospatial.html",
    "href": "reference/expression-geospatial.html",
    "title": "Geospatial Expressions",
    "section": "",
    "text": "Ibis supports the following geospatial expression APIs"
  },
  {
    "objectID": "reference/expression-geospatial.html#methods",
    "href": "reference/expression-geospatial.html#methods",
    "title": "Geospatial Expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\narea\nCompute the area of a geospatial value.\n\n\nas_binary\nGet the geometry as well-known bytes (WKB) without the SRID data.\n\n\nas_ewkb\nGet the geometry as well-known bytes (WKB) with the SRID data.\n\n\nas_ewkt\nGet the geometry as well-known text (WKT) with the SRID data.\n\n\nas_text\nGet the geometry as well-known text (WKT) without the SRID data.\n\n\nazimuth\nReturn the angle in radians from the horizontal of the vector defined by the inputs.\n\n\nbuffer\nReturn all points whose distance from this geometry is less than or equal to radius.\n\n\ncentroid\nReturns the centroid of the geometry.\n\n\ncontains\nCheck if the geometry contains the right.\n\n\ncontains_properly\nCheck if the first geometry contains the second one.\n\n\ncovered_by\nCheck if the first geometry is covered by the second one.\n\n\ncovers\nCheck if the first geometry covers the second one.\n\n\ncrosses\nCheck if the geometries have at least one interior point in common.\n\n\nd_fully_within\nCheck if self is entirely within distance from right.\n\n\nd_within\nCheck if self is partially within distance from right.\n\n\ndifference\nReturn the difference of two geometries.\n\n\ndisjoint\nCheck if the geometries have no points in common.\n\n\ndistance\nCompute the distance between two geospatial expressions.\n\n\nend_point\nReturn the last point of a LINESTRING geometry as a POINT.\n\n\nenvelope\nReturns a geometry representing the bounding box of self.\n\n\ngeo_equals\nCheck if the geometries are equal.\n\n\ngeometry_n\nGet the 1-based Nth geometry of a multi geometry.\n\n\ngeometry_type\nGet the type of a geometry.\n\n\nintersection\nReturn the intersection of two geometries.\n\n\nintersects\nCheck if the geometries share any points.\n\n\nis_valid\nCheck if the geometry is valid.\n\n\nlength\nCompute the length of a geospatial expression.\n\n\nline_locate_point\nLocate the distance a point falls along the length of a line.\n\n\nline_merge\nMerge a MultiLineString into a LineString.\n\n\nline_substring\nClip a substring from a LineString.\n\n\nmax_distance\nReturns the 2-dimensional max distance between two geometries in projected units.\n\n\nn_points\nReturn the number of points in a geometry. Works for all geometries.\n\n\nn_rings\nReturn the number of rings for polygons and multipolygons.\n\n\nordering_equals\nCheck if two geometries are equal and have the same point ordering.\n\n\noverlaps\nCheck if the geometries share space, have the same dimension, and are not completely contained by each other.\n\n\nperimeter\nCompute the perimeter of a geospatial expression.\n\n\npoint_n\nReturn the Nth point in a single linestring in the geometry.\n\n\nset_srid\nSet the spatial reference identifier for the ST_Geometry.\n\n\nsimplify\nSimplify a given geometry.\n\n\nsrid\nReturn the spatial reference identifier for the ST_Geometry.\n\n\nstart_point\nReturn the first point of a LINESTRING geometry as a POINT.\n\n\ntouches\nCheck if the geometries have at least one point in common, but do not intersect.\n\n\ntransform\nTransform a geometry into a new SRID.\n\n\nunion\nMerge two geometries into a union geometry.\n\n\nwithin\nCheck if the first geometry is completely inside of the second.\n\n\nx\nReturn the X coordinate of self, or NULL if not available.\n\n\nx_max\nReturn the X maxima of a geometry.\n\n\nx_min\nReturn the X minima of a geometry.\n\n\ny\nReturn the Y coordinate of self, or NULL if not available.\n\n\ny_max\nReturn the Y maxima of a geometry.\n\n\ny_min\nReturn the Y minima of a geometry.\n\n\n\n\narea\nexpr.types.geospatial.GeoSpatialValue.area(self)\nCompute the area of a geospatial value.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nThe area of self\n\n\n\n\n\n\nas_binary\nexpr.types.geospatial.GeoSpatialValue.as_binary(self)\nGet the geometry as well-known bytes (WKB) without the SRID data.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBinaryValue\nBinary value\n\n\n\n\n\n\nas_ewkb\nexpr.types.geospatial.GeoSpatialValue.as_ewkb(self)\nGet the geometry as well-known bytes (WKB) with the SRID data.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBinaryValue\nWKB value\n\n\n\n\n\n\nas_ewkt\nexpr.types.geospatial.GeoSpatialValue.as_ewkt(self)\nGet the geometry as well-known text (WKT) with the SRID data.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nString value\n\n\n\n\n\n\nas_text\nexpr.types.geospatial.GeoSpatialValue.as_text(self)\nGet the geometry as well-known text (WKT) without the SRID data.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nString value\n\n\n\n\n\n\nazimuth\nexpr.types.geospatial.GeoSpatialValue.azimuth(self, right)\nReturn the angle in radians from the horizontal of the vector defined by the inputs.\nAngle is computed clockwise from down-to-up on the clock: 12=0; 3=PI/2; 6=PI; 9=3PI/2.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nazimuth\n\n\n\n\n\n\nbuffer\nexpr.types.geospatial.GeoSpatialValue.buffer(self, radius)\nReturn all points whose distance from this geometry is less than or equal to radius.\nCalculations are in the Spatial Reference System of this Geometry.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nradius\nfloat | ibis.FloatingValue\nFloating expression\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nGeometry expression\n\n\n\n\n\n\ncentroid\nexpr.types.geospatial.GeoSpatialValue.centroid(self)\nReturns the centroid of the geometry.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nPointValue\nThe centroid\n\n\n\n\n\n\ncontains\nexpr.types.geospatial.GeoSpatialValue.contains(self, right)\nCheck if the geometry contains the right.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self contains right\n\n\n\n\n\n\ncontains_properly\nexpr.types.geospatial.GeoSpatialValue.contains_properly(self, right)\nCheck if the first geometry contains the second one.\nExcludes common border points.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self contains right excluding border points.\n\n\n\n\n\n\ncovered_by\nexpr.types.geospatial.GeoSpatialValue.covered_by(self, right)\nCheck if the first geometry is covered by the second one.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self is covered by right\n\n\n\n\n\n\ncovers\nexpr.types.geospatial.GeoSpatialValue.covers(self, right)\nCheck if the first geometry covers the second one.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self covers right\n\n\n\n\n\n\ncrosses\nexpr.types.geospatial.GeoSpatialValue.crosses(self, right)\nCheck if the geometries have at least one interior point in common.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self and right have at least one common interior point.\n\n\n\n\n\n\nd_fully_within\nexpr.types.geospatial.GeoSpatialValue.d_fully_within(self, right, distance)\nCheck if self is entirely within distance from right.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\ndistance\nibis.FloatingValue\nDistance to check\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self is within a specified distance from right.\n\n\n\n\n\n\nd_within\nexpr.types.geospatial.GeoSpatialValue.d_within(self, right, distance)\nCheck if self is partially within distance from right.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\ndistance\nibis.FloatingValue\nDistance to check\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self is partially within distance from right.\n\n\n\n\n\n\ndifference\nexpr.types.geospatial.GeoSpatialValue.difference(self, right)\nReturn the difference of two geometries.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nDifference of self and right\n\n\n\n\n\n\ndisjoint\nexpr.types.geospatial.GeoSpatialValue.disjoint(self, right)\nCheck if the geometries have no points in common.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self and right are disjoint\n\n\n\n\n\n\ndistance\nexpr.types.geospatial.GeoSpatialValue.distance(self, right)\nCompute the distance between two geospatial expressions.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry or geography\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nDistance between self and right\n\n\n\n\n\n\nend_point\nexpr.types.geospatial.GeoSpatialValue.end_point(self)\nReturn the last point of a LINESTRING geometry as a POINT.\nReturn NULL if the input parameter is not a LINESTRING\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nPointValue\nEnd point\n\n\n\n\n\n\nenvelope\nexpr.types.geospatial.GeoSpatialValue.envelope(self)\nReturns a geometry representing the bounding box of self.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nPolygonValue\nA polygon\n\n\n\n\n\n\ngeo_equals\nexpr.types.geospatial.GeoSpatialValue.geo_equals(self, right)\nCheck if the geometries are equal.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self equals right\n\n\n\n\n\n\ngeometry_n\nexpr.types.geospatial.GeoSpatialValue.geometry_n(self, n)\nGet the 1-based Nth geometry of a multi geometry.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint | ibis.IntegerValue\nNth geometry index\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nGeometry value\n\n\n\n\n\n\ngeometry_type\nexpr.types.geospatial.GeoSpatialValue.geometry_type(self)\nGet the type of a geometry.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nString representing the type of self.\n\n\n\n\n\n\nintersection\nexpr.types.geospatial.GeoSpatialValue.intersection(self, right)\nReturn the intersection of two geometries.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nIntersection of self and right\n\n\n\n\n\n\nintersects\nexpr.types.geospatial.GeoSpatialValue.intersects(self, right)\nCheck if the geometries share any points.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self intersects right\n\n\n\n\n\n\nis_valid\nexpr.types.geospatial.GeoSpatialValue.is_valid(self)\nCheck if the geometry is valid.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self is valid\n\n\n\n\n\n\nlength\nexpr.types.geospatial.GeoSpatialValue.length(self)\nCompute the length of a geospatial expression.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nLength of self\n\n\n\n\n\n\nline_locate_point\nexpr.types.geospatial.GeoSpatialValue.line_locate_point(self, right)\nLocate the distance a point falls along the length of a line.\nReturns a float between zero and one representing the location of the closest point on the linestring to the given point, as a fraction of the total 2d line length.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nPointValue\nPoint geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nFraction of the total line length\n\n\n\n\n\n\nline_merge\nexpr.types.geospatial.GeoSpatialValue.line_merge(self)\nMerge a MultiLineString into a LineString.\nReturns a (set of) LineString(s) formed by sewing together the constituent line work of a MultiLineString. If a geometry other than a LineString or MultiLineString is given, this will return an empty geometry collection.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nMerged linestrings\n\n\n\n\n\n\nline_substring\nexpr.types.geospatial.GeoSpatialValue.line_substring(self, start, end)\nClip a substring from a LineString.\nReturns a linestring that is a substring of the input one, starting and ending at the given fractions of the total 2d length. The second and third arguments are floating point values between zero and one. This only works with linestrings.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart\nibis.FloatingValue\nStart value\nrequired\n\n\nend\nibis.FloatingValue\nEnd value\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nLineStringValue\nClipped linestring\n\n\n\n\n\n\nmax_distance\nexpr.types.geospatial.GeoSpatialValue.max_distance(self, right)\nReturns the 2-dimensional max distance between two geometries in projected units.\nIf self and right are the same geometry the function will return the distance between the two vertices most far from each other in that geometry.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nMaximum distance\n\n\n\n\n\n\nn_points\nexpr.types.geospatial.GeoSpatialValue.n_points(self)\nReturn the number of points in a geometry. Works for all geometries.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nNumber of points\n\n\n\n\n\n\nn_rings\nexpr.types.geospatial.GeoSpatialValue.n_rings(self)\nReturn the number of rings for polygons and multipolygons.\nOuter rings are counted as well.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nNumber of rings\n\n\n\n\n\n\nordering_equals\nexpr.types.geospatial.GeoSpatialValue.ordering_equals(self, right)\nCheck if two geometries are equal and have the same point ordering.\nReturns true if the two geometries are equal and the coordinates are in the same order.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether points and orderings are equal.\n\n\n\n\n\n\noverlaps\nexpr.types.geospatial.GeoSpatialValue.overlaps(self, right)\nCheck if the geometries share space, have the same dimension, and are not completely contained by each other.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nOverlaps indicator\n\n\n\n\n\n\nperimeter\nexpr.types.geospatial.GeoSpatialValue.perimeter(self)\nCompute the perimeter of a geospatial expression.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nPerimeter of self\n\n\n\n\n\n\npoint_n\nexpr.types.geospatial.GeoSpatialValue.point_n(self, n)\nReturn the Nth point in a single linestring in the geometry.\nNegative values are counted backwards from the end of the LineString, so that -1 is the last point. Returns NULL if there is no linestring in the geometry.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nibis.IntegerValue\nNth point index\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nPointValue\nNth point in self\n\n\n\n\n\n\nset_srid\nexpr.types.geospatial.GeoSpatialValue.set_srid(self, srid)\nSet the spatial reference identifier for the ST_Geometry.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsrid\nibis.IntegerValue\nSRID integer value\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nself with SRID set to srid\n\n\n\n\n\n\nsimplify\nexpr.types.geospatial.GeoSpatialValue.simplify(self, tolerance, preserve_collapsed)\nSimplify a given geometry.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntolerance\nibis.FloatingValue\nTolerance\nrequired\n\n\npreserve_collapsed\nibis.BooleanValue\nWhether to preserve collapsed geometries\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nSimplified geometry\n\n\n\n\n\n\nsrid\nexpr.types.geospatial.GeoSpatialValue.srid(self)\nReturn the spatial reference identifier for the ST_Geometry.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nSRID\n\n\n\n\n\n\nstart_point\nexpr.types.geospatial.GeoSpatialValue.start_point(self)\nReturn the first point of a LINESTRING geometry as a POINT.\nReturn NULL if the input parameter is not a LINESTRING\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nPointValue\nStart point\n\n\n\n\n\n\ntouches\nexpr.types.geospatial.GeoSpatialValue.touches(self, right)\nCheck if the geometries have at least one point in common, but do not intersect.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self and right are touching\n\n\n\n\n\n\ntransform\nexpr.types.geospatial.GeoSpatialValue.transform(self, srid)\nTransform a geometry into a new SRID.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsrid\nibis.IntegerValue\nInteger expression\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nTransformed geometry\n\n\n\n\n\n\nunion\nexpr.types.geospatial.GeoSpatialValue.union(self, right)\nMerge two geometries into a union geometry.\nReturns the pointwise union of the two geometries. This corresponds to the non-aggregate version the PostGIS ST_Union.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nUnion of geometries\n\n\n\n\n\n\nwithin\nexpr.types.geospatial.GeoSpatialValue.within(self, right)\nCheck if the first geometry is completely inside of the second.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self is in right.\n\n\n\n\n\n\nx\nexpr.types.geospatial.GeoSpatialValue.x(self)\nReturn the X coordinate of self, or NULL if not available.\nInput must be a point.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nX coordinate of self\n\n\n\n\n\n\nx_max\nexpr.types.geospatial.GeoSpatialValue.x_max(self)\nReturn the X maxima of a geometry.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nX maxima\n\n\n\n\n\n\nx_min\nexpr.types.geospatial.GeoSpatialValue.x_min(self)\nReturn the X minima of a geometry.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nX minima\n\n\n\n\n\n\ny\nexpr.types.geospatial.GeoSpatialValue.y(self)\nReturn the Y coordinate of self, or NULL if not available.\nInput must be a point.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nY coordinate of self\n\n\n\n\n\n\ny_max\nexpr.types.geospatial.GeoSpatialValue.y_max(self)\nReturn the Y maxima of a geometry.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nY maxima\n\n\n\n\n\n\ny_min\nexpr.types.geospatial.GeoSpatialValue.y_min(self)\nReturn the Y minima of a geometry.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nY minima"
  },
  {
    "objectID": "reference/expression-geospatial.html#methods-1",
    "href": "reference/expression-geospatial.html#methods-1",
    "title": "Geospatial Expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nunary_union\nAggregate a set of geometries into a union.\n\n\n\n\nunary_union\nexpr.types.geospatial.GeoSpatialColumn.unary_union(self)\nAggregate a set of geometries into a union.\nThis corresponds to the aggregate version of the PostGIS ST_Union. We give it a different name (following the corresponding method in GeoPandas) to avoid name conflicts with the non-aggregate version.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialScalar\nUnion of geometries"
  },
  {
    "objectID": "reference/temporal.html",
    "href": "reference/temporal.html",
    "title": "temporal",
    "section": "",
    "text": "expr.types.temporal\n\n\n\n\n\nName\nDescription\n\n\n\n\nDayOfWeek\nA namespace of methods for extracting day of week information.\n\n\n\n\n\nexpr.types.temporal.DayOfWeek(self, expr)\nA namespace of methods for extracting day of week information.\n\n\n\n\n\nName\nDescription\n\n\n\n\nfull_name\nGet the name of the day of the week.\n\n\nindex\nGet the index of the day of the week.\n\n\n\n\n\nexpr.types.temporal.DayOfWeek.full_name(self)\nGet the name of the day of the week.\n\n\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nThe name of the day of the week\n\n\n\n\n\n\n\nexpr.types.temporal.DayOfWeek.index(self)\nGet the index of the day of the week.\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nThe index of the day of the week. !!! note “Ibis follows pandas’ conventions for day numbers: Monday = 0 and Sunday = 6.”"
  },
  {
    "objectID": "reference/temporal.html#classes",
    "href": "reference/temporal.html#classes",
    "title": "temporal",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nDayOfWeek\nA namespace of methods for extracting day of week information.\n\n\n\n\n\nexpr.types.temporal.DayOfWeek(self, expr)\nA namespace of methods for extracting day of week information.\n\n\n\n\n\nName\nDescription\n\n\n\n\nfull_name\nGet the name of the day of the week.\n\n\nindex\nGet the index of the day of the week.\n\n\n\n\n\nexpr.types.temporal.DayOfWeek.full_name(self)\nGet the name of the day of the week.\n\n\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nThe name of the day of the week\n\n\n\n\n\n\n\nexpr.types.temporal.DayOfWeek.index(self)\nGet the index of the day of the week.\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nThe index of the day of the week. !!! note “Ibis follows pandas’ conventions for day numbers: Monday = 0 and Sunday = 6.”"
  },
  {
    "objectID": "blog/ibis_substrait_to_duckdb.html",
    "href": "blog/ibis_substrait_to_duckdb.html",
    "title": "Ibis + Substrait + DuckDB",
    "section": "",
    "text": "by Gil Forsyth\nIbis strives to provide a consistent interface for interacting with a multitude of different analytical execution engines, most of which (but not all) speak some dialect of SQL.\nToday, Ibis accomplishes this with a lot of help from sqlalchemy and sqlglot to handle differences in dialect, or we interact directly with available Python bindings (for instance with the pandas, datafusion, and polars backends).\nIbis goes to great lengths to generate sane and consistent SQL for those backends that use it. We are also interested in exploring other means of communicating consistently with those backends.\nSubstrait is a new cross-language serialization format for communicating (among other things) query plans. It’s still in its early days, but there is already nascent support for Substrait in Apache Arrow, DuckDB, and Velox.\nIbis supports producing Substrait plans from Ibis table expressions, with the help of the ibis-substrait library. Let’s take a quick peek at how we might use it for query execution.\n\n\nFirst, we can create a conda environment using the latest versions of duckdb, ibis, and ibis_substrait.\nmamba create -n ibis_substrait_duckdb ibis-framework==4.1 ibis-substrait==2.19 ipython python-duckdb parsy==2\nNext, we’ll need to choose a dataset. For this example, we’ll use data from IMDB, available through their dataset portal.\nFor convenience, I used Ready, Set, Data! to grab the data in parquet format and then insert it into a DuckDB database.\nimport duckdb\ncon = duckdb.connect(\"/home/gil/imdb.ddb\")\ncon.execute(\n    \"CREATE TABLE ratings AS SELECT * FROM '/home/gil/data/imdb/imdb_ratings.parquet'\"\n)\ncon.execute(\n    \"CREATE TABLE basics AS SELECT * FROM '/home/gil/data/imdb/imdb_basics.parquet'\"\n)\n\n\n\nFor our example, we’ll build up a query using Ibis but without connecting to our execution engine (DuckDB). Once we have an Ibis table expression, we’ll create a Substrait plan, then execute that plan directly on DuckDB to get results.\nTo do this, all we need is some knowledge of the schema of the tables we want to interact with. We might get these schema from a metadata store, or possibly a coworker, or a friendly mouse.\nHowever we arrive at it, if we know the column names and the datatypes, we can build up a query in Ibis, so let’s do that.\nimport ibis\nfrom ibis import _\n\nratings = ibis.table(\n    [\n        (\"tconst\", \"str\"),\n        (\"averageRating\", \"str\"),\n        (\"numVotes\", \"str\"),\n    ],\n    name=\"ratings\",\n)\n\nbasics = ibis.table(\n    [\n        (\"tconst\", \"str\"),\n        (\"titleType\", \"str\"),\n        (\"primaryTitle\", \"str\"),\n        (\"originalTitle\", \"str\"),\n        (\"isAdult\", \"str\"),\n        (\"startYear\", \"str\"),\n        (\"endYear\", \"str\"),\n        (\"runtimeMinutes\", \"str\"),\n        (\"genres\", \"str\"),\n    ],\n    name=\"basics\",\n)\nNow that those tables are represented in Ibis, we can start creating our query. We’ll try to recreate the top-ten movies on the IMDB leaderboard. For that, we’ll need movie titles and their respective ratings.\nWe know that the data we have for ratings looks something like the following:\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ tconst    ┃ averageRating ┃ numVotes ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string    │ string        │ string   │\n├───────────┼───────────────┼──────────┤\n│ tt0000001 │ 5.7           │ 1919\\n   │\n│ tt0000002 │ 5.8           │ 260\\n    │\n│ tt0000003 │ 6.5           │ 1726\\n   │\n│ tt0000004 │ 5.6           │ 173\\n    │\n│ tt0000005 │ 6.2           │ 2541\\n   │\n└───────────┴───────────────┴──────────┘\nBased on the column names alone, averageRating is almost certainly supposed to be a float, and numVotes should be an integer. We can cast those so we can make useful comparisons between ratings and vote numbers.\nratings = ratings.select(\n    ratings.tconst,\n    avg_rating=ratings.averageRating.cast(\"float\"),\n    num_votes=ratings.numVotes.cast(\"int\"),\n)\nThe first few rows of basics looks like this:\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━┓\n┃ tconst    ┃ titleType ┃ primaryTitle           ┃ originalTitle          ┃ isAdult ┃ startYear ┃ … ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━┩\n│ string    │ string    │ string                 │ string                 │ string  │ string    │ … │\n├───────────┼───────────┼────────────────────────┼────────────────────────┼─────────┼───────────┼───┤\n│ tt0000001 │ short     │ Carmencita             │ Carmencita             │ 0       │ 1894      │ … │\n│ tt0000002 │ short     │ Le clown et ses chiens │ Le clown et ses chiens │ 0       │ 1892      │ … │\n│ tt0000003 │ short     │ Pauvre Pierrot         │ Pauvre Pierrot         │ 0       │ 1892      │ … │\n│ tt0000004 │ short     │ Un bon bock            │ Un bon bock            │ 0       │ 1892      │ … │\n│ tt0000005 │ short     │ Blacksmith Scene       │ Blacksmith Scene       │ 0       │ 1893      │ … │\n└───────────┴───────────┴────────────────────────┴────────────────────────┴─────────┴───────────┴───┘\nIn the interest of keeping things family-friendly, we can filter out any adult films. We can filter out any IMDB titles that aren’t movies, then select out the columns tconst and primaryTitle. And we’ll include startYear just in case it’s interesting.\nbasics = basics.filter([basics.titleType == \"movie\", basics.isAdult == \"0\"]).select(\n    \"tconst\",\n    \"primaryTitle\",\n    \"startYear\",\n)\nWith the data (lightly) cleaned up, we can construct our query for top films. We want to join the two tables ratings and basics. Then we’ll order them by avg_rating and num_votes, and include an additional filter that the movie has to have at least 200,000 votes.\ntopfilms = (\n    ratings.join(basics, \"tconst\")\n    .order_by([_.avg_rating.desc(), _.num_votes.desc()])\n    .filter(_.num_votes &gt; 200_000)\n    .limit(10)\n)\nNow that we have an Ibis table expression, it’s time for Substrait to enter the scene.\n\n\n\nWe’re going to import ibis_substrait and compile the topfilms table expression into a Substrait plan.\nfrom ibis_substrait.compiler.core import SubstraitCompiler\n\ncompiler = SubstraitCompiler()\n\nplan = compiler.compile(topfilms)\n\n# type(plan) --&gt; &lt;class 'substrait.ibis.plan_pb2.Plan'&gt;\nSubstrait is built using protobuf. If you look at the repr for plan, you’ll see a LOOOONG JSON-ish representation of the Substrait plan. This representation is not really meant for human eyes.\nWe’ll serialize the Substrait plan to disk and open it up in a separate session, or on another machine, entirely. That’s one of the notions of Substrait: plans can be serialized and shuttled around between various systems. It’s similar to Ibis in that it allows a separation of plan creation from plan execution.\nwith open(\"topfilms.proto\", \"wb\") as f:\n    f.write(plan.SerializeToString())\n\n\n\nNow we can open up the serialized Substrait plan in a new session where we execute it using DuckDB directly. One important point to note here is that our plan refers to two tables, named basics and ratings. If those tables don’t exist in our execution engine, then this isn’t going to work.\nimport duckdb\n\ncon = duckdb.connect(\"/home/gil/imdb.ddb\")\n\ncon.execute(\"PRAGMA show_tables;\").fetchall()\n\n\n\n\n\n\n\n\nbasics\n\n\n\n\nratings\n\n\n\n\n\nLuckily, they do exist! Let’s install and load the DuckDB Substrait extension, then execute the Substrait plan, and finally grab our results.\ncon.install_extension(\"substrait\")\ncon.load_extension(\"substrait\")\n\nwith open(\"topfilms.proto\", \"rb\") as f:\n    plan_blob = f.read()\n\nresult = con.from_substrait(plan_blob)\n\nresult.fetchall()\n\n\n\n\n\n\n\n\n\n\n\n\ntt0111161\n\n\n9.3\n\n\n2651547\n\n\nThe Shawshank Redemption\n\n\n1994\n\n\n\n\ntt0068646\n\n\n9.2\n\n\n1838044\n\n\nThe Godfather\n\n\n1972\n\n\n\n\ntt0468569\n\n\n9.0\n\n\n2623735\n\n\nThe Dark Knight\n\n\n2008\n\n\n\n\ntt0167260\n\n\n9.0\n\n\n1827464\n\n\nThe Lord of the Rings: The Return of the King\n\n\n2003\n\n\n\n\ntt0108052\n\n\n9.0\n\n\n1343647\n\n\nSchindler’s List\n\n\n1993\n\n\n\n\ntt0071562\n\n\n9.0\n\n\n1259465\n\n\nThe Godfather Part II\n\n\n1974\n\n\n\n\ntt0050083\n\n\n9.0\n\n\n782903\n\n\n12 Angry Men\n\n\n1957\n\n\n\n\ntt0110912\n\n\n8.9\n\n\n2029684\n\n\nPulp Fiction\n\n\n1994\n\n\n\n\ntt1375666\n\n\n8.8\n\n\n2325417\n\n\nInception\n\n\n2010\n\n\n\n\ntt0137523\n\n\n8.8\n\n\n2096752\n\n\nFight Club\n\n\n1999\n\n\n\n\n\nThat looks about right to me. There may be some small differences with the current Top 10 list on IMDB if our data are a little stale.\nIt’s early days still for Substrait, but it’s exciting to see how far it’s come in the last 18 months!\n\n\n\nIt’s a fair question. SQL is everywhere, after all.\nThere are a few reasons we think you shouldn’t ignore Substrait.\n\n\nSQL has a standard, but how closely do engines follow the standard? In our experience, queries don’t translate well between engines (this is one reason Ibis exists!)\n\n\n\nSubstrait is more extensible than SQL. Some DBMS have added in some very cool features, but it usually involves diverging (sometimes widely) from the SQL standard. Substrait has an extension system that allows plan producers and plan consumers to agree on a well-typed and well-defined interaction that exists outside of the core Substrait specification.\n\n\n\nParsing SQL can be a big pain (trust us). If you send a big string over the wire, you need the engine on the other side to have a SQL parser to understand what the message is. Now, obviously, SQL engines have those. But here, again, standards (or lack of adherence to standards) can bite you. Extensibility is also difficult here, because now the SQL parser needs to understand some new custom syntax.\nProtobuf is hardly a dream to work with, but it’s a lot easier to consistently define behavior AND to validate that behavior is correct. It’s also smaller than raw text.\n\n\n\n\nThat’s all for now! To quickly summarize:\nSubstrait is a new standard for representing relational algebra queries with support in Apache Arrow, DuckDB, Velox, and more (and more to come!).\nIbis can now generate substrait instead of string SQL, letting it take advantage of this new standard.\nInterested in substrait or ibis? Docs are available at\n\nSubstrait\nIbis Docs\n\nand the relevant GitHub repos are\n\nSubstrait GitHub\nIbis Substrait GitHub\nIbis GitHub\n\nPlease feel free to reach out on GitHub!"
  },
  {
    "objectID": "blog/ibis_substrait_to_duckdb.html#getting-started",
    "href": "blog/ibis_substrait_to_duckdb.html#getting-started",
    "title": "Ibis + Substrait + DuckDB",
    "section": "",
    "text": "First, we can create a conda environment using the latest versions of duckdb, ibis, and ibis_substrait.\nmamba create -n ibis_substrait_duckdb ibis-framework==4.1 ibis-substrait==2.19 ipython python-duckdb parsy==2\nNext, we’ll need to choose a dataset. For this example, we’ll use data from IMDB, available through their dataset portal.\nFor convenience, I used Ready, Set, Data! to grab the data in parquet format and then insert it into a DuckDB database.\nimport duckdb\ncon = duckdb.connect(\"/home/gil/imdb.ddb\")\ncon.execute(\n    \"CREATE TABLE ratings AS SELECT * FROM '/home/gil/data/imdb/imdb_ratings.parquet'\"\n)\ncon.execute(\n    \"CREATE TABLE basics AS SELECT * FROM '/home/gil/data/imdb/imdb_basics.parquet'\"\n)"
  },
  {
    "objectID": "blog/ibis_substrait_to_duckdb.html#query-creation",
    "href": "blog/ibis_substrait_to_duckdb.html#query-creation",
    "title": "Ibis + Substrait + DuckDB",
    "section": "",
    "text": "For our example, we’ll build up a query using Ibis but without connecting to our execution engine (DuckDB). Once we have an Ibis table expression, we’ll create a Substrait plan, then execute that plan directly on DuckDB to get results.\nTo do this, all we need is some knowledge of the schema of the tables we want to interact with. We might get these schema from a metadata store, or possibly a coworker, or a friendly mouse.\nHowever we arrive at it, if we know the column names and the datatypes, we can build up a query in Ibis, so let’s do that.\nimport ibis\nfrom ibis import _\n\nratings = ibis.table(\n    [\n        (\"tconst\", \"str\"),\n        (\"averageRating\", \"str\"),\n        (\"numVotes\", \"str\"),\n    ],\n    name=\"ratings\",\n)\n\nbasics = ibis.table(\n    [\n        (\"tconst\", \"str\"),\n        (\"titleType\", \"str\"),\n        (\"primaryTitle\", \"str\"),\n        (\"originalTitle\", \"str\"),\n        (\"isAdult\", \"str\"),\n        (\"startYear\", \"str\"),\n        (\"endYear\", \"str\"),\n        (\"runtimeMinutes\", \"str\"),\n        (\"genres\", \"str\"),\n    ],\n    name=\"basics\",\n)\nNow that those tables are represented in Ibis, we can start creating our query. We’ll try to recreate the top-ten movies on the IMDB leaderboard. For that, we’ll need movie titles and their respective ratings.\nWe know that the data we have for ratings looks something like the following:\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ tconst    ┃ averageRating ┃ numVotes ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string    │ string        │ string   │\n├───────────┼───────────────┼──────────┤\n│ tt0000001 │ 5.7           │ 1919\\n   │\n│ tt0000002 │ 5.8           │ 260\\n    │\n│ tt0000003 │ 6.5           │ 1726\\n   │\n│ tt0000004 │ 5.6           │ 173\\n    │\n│ tt0000005 │ 6.2           │ 2541\\n   │\n└───────────┴───────────────┴──────────┘\nBased on the column names alone, averageRating is almost certainly supposed to be a float, and numVotes should be an integer. We can cast those so we can make useful comparisons between ratings and vote numbers.\nratings = ratings.select(\n    ratings.tconst,\n    avg_rating=ratings.averageRating.cast(\"float\"),\n    num_votes=ratings.numVotes.cast(\"int\"),\n)\nThe first few rows of basics looks like this:\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━┓\n┃ tconst    ┃ titleType ┃ primaryTitle           ┃ originalTitle          ┃ isAdult ┃ startYear ┃ … ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━┩\n│ string    │ string    │ string                 │ string                 │ string  │ string    │ … │\n├───────────┼───────────┼────────────────────────┼────────────────────────┼─────────┼───────────┼───┤\n│ tt0000001 │ short     │ Carmencita             │ Carmencita             │ 0       │ 1894      │ … │\n│ tt0000002 │ short     │ Le clown et ses chiens │ Le clown et ses chiens │ 0       │ 1892      │ … │\n│ tt0000003 │ short     │ Pauvre Pierrot         │ Pauvre Pierrot         │ 0       │ 1892      │ … │\n│ tt0000004 │ short     │ Un bon bock            │ Un bon bock            │ 0       │ 1892      │ … │\n│ tt0000005 │ short     │ Blacksmith Scene       │ Blacksmith Scene       │ 0       │ 1893      │ … │\n└───────────┴───────────┴────────────────────────┴────────────────────────┴─────────┴───────────┴───┘\nIn the interest of keeping things family-friendly, we can filter out any adult films. We can filter out any IMDB titles that aren’t movies, then select out the columns tconst and primaryTitle. And we’ll include startYear just in case it’s interesting.\nbasics = basics.filter([basics.titleType == \"movie\", basics.isAdult == \"0\"]).select(\n    \"tconst\",\n    \"primaryTitle\",\n    \"startYear\",\n)\nWith the data (lightly) cleaned up, we can construct our query for top films. We want to join the two tables ratings and basics. Then we’ll order them by avg_rating and num_votes, and include an additional filter that the movie has to have at least 200,000 votes.\ntopfilms = (\n    ratings.join(basics, \"tconst\")\n    .order_by([_.avg_rating.desc(), _.num_votes.desc()])\n    .filter(_.num_votes &gt; 200_000)\n    .limit(10)\n)\nNow that we have an Ibis table expression, it’s time for Substrait to enter the scene."
  },
  {
    "objectID": "blog/ibis_substrait_to_duckdb.html#substrait-serialization",
    "href": "blog/ibis_substrait_to_duckdb.html#substrait-serialization",
    "title": "Ibis + Substrait + DuckDB",
    "section": "",
    "text": "We’re going to import ibis_substrait and compile the topfilms table expression into a Substrait plan.\nfrom ibis_substrait.compiler.core import SubstraitCompiler\n\ncompiler = SubstraitCompiler()\n\nplan = compiler.compile(topfilms)\n\n# type(plan) --&gt; &lt;class 'substrait.ibis.plan_pb2.Plan'&gt;\nSubstrait is built using protobuf. If you look at the repr for plan, you’ll see a LOOOONG JSON-ish representation of the Substrait plan. This representation is not really meant for human eyes.\nWe’ll serialize the Substrait plan to disk and open it up in a separate session, or on another machine, entirely. That’s one of the notions of Substrait: plans can be serialized and shuttled around between various systems. It’s similar to Ibis in that it allows a separation of plan creation from plan execution.\nwith open(\"topfilms.proto\", \"wb\") as f:\n    f.write(plan.SerializeToString())"
  },
  {
    "objectID": "blog/ibis_substrait_to_duckdb.html#substrait-plan-execution",
    "href": "blog/ibis_substrait_to_duckdb.html#substrait-plan-execution",
    "title": "Ibis + Substrait + DuckDB",
    "section": "",
    "text": "Now we can open up the serialized Substrait plan in a new session where we execute it using DuckDB directly. One important point to note here is that our plan refers to two tables, named basics and ratings. If those tables don’t exist in our execution engine, then this isn’t going to work.\nimport duckdb\n\ncon = duckdb.connect(\"/home/gil/imdb.ddb\")\n\ncon.execute(\"PRAGMA show_tables;\").fetchall()\n\n\n\n\n\n\n\n\nbasics\n\n\n\n\nratings\n\n\n\n\n\nLuckily, they do exist! Let’s install and load the DuckDB Substrait extension, then execute the Substrait plan, and finally grab our results.\ncon.install_extension(\"substrait\")\ncon.load_extension(\"substrait\")\n\nwith open(\"topfilms.proto\", \"rb\") as f:\n    plan_blob = f.read()\n\nresult = con.from_substrait(plan_blob)\n\nresult.fetchall()\n\n\n\n\n\n\n\n\n\n\n\n\ntt0111161\n\n\n9.3\n\n\n2651547\n\n\nThe Shawshank Redemption\n\n\n1994\n\n\n\n\ntt0068646\n\n\n9.2\n\n\n1838044\n\n\nThe Godfather\n\n\n1972\n\n\n\n\ntt0468569\n\n\n9.0\n\n\n2623735\n\n\nThe Dark Knight\n\n\n2008\n\n\n\n\ntt0167260\n\n\n9.0\n\n\n1827464\n\n\nThe Lord of the Rings: The Return of the King\n\n\n2003\n\n\n\n\ntt0108052\n\n\n9.0\n\n\n1343647\n\n\nSchindler’s List\n\n\n1993\n\n\n\n\ntt0071562\n\n\n9.0\n\n\n1259465\n\n\nThe Godfather Part II\n\n\n1974\n\n\n\n\ntt0050083\n\n\n9.0\n\n\n782903\n\n\n12 Angry Men\n\n\n1957\n\n\n\n\ntt0110912\n\n\n8.9\n\n\n2029684\n\n\nPulp Fiction\n\n\n1994\n\n\n\n\ntt1375666\n\n\n8.8\n\n\n2325417\n\n\nInception\n\n\n2010\n\n\n\n\ntt0137523\n\n\n8.8\n\n\n2096752\n\n\nFight Club\n\n\n1999\n\n\n\n\n\nThat looks about right to me. There may be some small differences with the current Top 10 list on IMDB if our data are a little stale.\nIt’s early days still for Substrait, but it’s exciting to see how far it’s come in the last 18 months!"
  },
  {
    "objectID": "blog/ibis_substrait_to_duckdb.html#why-wouldnt-i-just-use-sql-for-this",
    "href": "blog/ibis_substrait_to_duckdb.html#why-wouldnt-i-just-use-sql-for-this",
    "title": "Ibis + Substrait + DuckDB",
    "section": "",
    "text": "It’s a fair question. SQL is everywhere, after all.\nThere are a few reasons we think you shouldn’t ignore Substrait.\n\n\nSQL has a standard, but how closely do engines follow the standard? In our experience, queries don’t translate well between engines (this is one reason Ibis exists!)\n\n\n\nSubstrait is more extensible than SQL. Some DBMS have added in some very cool features, but it usually involves diverging (sometimes widely) from the SQL standard. Substrait has an extension system that allows plan producers and plan consumers to agree on a well-typed and well-defined interaction that exists outside of the core Substrait specification.\n\n\n\nParsing SQL can be a big pain (trust us). If you send a big string over the wire, you need the engine on the other side to have a SQL parser to understand what the message is. Now, obviously, SQL engines have those. But here, again, standards (or lack of adherence to standards) can bite you. Extensibility is also difficult here, because now the SQL parser needs to understand some new custom syntax.\nProtobuf is hardly a dream to work with, but it’s a lot easier to consistently define behavior AND to validate that behavior is correct. It’s also smaller than raw text."
  },
  {
    "objectID": "blog/ibis_substrait_to_duckdb.html#wrapping-up",
    "href": "blog/ibis_substrait_to_duckdb.html#wrapping-up",
    "title": "Ibis + Substrait + DuckDB",
    "section": "",
    "text": "That’s all for now! To quickly summarize:\nSubstrait is a new standard for representing relational algebra queries with support in Apache Arrow, DuckDB, Velox, and more (and more to come!).\nIbis can now generate substrait instead of string SQL, letting it take advantage of this new standard.\nInterested in substrait or ibis? Docs are available at\n\nSubstrait\nIbis Docs\n\nand the relevant GitHub repos are\n\nSubstrait GitHub\nIbis Substrait GitHub\nIbis GitHub\n\nPlease feel free to reach out on GitHub!"
  },
  {
    "objectID": "blog/ibis-to-file.html",
    "href": "blog/ibis-to-file.html",
    "title": "Ibis Sneak Peek: Writing to Files",
    "section": "",
    "text": "by Kae Suarez\nIbis 5.0 is coming soon and will offer new functionality and fixes to users. To enhance clarity around this process, we’re sharing a sneak peek into what we’re working on.\nIn Ibis 4.0, we added the ability to read CSVs and Parquet via the Ibis interface. We felt this was important because, well, the ability to read files is simply necessary, be it on a local scale, legacy data, data not yet in a database, and so on. However, for a user, the natural next question was “can I go ahead and write when I’m done?” The answer was no. We didn’t like that, especially since we do care about file-based use cases.\nSo, we’ve gone ahead and fixed that for Ibis 5.0.\n\n\nBefore we can write a file, we need data — so let’s read in a file, to start this off:\nt = ibis.read_csv(\n    \"https://storage.googleapis.com/ibis-examples/data/penguins.csv.gz\"\n)\nOf course, we could just write out, but let’s do an operation first — how about using selectors, which you can read more about here? Self-promotion aside, here’s an operation:\nexpr = (\n    t.group_by(\"species\")\n     .mutate(s.across(s.numeric() & ~s.c(\"year\"), (_ - _.mean()) / _.std()))\n)\nNow, finally, time to do the exciting part:\nexpr.to_parquet(\"normalized.parquet\")\nLike many things in Ibis, this is as simple and plain-looking as it is important. Being able to create files from Ibis instead of redirecting into other libraries first enables operation at larger scales and fewer steps. Where desired, you can address a backend directly to use its native export functionality — we want to make sure you have the flexibility to use Ibis or the backend as you see fit.\n\n\n\nIbis is an interface tool for analytical engines that can reach scales far beyond a laptop. Files are important to Ibis because:\n\nIbis also supports local execution, where files are the standard unit of data — we want to support all our users.\nFiles are useful for moving between platforms, and long-term storage that isn’t tied to a particular backend.\nFiles can move more easily between our backends than database files, so we think this adds some convenience for the multi-backend use case.\n\nWe’re excited to release this functionality in Ibis 5.0.\nInterested in Ibis? Docs are available on this very website, at:\n\nIbis Docs\n\nand the repo is always at:\n\nIbis GitHub\n\nPlease feel free to reach out on GitHub!"
  },
  {
    "objectID": "blog/ibis-to-file.html#files-in-files-out",
    "href": "blog/ibis-to-file.html#files-in-files-out",
    "title": "Ibis Sneak Peek: Writing to Files",
    "section": "",
    "text": "Before we can write a file, we need data — so let’s read in a file, to start this off:\nt = ibis.read_csv(\n    \"https://storage.googleapis.com/ibis-examples/data/penguins.csv.gz\"\n)\nOf course, we could just write out, but let’s do an operation first — how about using selectors, which you can read more about here? Self-promotion aside, here’s an operation:\nexpr = (\n    t.group_by(\"species\")\n     .mutate(s.across(s.numeric() & ~s.c(\"year\"), (_ - _.mean()) / _.std()))\n)\nNow, finally, time to do the exciting part:\nexpr.to_parquet(\"normalized.parquet\")\nLike many things in Ibis, this is as simple and plain-looking as it is important. Being able to create files from Ibis instead of redirecting into other libraries first enables operation at larger scales and fewer steps. Where desired, you can address a backend directly to use its native export functionality — we want to make sure you have the flexibility to use Ibis or the backend as you see fit."
  },
  {
    "objectID": "blog/ibis-to-file.html#wrapping-up",
    "href": "blog/ibis-to-file.html#wrapping-up",
    "title": "Ibis Sneak Peek: Writing to Files",
    "section": "",
    "text": "Ibis is an interface tool for analytical engines that can reach scales far beyond a laptop. Files are important to Ibis because:\n\nIbis also supports local execution, where files are the standard unit of data — we want to support all our users.\nFiles are useful for moving between platforms, and long-term storage that isn’t tied to a particular backend.\nFiles can move more easily between our backends than database files, so we think this adds some convenience for the multi-backend use case.\n\nWe’re excited to release this functionality in Ibis 5.0.\nInterested in Ibis? Docs are available on this very website, at:\n\nIbis Docs\n\nand the repo is always at:\n\nIbis GitHub\n\nPlease feel free to reach out on GitHub!"
  },
  {
    "objectID": "blog/ibis-version-4.0.0-release.html",
    "href": "blog/ibis-version-4.0.0-release.html",
    "title": "Ibis v4.0.0",
    "section": "",
    "text": "by Patrick Clarke\n09 January 2023\n\n\nIbis 4.0 has officially been released as the latest version of the package. This release includes several new backends, improved functionality, and some major internal refactors. A full list of the changes can be found in the project release notes. Let’s talk about some of the new changes 4.0 brings for Ibis users.\n\n\n\nIbis 4.0 brings Polars, Snowflake, and Trino into an already-impressive stock of supported backends. The Polars backend adds another way for users to work locally with DataFrames. The Snowflake and Trino backends add a free and familiar python API to popular data warehouses.\nAlongside these new backends, Google BigQuery and Microsoft SQL have been moved to the main repo, so their release cycle will follow the Ibis core.\n\n\n\nThere are a lot of improvements incoming, but some notable changes include:\n\nread API: allows users to read various file formats directly into their configured default_backend (default DuckDB) through read_* functions, which makes working with local files easier than ever.\nto_pyarrow and to_pyarrow_batches: users can now return PyArrow objects (Tables, Arrays, Scalars, RecordBatchReader) and therefore grants all of the functionality that PyArrow provides\nJSON getitem: users can now run getitem on a JSON field using Ibis expressions with some backends\nPlotting support through __array__: allows users to plot Ibis expressions out of the box\n\n\n\n\nThis won’t be visible to most users, but the project underwent a series of refactors that spans multiple PRs. Notable changes include removing intermediate expressions, improving the testing framework, and UX updates.\n\n\n\nAs mentioned previously, additional functionality, bugfixes, and more have been included in the latest 4.0 release. To stay up to date and learn more about recent changes: check out the project’s homepage at ibis-project.org, follow @IbisData on Twitter, find the source code and community on GitHub, and join the discussion on Gitter.\nAs always, try Ibis by installing it today."
  },
  {
    "objectID": "blog/ibis-version-4.0.0-release.html#introduction",
    "href": "blog/ibis-version-4.0.0-release.html#introduction",
    "title": "Ibis v4.0.0",
    "section": "",
    "text": "Ibis 4.0 has officially been released as the latest version of the package. This release includes several new backends, improved functionality, and some major internal refactors. A full list of the changes can be found in the project release notes. Let’s talk about some of the new changes 4.0 brings for Ibis users."
  },
  {
    "objectID": "blog/ibis-version-4.0.0-release.html#backends",
    "href": "blog/ibis-version-4.0.0-release.html#backends",
    "title": "Ibis v4.0.0",
    "section": "",
    "text": "Ibis 4.0 brings Polars, Snowflake, and Trino into an already-impressive stock of supported backends. The Polars backend adds another way for users to work locally with DataFrames. The Snowflake and Trino backends add a free and familiar python API to popular data warehouses.\nAlongside these new backends, Google BigQuery and Microsoft SQL have been moved to the main repo, so their release cycle will follow the Ibis core."
  },
  {
    "objectID": "blog/ibis-version-4.0.0-release.html#functionality",
    "href": "blog/ibis-version-4.0.0-release.html#functionality",
    "title": "Ibis v4.0.0",
    "section": "",
    "text": "There are a lot of improvements incoming, but some notable changes include:\n\nread API: allows users to read various file formats directly into their configured default_backend (default DuckDB) through read_* functions, which makes working with local files easier than ever.\nto_pyarrow and to_pyarrow_batches: users can now return PyArrow objects (Tables, Arrays, Scalars, RecordBatchReader) and therefore grants all of the functionality that PyArrow provides\nJSON getitem: users can now run getitem on a JSON field using Ibis expressions with some backends\nPlotting support through __array__: allows users to plot Ibis expressions out of the box"
  },
  {
    "objectID": "blog/ibis-version-4.0.0-release.html#refactors",
    "href": "blog/ibis-version-4.0.0-release.html#refactors",
    "title": "Ibis v4.0.0",
    "section": "",
    "text": "This won’t be visible to most users, but the project underwent a series of refactors that spans multiple PRs. Notable changes include removing intermediate expressions, improving the testing framework, and UX updates."
  },
  {
    "objectID": "blog/ibis-version-4.0.0-release.html#additional-changes",
    "href": "blog/ibis-version-4.0.0-release.html#additional-changes",
    "title": "Ibis v4.0.0",
    "section": "",
    "text": "As mentioned previously, additional functionality, bugfixes, and more have been included in the latest 4.0 release. To stay up to date and learn more about recent changes: check out the project’s homepage at ibis-project.org, follow @IbisData on Twitter, find the source code and community on GitHub, and join the discussion on Gitter.\nAs always, try Ibis by installing it today."
  },
  {
    "objectID": "blog/ffill-and-bfill-using-ibis.html",
    "href": "blog/ffill-and-bfill-using-ibis.html",
    "title": "ffill and bfill using Ibis",
    "section": "",
    "text": "ffill and bfill using Ibis\nby Patrick Clarke\nSuppose you have a table of data mapping events and dates to values, and that this data contains gaps in values.\nSuppose you want to forward fill these gaps such that, one-by-one, if a value is null, it is replaced by the non-null value preceding.\nFor example, you might be measuring the total value of an account over time. Saving the same value until that value changes is an inefficient use of space, so you might only measure the value during certain events, like a change in ownership or value.\nIn that case, to view the value of the account by day, you might want to interpolate dates and then ffill or bfill value to show the account value over time by date.\nDate interpolation will be covered in a different guide, but if you already have the dates then you can fill in some values.\nThis was heavily inspired by Gil Forsyth’s writeup on ffill and bfill on the Ibis GitHub Wiki.\n\nSetup\nFirst, we want to make some mock data. To demonstrate this technique in a non-pandas backend, we will use the DuckDB backend.\nOur data will have measurements by date, and these measurements will be grouped by an event id. We will then save this data to data.parquet so we can register that parquet file as a table in our DuckDB connector.\nIn [1]: import ibis; from datetime import date\nIn [2]: import numpy as np; import pandas as pd\n\nIn [3]: df = pd.DataFrame({\n   ...:     \"event_id\": [0] * 2 + [1] * 3 + [2] * 5 + [3] * 2\n   ...:     ,\"measured_on\": map(\n   ...:         date\n   ...:         ,[2021] * 12, [6] * 4 + [5] * 6 + [7] * 2\n   ...:         ,range(1, 13)\n   ...:     )\n   ...:     ,\"measurement\": np.nan\n   ...: })\n\nIn [4]: df.head()\nOut[4]:\n   event_id measured_on  measurement\n0         0  2021-06-01          NaN\n1         0  2021-06-02          NaN\n2         1  2021-06-03          NaN\n3         1  2021-06-04          NaN\n4         1  2021-05-05          NaN\n\nIn [5]: df.at[1, \"measurement\"] = 5.\nIn [6]: df.at[4, \"measurement\"] = 42.\nIn [7]: df.at[5, \"measurement\"] = 42.\nIn [8]: df.at[7, \"measurement\"] = 11.\n\nIn [9]: df\nOut[9]:\n    event_id measured_on  measurement\n0          0  2021-06-01          NaN\n1          0  2021-06-02          5.0\n2          1  2021-06-03          NaN\n3          1  2021-06-04          NaN\n4          1  2021-05-05         42.0\n5          2  2021-05-06         42.0\n6          2  2021-05-07          NaN\n7          2  2021-05-08         11.0\n8          2  2021-05-09          NaN\n9          2  2021-05-10          NaN\n10         3  2021-07-11          NaN\n11         3  2021-07-12          NaN\n\nIn [10]: df.to_parquet(\"data.parquet\")\nTo use the DuckDB backend with our data, we will spin up a DuckDB connection and then register data.parquet as data:\nIn [11]: conn = ibis.connect('duckdb://:memory:')\n\nIn [12]: conn.register('data.parquet', table_name='data')\nOut[12]:\nAlchemyTable: data\n  event_id    int64\n  measured_on date\n  measurement float64\n\nIn [13]: data = conn.table(\"data\")\n\nIn [14]: data.execute()\nOut[14]:\n    event_id measured_on  measurement\n0          0  2021-06-01          NaN\n1          0  2021-06-02          5.0\n2          1  2021-06-03          NaN\n3          1  2021-06-04          NaN\n4          1  2021-05-05         42.0\n5          2  2021-05-06         42.0\n6          2  2021-05-07          NaN\n7          2  2021-05-08         11.0\n8          2  2021-05-09          NaN\n9          2  2021-05-10          NaN\n10         3  2021-07-11          NaN\n11         3  2021-07-12          NaN\n\nIn [15]: data\nOut[15]:\nAlchemyTable: data\n  event_id    int64\n  measured_on date\n  measurement float64\n\n\nffill Strategy\nTo better understand how we can forward-fill our gaps, let’s take a minute to explain the strategy and then look at the manual result.\nWe will partition our data by event groups and then sort those groups by date.\nOur logic for forward fill is then: let j be an event group sorted by date and let i be a date within j. If i is the first date in j, then continue. If i is not the first date in j, then if measurement in i is null then replace it with measurement for i-1. Otherwise, do nothing.\nLet’s take a look at what this means for the first few rows of our data:\n    event_id measured_on  measurement\n0          0  2021-06-01          NaN # Since this is the first row of the event group (group 0), do nothing\n1          0  2021-06-02          5.0 # Since this is not the first row of the group and is not null: do nothing\n4          1  2021-05-05         42.0 # This is the first row of the event group (group 1): do nothing\n2          1  2021-06-03          NaN # This is not the first row and is null: replace it (NaN → 42.0)\n3          1  2021-06-04          NaN # This is not the first row and is null: replace it (NaN → 42.0)\n5          2  2021-05-06         42.0 # This is the first row of the event group (group 2): do nothing\n6          2  2021-05-07          NaN # This is not the first row and is null: replace it (NaN → 42.0)\n7          2  2021-05-08         11.0 # This is not the first row and is not null: do nothing\n8          2  2021-05-09          NaN # This is not the first row and is null: replace it (NaN → 11.0)\n9          2  2021-05-10          NaN # This is not the first row and is null: replace it (NaN → 11.0)\n10         3  2021-07-11          NaN # This is the first row of the event group (group 3): do nothing\n11         3  2021-07-12          NaN # This is not the first row and is null: replace it (NaN → NaN)\nOur result should for forward fill should look like this:\n    event_id measured_on  measurement\n0          0  2021-06-01          NaN\n1          0  2021-06-02          5.0\n2          1  2021-05-05         42.0\n3          1  2021-06-03         42.0\n4          1  2021-06-04         42.0\n5          2  2021-05-06         42.0\n6          2  2021-05-07         42.0\n7          2  2021-05-08         11.0\n8          2  2021-05-09         11.0\n9          2  2021-05-10         11.0\n10         3  2021-07-11          NaN\n11         3  2021-07-12          NaN\nTo accomplish this, we will create a window over our event_id to partition our data into groups. We will take these groups and order them by measured_on:\nIn [16]: win = ibis.window(group_by=data.event_id, order_by=data.measured_on, following=0)\nOnce we have our window defined, we can flag the first non-null value in an event group using count, as it will count non-null values row-by-row within our group:\nIn [17]: grouped = data.mutate(grouper=data.measurement.count().over(win))\n\nIn [18]: grouped.execute().sort_values(by=['event_id', 'measured_on'])\nOut[18]:\n    event_id measured_on  measurement  grouper\n0          0  2021-06-01          NaN        0\n1          0  2021-06-02          5.0        1\n2          1  2021-05-05         42.0        1\n3          1  2021-06-03          NaN        1\n4          1  2021-06-04          NaN        1\n5          2  2021-05-06         42.0        1\n6          2  2021-05-07          NaN        1\n7          2  2021-05-08         11.0        2\n8          2  2021-05-09          NaN        2\n9          2  2021-05-10          NaN        2\n10         3  2021-07-11          NaN        0\n11         3  2021-07-12          NaN        0\nTo see this a bit clearer: look at rows 0, 1, and 2. Row 0 is NaN and is the first row of the group (event_id = 0), so at row 0 we have 0 non-null values (grouper = 0). Row 1 is not null (5.0) and is the second row the group, so our count has increased by 1 (grouper = 1). Row 2 is the first row of its group (event_id = 1) and is not null, so our count is 1 (grouper = 1).\nSkip down to rows 9, 10, and 11. Row 9 is the sixth row of group 2 and there are three non-null values in group 2 before row 9. Therefore the count at row 9 is 3.\nRow 10 is the first row of group 3 and is null, therefore its count is 0. Finally: row 11 is the second row of group 3 and is null as well, therefore the count remains 0.\nUnder this design, we now have another partition.\nOur first partition is by event_id. Within each set in that partition, we have a partition by grouper, where each set has up to one non-null value.\nSince there less than or equal to one non-null value in each group of ['event_id', 'grouper'], we can simply fill values by overwriting all values within the group by the max value in the group.\nSo:\n\nGroup by event_id and grouper\nMutate the data along that grouping by populating a new column ffill with the max value of measurement.\n\nIn [19]: result = (\n    ...:     grouped\n    ...:     .group_by([grouped.event_id, grouped.grouper])\n    ...:     .mutate(ffill=grouped.measurement.max())\n    ...:     .execute()\n    ...: ).sort_values(by=['event_id', 'measured_on']).reset_index(drop=True)\n\nIn [20]: result\nOut[20]:\n    event_id measured_on  measurement  grouper  ffill\n0          0  2021-06-01          NaN        0    NaN\n1          0  2021-06-02          5.0        1    5.0\n2          1  2021-05-05         42.0        1   42.0\n3          1  2021-06-03          NaN        1   42.0\n4          1  2021-06-04          NaN        1   42.0\n5          2  2021-05-06         42.0        1   42.0\n6          2  2021-05-07          NaN        1   42.0\n7          2  2021-05-08         11.0        2   11.0\n8          2  2021-05-09          NaN        2   11.0\n9          2  2021-05-10          NaN        2   11.0\n10         3  2021-07-11          NaN        0    NaN\n11         3  2021-07-12          NaN        0    NaN\n\n\nbfill Strategy\nInstead of sorting the dates ascending, we will sort them descending. This is akin to starting at the last row in an event group and going backwards using the same logic outlined above.\nLet’s take a look:\n    event_id measured_on  measurement  grouper\n0          0  2021-06-01          NaN        1 # null, take the previous row value (NaN → 5.0)\n1          0  2021-06-02          5.0        1 # last row, do nothing\n2          1  2021-05-05         42.0        1 # not null, do nothing\n3          1  2021-06-03          NaN        0 # null, take previous row value (NaN → NaN)\n4          1  2021-06-04          NaN        0 # last row, do nothing\n5          2  2021-05-06         42.0        2 # not null, do nothing\n6          2  2021-05-07          NaN        1 # null, take previous row value (NaN → 11.0)\n7          2  2021-05-08         11.0        1 # not null, do nothing\n8          2  2021-05-09          NaN        0 # null, take previous row value (NaN → NaN)\n9          2  2021-05-10          NaN        0 # not null, do nothing\n10         3  2021-07-11          NaN        0 # null, take previous row value (NaN → NaN)\n11         3  2021-07-12          NaN        0 # last row, do nothing\nCodewise, bfill follows the same strategy as ffill, we need to specify order_by to use ibis.desc. This will flip our dates and our counts (therefore our groupers) will start backwards.\nIn [21]: win = ibis.window(group_by=data.event_id, order_by=ibis.desc(data.measured_on), following=0)\n\nIn [22]: grouped = data.mutate(grouper=data.measurement.count().over(win))\n\nIn [23]: grouped.execute().sort_values(by=['event_id', 'measured_on']).reset_index(drop=True)\nOut[23]:\n    event_id measured_on  measurement  grouper\n0          0  2021-06-01          NaN        1\n1          0  2021-06-02          5.0        1\n2          1  2021-05-05         42.0        1\n3          1  2021-06-03          NaN        0\n4          1  2021-06-04          NaN        0\n5          2  2021-05-06         42.0        2\n6          2  2021-05-07          NaN        1\n7          2  2021-05-08         11.0        1\n8          2  2021-05-09          NaN        0\n9          2  2021-05-10          NaN        0\n10         3  2021-07-11          NaN        0\n11         3  2021-07-12          NaN        0\nAnd, again, if we take max of our grouper value, we will get the only non-null value if it exists:\nIn [24]: result = (\n    ...:     grouped\n    ...:     .group_by([grouped.event_id, grouped.grouper])\n    ...:     .mutate(bfill=grouped.measurement.max())\n    ...:     .execute()\n    ...: ).sort_values(by=['event_id', 'measured_on']).reset_index(drop=True)\n\nIn [25]: result\nOut[25]:\n    event_id measured_on  measurement  grouper  bfill\n0          0  2021-06-01          NaN        1    5.0\n1          0  2021-06-02          5.0        1    5.0\n2          1  2021-05-05         42.0        1   42.0\n3          1  2021-06-03          NaN        0    NaN\n4          1  2021-06-04          NaN        0    NaN\n5          2  2021-05-06         42.0        2   42.0\n6          2  2021-05-07          NaN        1   11.0\n7          2  2021-05-08         11.0        1   11.0\n8          2  2021-05-09          NaN        0    NaN\n9          2  2021-05-10          NaN        0    NaN\n10         3  2021-07-11          NaN        0    NaN\n11         3  2021-07-12          NaN        0    NaN\n\n\nbfill and ffill without Event Groups\nYou can bfill and ffill without event groups by ignoring that grouping. Remove all references of event_id and you can treat the entire dataset as one event.\nYour window function will increment whenever a new non-null value is observed, creating that partition where each set has up to one non-null value.\nFor example, reasoning through bfill:\nIn [26]: data.execute().sort_values(by=['measured_on'])\nOut[26]:\n    event_id measured_on  measurement\n4          1  2021-05-05         42.0 # not null, do nothing\n5          2  2021-05-06         42.0 # not null, do nothing\n6          2  2021-05-07          NaN # null, take previous value (NaN → 11.0)\n7          2  2021-05-08         11.0 # not null, do nothing\n8          2  2021-05-09          NaN # null, take previous value (NaN → 5.0)\n9          2  2021-05-10          NaN # null, take previous value (NaN → 5.0)\n0          0  2021-06-01          NaN # null, take previous value (NaN → 5.0)\n1          0  2021-06-02          5.0 # not null, do nothing\n2          1  2021-06-03          NaN # null, take previous value (NaN → NaN)\n3          1  2021-06-04          NaN # null, take previous value (NaN → NaN)\n10         3  2021-07-11          NaN # null, take previous value (NaN → NaN)\n11         3  2021-07-12          NaN # last row, do nothing\n\nIn [27]: win = ibis.window(order_by=ibis.desc(data.measured_on), following=0)\n\nIn [28]: grouped = data.mutate(grouper=data.measurement.count().over(win))\n\nIn [29]: result = (\n    ...:     grouped\n    ...:     .group_by([grouped.grouper])\n    ...:     .mutate(bfill=grouped.measurement.max())\n    ...: )\n\nIn [30]: result.execute().sort_values(by=['measured_on'])\nOut[30]:\n    event_id measured_on  measurement  grouper  bfill\n9          1  2021-05-05         42.0        4   42.0\n4          2  2021-05-06         42.0        3   42.0\n11         2  2021-05-07          NaN        2   11.0\n10         2  2021-05-08         11.0        2   11.0\n8          2  2021-05-09          NaN        1    5.0\n7          2  2021-05-10          NaN        1    5.0\n6          0  2021-06-01          NaN        1    5.0\n5          0  2021-06-02          5.0        1    5.0\n3          1  2021-06-03          NaN        0    NaN\n2          1  2021-06-04          NaN        0    NaN\n1          3  2021-07-11          NaN        0    NaN\n0          3  2021-07-12          NaN        0    NaN\nAs an exercise, try to take your time and reason your way through ffill.\nHappy coding!"
  },
  {
    "objectID": "blog/Ibis-version-3.0.0-release.html",
    "href": "blog/Ibis-version-3.0.0-release.html",
    "title": "Ibis v3.0.0",
    "section": "",
    "text": "by Marlene Mhangami\nThe latest version of Ibis, version 3.0.0, has just been released! This post highlights some of the new features, breaking changes, and performance improvements that come with the new release. 3.0.0 is a major release and includes more changes than those listed in this post. A full list of the changes can be found in the project release notes here.\n\n\nAligned to the roadmap and in response to the community’s requests, Ibis 3.0.0 introduces many new features and functionality.\n\nNow query an Ibis table using inline SQL\nNEW DuckDB backend\nExplore the NEW backend support matrix tool\nImproved support for arrays and tuples in ClickHouse\nSuffixes now supported in join API expressions\nAPIs for creating timestamps and dates from component fields\nPretty printing in ipython/ notebooks\n\nRefer to the sections below for more detail on each new feature.\n\n\nThe most exciting feature of this release is inline SQL! Many data scientists or developers may be familiar with both Python and SQL. However there may be some queries, transformations that they feel comfortable doing in SQL instead of Python. In the updated version of Ibis users can query an Ibis table using SQL! The new .sql method allows users to mix SQL strings with ibis expressions as well as query ibis table expressions in SQL strings.\nThis functionality currently works for the following backends:\n\nPostgreSQL\nDuckDB\nPySpark\nMySQL\n\nIf you’re interested in adding .sql support for other backends please open an issue.\n\n\n\nIbis now supports DuckDB as a backend. DuckDB is a high-performance SQL OLAP database management system. It is designed to be fast, reliable and easy to use and can be embedded. Many Ibis use cases start from getting tables from a single-node backend so directly supporting DuckDB offers a lot of value. As mentioned earlier, the DuckDB backend allows for the new .sql method on tables for mixing sql and Ibis expressions.\n\n\n\nAs the number of backends Ibis supports grows, it can be challenging for users to decide which one best fits their needs. One way to make a more informed decision is for users to find the backend that supports the operations they intend to use. The 3.0.0 release comes with a backend support matrix that allows users to do just that. A screenshot of part of the matrix can be seen below and the full version can be found here.\nIn addition to this users can now call ibis.${backend}.has_operation to find out if a specific operation is supported by a backend.\n\n\n\nbackend support matrix\n\n\n\n\n\nThe 3.0.0 release includes a slew of important improvements for the ClickHouse backend. Most prominently ibis now supports ClickHouse arrays and tuples. Some of the related operations that have been implemented are:\n\nArrayIndex\nArrayConcat\nArrayRepeat\nArraySlice\n\nOther additional operations now supported for the clickhouse backend are string concat, string slicing, table union, trim, pad and string predicates (LIKE and ILIKE) and all remaining joins.\n\n\n\nIn previous versions Ibis’ join API did not accept suffixes as a parameter, leaving backends to either use some default value or raise an error at execution time when column names overlapped. In 3.0.0 suffixes are now directly supported in the join API itself. Along with the removal of materialize, ibis now automatically adds a default suffix to any overlapping column names.\n\n\n\nIt is now possible to create timestamps directly from component fields. This is now possible using the new method ibis.date(y,m,d). A user can pass in a year, month and day and the result is a datetime object. That is we can assert for example that ibis.date (2022, 2, 4).type() == dt.date\n\n\n\nFor users that use jupyter notebooks, repr_html has been added for expressions to enable pretty printing tables in the notebook. This is currently only available for interactive mode (currently delegating to pandas implementation) and should help notebooks become more readable. An example of what this looks like can be seen below.\n\n\n\npretty print repr\n\n\n\n\n\n\n3.0.0 is a major release and according to the project’s use of semantic versioning, breaking changes are on the table. The full list of these changes can be found here. Some of the important changes include:\n\nPython 3.8 is now the minimum supported version\nDeprecation of .materialize()\n\nRefer to the sections below for more detail on these changes.\n\n\nIbis currently follows NEP 29, a community policy standard that recommends Python and Numpy versions to support. NEP 29 suggests that all projects across the Scientific Python ecosystem adopt a common “time window-based” policy for support of Python and NumPy versions. Standardizing a recommendation for project support of minimum Python and NumPy versions will improve downstream project planning. As part of the 3.0.0 release, support for Python 3.7 has been dropped and the project has now adopted support for version 3.8 and higher.\n\n\n\nThis release sees the deprecation of the .materialize() method from TableExpr. In the past, the materialize method has caused a lot of confusion. Doing simple things like t.join(s, t.foo == s.foo).select([\"unambiguous_column\"]) raised an exception because of it. It turns out that .materialize() isn’t necessary. The materialize method still exists, but is now a no-op and doesn’t need to be used.\n\n\n\n\nThe following changes to the Ibis codebase have resulted in performance improvements.\n\nSpeeding up __str__ and __hash__ datatypes\nCreating a fast path for simple column selection (pandas/dask backends)\nGlobal equality cache\nRemoving full tree repr from rule validator error message\nSpeed up attribute access\nUsing assign instead of concat in projections when possible (pandas/dask backends)\n\nAdditionally, all TPC-H suite queries can be represented in Ibis. All queries are ready-to-run, using the default substitution parameters as specified by the TPC-H spec. Queries have been added here.\n\n\n\nIn summary, the 3.0.0 release includes a number of new features including the ability to query an Ibis table using inline SQL, a DuckDB backend, a backend support matrix tool, support for arrays and tuples, suffixes in joins, timestamps from component fields and prettier tables in ipython. Some breaking changes to take note of are the removal of .materialize() and the switch to Python 3.8 as the minimum supported version. A wide range of changes to the code has also led to significant speed ups in 3.0.0 as well.\nIbis is a community led, open source project. If you’d like to contribute to the project check out the contribution guide here. If you run into a problem and would like to submit an issue you can do so through Ibis’ Github repository. Finally, Ibis relies on community support to grow and to become successful! You can help promote Ibis by following and sharing the project on Twitter, starring the repo or contributing to the code. Ibis continues to improve with every release. Keep an eye on the blog for updates on the next one!"
  },
  {
    "objectID": "blog/Ibis-version-3.0.0-release.html#new-features",
    "href": "blog/Ibis-version-3.0.0-release.html#new-features",
    "title": "Ibis v3.0.0",
    "section": "",
    "text": "Aligned to the roadmap and in response to the community’s requests, Ibis 3.0.0 introduces many new features and functionality.\n\nNow query an Ibis table using inline SQL\nNEW DuckDB backend\nExplore the NEW backend support matrix tool\nImproved support for arrays and tuples in ClickHouse\nSuffixes now supported in join API expressions\nAPIs for creating timestamps and dates from component fields\nPretty printing in ipython/ notebooks\n\nRefer to the sections below for more detail on each new feature.\n\n\nThe most exciting feature of this release is inline SQL! Many data scientists or developers may be familiar with both Python and SQL. However there may be some queries, transformations that they feel comfortable doing in SQL instead of Python. In the updated version of Ibis users can query an Ibis table using SQL! The new .sql method allows users to mix SQL strings with ibis expressions as well as query ibis table expressions in SQL strings.\nThis functionality currently works for the following backends:\n\nPostgreSQL\nDuckDB\nPySpark\nMySQL\n\nIf you’re interested in adding .sql support for other backends please open an issue.\n\n\n\nIbis now supports DuckDB as a backend. DuckDB is a high-performance SQL OLAP database management system. It is designed to be fast, reliable and easy to use and can be embedded. Many Ibis use cases start from getting tables from a single-node backend so directly supporting DuckDB offers a lot of value. As mentioned earlier, the DuckDB backend allows for the new .sql method on tables for mixing sql and Ibis expressions.\n\n\n\nAs the number of backends Ibis supports grows, it can be challenging for users to decide which one best fits their needs. One way to make a more informed decision is for users to find the backend that supports the operations they intend to use. The 3.0.0 release comes with a backend support matrix that allows users to do just that. A screenshot of part of the matrix can be seen below and the full version can be found here.\nIn addition to this users can now call ibis.${backend}.has_operation to find out if a specific operation is supported by a backend.\n\n\n\nbackend support matrix\n\n\n\n\n\nThe 3.0.0 release includes a slew of important improvements for the ClickHouse backend. Most prominently ibis now supports ClickHouse arrays and tuples. Some of the related operations that have been implemented are:\n\nArrayIndex\nArrayConcat\nArrayRepeat\nArraySlice\n\nOther additional operations now supported for the clickhouse backend are string concat, string slicing, table union, trim, pad and string predicates (LIKE and ILIKE) and all remaining joins.\n\n\n\nIn previous versions Ibis’ join API did not accept suffixes as a parameter, leaving backends to either use some default value or raise an error at execution time when column names overlapped. In 3.0.0 suffixes are now directly supported in the join API itself. Along with the removal of materialize, ibis now automatically adds a default suffix to any overlapping column names.\n\n\n\nIt is now possible to create timestamps directly from component fields. This is now possible using the new method ibis.date(y,m,d). A user can pass in a year, month and day and the result is a datetime object. That is we can assert for example that ibis.date (2022, 2, 4).type() == dt.date\n\n\n\nFor users that use jupyter notebooks, repr_html has been added for expressions to enable pretty printing tables in the notebook. This is currently only available for interactive mode (currently delegating to pandas implementation) and should help notebooks become more readable. An example of what this looks like can be seen below.\n\n\n\npretty print repr"
  },
  {
    "objectID": "blog/Ibis-version-3.0.0-release.html#other-changes",
    "href": "blog/Ibis-version-3.0.0-release.html#other-changes",
    "title": "Ibis v3.0.0",
    "section": "",
    "text": "3.0.0 is a major release and according to the project’s use of semantic versioning, breaking changes are on the table. The full list of these changes can be found here. Some of the important changes include:\n\nPython 3.8 is now the minimum supported version\nDeprecation of .materialize()\n\nRefer to the sections below for more detail on these changes.\n\n\nIbis currently follows NEP 29, a community policy standard that recommends Python and Numpy versions to support. NEP 29 suggests that all projects across the Scientific Python ecosystem adopt a common “time window-based” policy for support of Python and NumPy versions. Standardizing a recommendation for project support of minimum Python and NumPy versions will improve downstream project planning. As part of the 3.0.0 release, support for Python 3.7 has been dropped and the project has now adopted support for version 3.8 and higher.\n\n\n\nThis release sees the deprecation of the .materialize() method from TableExpr. In the past, the materialize method has caused a lot of confusion. Doing simple things like t.join(s, t.foo == s.foo).select([\"unambiguous_column\"]) raised an exception because of it. It turns out that .materialize() isn’t necessary. The materialize method still exists, but is now a no-op and doesn’t need to be used."
  },
  {
    "objectID": "blog/Ibis-version-3.0.0-release.html#performance-improvements",
    "href": "blog/Ibis-version-3.0.0-release.html#performance-improvements",
    "title": "Ibis v3.0.0",
    "section": "",
    "text": "The following changes to the Ibis codebase have resulted in performance improvements.\n\nSpeeding up __str__ and __hash__ datatypes\nCreating a fast path for simple column selection (pandas/dask backends)\nGlobal equality cache\nRemoving full tree repr from rule validator error message\nSpeed up attribute access\nUsing assign instead of concat in projections when possible (pandas/dask backends)\n\nAdditionally, all TPC-H suite queries can be represented in Ibis. All queries are ready-to-run, using the default substitution parameters as specified by the TPC-H spec. Queries have been added here."
  },
  {
    "objectID": "blog/Ibis-version-3.0.0-release.html#conclusion",
    "href": "blog/Ibis-version-3.0.0-release.html#conclusion",
    "title": "Ibis v3.0.0",
    "section": "",
    "text": "In summary, the 3.0.0 release includes a number of new features including the ability to query an Ibis table using inline SQL, a DuckDB backend, a backend support matrix tool, support for arrays and tuples, suffixes in joins, timestamps from component fields and prettier tables in ipython. Some breaking changes to take note of are the removal of .materialize() and the switch to Python 3.8 as the minimum supported version. A wide range of changes to the code has also led to significant speed ups in 3.0.0 as well.\nIbis is a community led, open source project. If you’d like to contribute to the project check out the contribution guide here. If you run into a problem and would like to submit an issue you can do so through Ibis’ Github repository. Finally, Ibis relies on community support to grow and to become successful! You can help promote Ibis by following and sharing the project on Twitter, starring the repo or contributing to the code. Ibis continues to improve with every release. Keep an eye on the blog for updates on the next one!"
  },
  {
    "objectID": "blog/ibis-examples.html",
    "href": "blog/ibis-examples.html",
    "title": "Ibis Sneak Peek: Examples",
    "section": "",
    "text": "by Kae Suarez\nIbis has been moving quickly to provide a powerful but easy-to-use interface for interacting with analytical engines. However, as we’re approaching the 5.0 release of Ibis, we’ve realized that moving from not knowing Ibis to writing a first expression is not trivial.\nAs is, in our tutorial structure, work must be done on the user’s part — though we do provide the commands — to download a SQLite database onto disk, which can only be used with said backend. We feel that this put too much emphasis on a single backend, and added too much effort into picking the right backend for the first tutorial. We want minimal steps between users and learning the Ibis API.\nThis is why we’ve added the examples module.\n\n\nThis module offers in-Ibis access to multiple small tables (the largest is around only 30k rows), which are downloaded when requested and immediately read into the backend upon completion. We worked to keep pulling in examples simple, so it looks like this:\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.examples as ex\n\n&gt;&gt;&gt; t = ex.penguins.fetch()\n&gt;&gt;&gt; t.head()\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\nAnother advantage of this new method is that we were able to register all of them so you can tab-complete, as you can see here:\n\n\n\nTab Complete\n\n\nOnce you’ve retrieved an example table, you can get straight to learning and experimenting, instead of struggling with just getting the data itself.\nIn the future, our tutorials will use the examples module to to help speed up learning of the Ibis framework.\nInterested in Ibis? Docs are available on this very website, at:\n\nIbis Docs\n\nand the repo is always at:\n\nIbis GitHub\n\nPlease feel free to reach out on GitHub!"
  },
  {
    "objectID": "blog/ibis-examples.html#getting-started-with-examples",
    "href": "blog/ibis-examples.html#getting-started-with-examples",
    "title": "Ibis Sneak Peek: Examples",
    "section": "",
    "text": "This module offers in-Ibis access to multiple small tables (the largest is around only 30k rows), which are downloaded when requested and immediately read into the backend upon completion. We worked to keep pulling in examples simple, so it looks like this:\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.examples as ex\n\n&gt;&gt;&gt; t = ex.penguins.fetch()\n&gt;&gt;&gt; t.head()\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\nAnother advantage of this new method is that we were able to register all of them so you can tab-complete, as you can see here:\n\n\n\nTab Complete\n\n\nOnce you’ve retrieved an example table, you can get straight to learning and experimenting, instead of struggling with just getting the data itself.\nIn the future, our tutorials will use the examples module to to help speed up learning of the Ibis framework.\nInterested in Ibis? Docs are available on this very website, at:\n\nIbis Docs\n\nand the repo is always at:\n\nIbis GitHub\n\nPlease feel free to reach out on GitHub!"
  },
  {
    "objectID": "blog/rendered/ci-analysis.html",
    "href": "blog/rendered/ci-analysis.html",
    "title": "Analysis of Ibis’s CI Performance",
    "section": "",
    "text": "This notebook takes you through an analysis of Ibis’s CI data using ibis on top of Google BigQuery.\n\nFirst, we load some data and poke around at it to see what’s what.\nSecond, we figure out some useful things to calculate based on our poking.\nThird, we’ll visualize the results of calculations to showcase what changed and how."
  },
  {
    "objectID": "blog/rendered/ci-analysis.html#summary",
    "href": "blog/rendered/ci-analysis.html#summary",
    "title": "Analysis of Ibis’s CI Performance",
    "section": "",
    "text": "This notebook takes you through an analysis of Ibis’s CI data using ibis on top of Google BigQuery.\n\nFirst, we load some data and poke around at it to see what’s what.\nSecond, we figure out some useful things to calculate based on our poking.\nThird, we’ll visualize the results of calculations to showcase what changed and how."
  },
  {
    "objectID": "blog/rendered/ci-analysis.html#imports",
    "href": "blog/rendered/ci-analysis.html#imports",
    "title": "Analysis of Ibis’s CI Performance",
    "section": "Imports",
    "text": "Imports\nLet’s start out by importing ibis and turning on interactive mode.\n\nimport ibis\nfrom ibis import _\n\nibis.options.interactive = True"
  },
  {
    "objectID": "blog/rendered/ci-analysis.html#connect-to-bigquery",
    "href": "blog/rendered/ci-analysis.html#connect-to-bigquery",
    "title": "Analysis of Ibis’s CI Performance",
    "section": "Connect to BigQuery",
    "text": "Connect to BigQuery\nWe connect to BigQuery using the ibis.connect API, which accepts a URL string indicating the backend and various bit of information needed to connect to the backend. Here we’re using BigQuery, so we need the project id (ibis-gbq) and the dataset id (workflows).\nDatasets are analogous to schemas in other systems.\n\nurl = \"bigquery://ibis-gbq/workflows\"\ncon = ibis.connect(url)\n\nLet’s see what tables are available.\n\ncon.list_tables()\n\n['analysis', 'jobs', 'workflows']"
  },
  {
    "objectID": "blog/rendered/ci-analysis.html#analysis",
    "href": "blog/rendered/ci-analysis.html#analysis",
    "title": "Analysis of Ibis’s CI Performance",
    "section": "Analysis",
    "text": "Analysis\nHere we’ve got our first bit of interesting information: the jobs and workflows tables.\n\nTerminology\nBefore we jump in, it helps to lay down some terminology.\n\nA workflow corresponds to an individual GitHub Actions YAML file in a GitHub repository under the .github/workflows directory.\nA job is a named set of steps to run inside a workflow file.\n\n\n\nWhat’s in the workflows table?\nEach row in the workflows table corresponds to a workflow run.\n\nA workflow run is an instance of a workflow that was triggered by some entity: a GitHub user, bot, or other entity. Each row of the workflows table is a workflow run.\n\n\n\nWhat’s in the jobs table?\nSimilarly, each row in the jobs table is a job run. That is, for a given workflow run there are a set of jobs run with it.\n\nA job run is an instance of a job in a workflow. It is associated with a single workflow run."
  },
  {
    "objectID": "blog/rendered/ci-analysis.html#rationale",
    "href": "blog/rendered/ci-analysis.html#rationale",
    "title": "Analysis of Ibis’s CI Performance",
    "section": "Rationale",
    "text": "Rationale\nThe goal of this analysis is to try to understand ibis’s CI performance, and whether the amount of time we spent waiting on CI has decreased, stayed the same or increased. Ideally, we can understand the pieces that contribute to the change or lack thereof.\n\nMetrics\nTo that end there are a few interesting metrics to look at:\n\njob run duration: this is the amount of time it takes for a given job to complete\nworkflow run duration: the amount of time it takes for all job runs in a workflow run to complete.\nqueueing duration: the amount time time spent waiting for the first job run to commence.\n\n\n\nMitigating Factors\n\nAround October 2021, we changed our CI infrastructure to use Poetry instead of Conda. The goal there was to see if we could cache dependencies using the lock file generated by poetry. We should see whether that had any effect.\nAt the end of November 2022, we switch to the Team Plan (a paid GitHub plan) for the Ibis organzation. This tripled the amount of job runs that could execute in parallel. We should see if that helped anything.\n\nAlright, let’s jump into some data!\n\njobs = con.tables.jobs\n\n\njobs\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ url                       ┃ steps                     ┃ status    ┃ started_at          ┃ runner_group_name ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━┩\n│                           │ array&lt;struct&lt;status:      │           │                     │                   │   │\n│ string                    │ string, conclusion:       │ string    │ timestamp           │ string            │ … │\n│                           │ string, started_at:       │           │                     │                   │   │\n│                           │ timestamp, number:…       │           │                     │                   │   │\n├───────────────────────────┼───────────────────────────┼───────────┼─────────────────────┼───────────────────┼───┤\n│ https://api.github.com/r… │ [{...}, {...}, ... +12]   │ completed │ 2020-08-04 23:54:37 │ ∅                 │ … │\n│ https://api.github.com/r… │ [{...}, {...}, ... +5]    │ completed │ 2020-08-04 23:54:37 │ ∅                 │ … │\n│ https://api.github.com/r… │ [{...}, {...}, ... +5]    │ completed │ 2020-08-04 23:51:54 │ ∅                 │ … │\n│ https://api.github.com/r… │ [{...}, {...}, ... +12]   │ completed │ 2020-08-04 23:51:53 │ ∅                 │ … │\n│ https://api.github.com/r… │ [{...}, {...}, ... +11]   │ completed │ 2020-08-04 23:50:19 │ ∅                 │ … │\n│ https://api.github.com/r… │ [{...}, {...}, ... +5]    │ completed │ 2020-08-04 23:50:20 │ ∅                 │ … │\n│ https://api.github.com/r… │ [{...}, {...}, ... +5]    │ completed │ 2020-08-04 23:39:58 │ ∅                 │ … │\n│ https://api.github.com/r… │ [{...}, {...}, ... +9]    │ completed │ 2020-08-04 23:39:57 │ ∅                 │ … │\n│ https://api.github.com/r… │ [{...}, {...}, ... +9]    │ completed │ 2020-08-04 23:34:19 │ ∅                 │ … │\n│ https://api.github.com/r… │ [{...}, {...}, ... +5]    │ completed │ 2020-08-04 23:34:19 │ ∅                 │ … │\n│ …                         │ …                         │ …         │ …                   │ …                 │ … │\n└───────────────────────────┴───────────────────────────┴───────────┴─────────────────────┴───────────────────┴───┘\n\n\n\nThese first few columns in the jobs table aren’t that interesting so we should look at what else is there\n\njobs.columns\n\n['url',\n 'steps',\n 'status',\n 'started_at',\n 'runner_group_name',\n 'run_attempt',\n 'name',\n 'labels',\n 'node_id',\n 'id',\n 'runner_id',\n 'run_url',\n 'run_id',\n 'check_run_url',\n 'html_url',\n 'runner_name',\n 'runner_group_id',\n 'head_sha',\n 'conclusion',\n 'completed_at']\n\n\nA bunch of these aren’t that useful for our purposes. However, run_id, started_at, completed_at are useful for us. The GitHub documentation for job information provides useful detail about the meaning of these fields.\n\nrun_id: the workflow run associated with this job run\nstarted_at: when the job started\ncompleted_at: when the job completed\n\nWhat we’re interested in to a first degree is the job duration, so let’s compute that.\nWe also need to compute when the last job for a given run_id started and when it completed. We’ll use the former to compute the queueing duration, and the latter to compute the total time it took for a given workflow run to complete.\n\nrun_id_win = ibis.window(group_by=_.run_id)\njobs = jobs.select(\n    _.run_id,\n    job_duration=_.completed_at.cast(\"int\") - _.started_at.cast(\"int\"),\n    last_job_started_at=_.started_at.max().over(run_id_win),\n    last_job_completed_at=_.completed_at.max().over(run_id_win),\n)\njobs\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ run_id    ┃ job_duration ┃ last_job_started_at ┃ last_job_completed_at ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64     │ int64        │ timestamp           │ timestamp             │\n├───────────┼──────────────┼─────────────────────┼───────────────────────┤\n│ 199092057 │   3148000000 │ 2020-08-07 11:24:05 │ 2020-08-07 12:16:33   │\n│ 202720732 │     68000000 │ 2020-08-10 15:42:13 │ 2020-08-10 16:31:20   │\n│ 202720732 │   2947000000 │ 2020-08-10 15:42:13 │ 2020-08-10 16:31:20   │\n│ 240931982 │    943000000 │ 2020-09-05 20:52:43 │ 2020-09-05 20:52:43   │\n│ 240931982 │    648000000 │ 2020-09-05 20:52:43 │ 2020-09-05 20:52:43   │\n│ 240931982 │    562000000 │ 2020-09-05 20:52:43 │ 2020-09-05 20:52:43   │\n│ 240931982 │    421000000 │ 2020-09-05 20:52:43 │ 2020-09-05 20:52:43   │\n│ 240931982 │    469000000 │ 2020-09-05 20:52:43 │ 2020-09-05 20:52:43   │\n│ 240931982 │   3244000000 │ 2020-09-05 20:52:43 │ 2020-09-05 20:52:43   │\n│ 240931982 │            0 │ 2020-09-05 20:52:43 │ 2020-09-05 20:52:43   │\n│         … │            … │ …                   │ …                     │\n└───────────┴──────────────┴─────────────────────┴───────────────────────┘\n\n\n\nLet’s take a look at workflows\n\nworkflows = con.tables.workflows\n\n\nworkflows\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ workflow_url              ┃ workflow_id ┃ triggering_actor ┃ run_number ┃ run_attempt ┃ updated_at          ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string                    │ int64       │ struct&lt;subscrip… │ int64      │ int64       │ timestamp           │ … │\n├───────────────────────────┼─────────────┼──────────────────┼────────────┼─────────────┼─────────────────────┼───┤\n│ https://api.github.com/r… │     2100986 │ ∅                │         13 │           1 │ 2020-08-04 23:57:03 │ … │\n│ https://api.github.com/r… │     2100986 │ ∅                │         12 │           1 │ 2020-08-04 23:55:08 │ … │\n│ https://api.github.com/r… │     2100986 │ ∅                │         11 │           1 │ 2020-08-04 23:52:58 │ … │\n│ https://api.github.com/r… │     2100986 │ ∅                │         10 │           1 │ 2020-08-04 23:42:41 │ … │\n│ https://api.github.com/r… │     2100986 │ ∅                │          9 │           1 │ 2020-08-04 23:36:32 │ … │\n│ https://api.github.com/r… │     2100986 │ ∅                │          8 │           1 │ 2020-08-04 23:31:43 │ … │\n│ https://api.github.com/r… │     2100986 │ ∅                │          7 │           1 │ 2020-08-04 23:19:50 │ … │\n│ https://api.github.com/r… │     2100986 │ ∅                │          6 │           1 │ 2020-08-04 23:14:16 │ … │\n│ https://api.github.com/r… │     2100986 │ ∅                │          5 │           1 │ 2020-08-04 23:05:14 │ … │\n│ https://api.github.com/r… │     2100986 │ ∅                │          4 │           1 │ 2020-08-04 23:01:32 │ … │\n│ …                         │           … │ …                │          … │           … │ …                   │ … │\n└───────────────────────────┴─────────────┴──────────────────┴────────────┴─────────────┴─────────────────────┴───┘\n\n\n\nAgain we have a bunch of columns that aren’t so useful to us, so let’s see what else is there.\n\nworkflows.columns\n\n['workflow_url',\n 'workflow_id',\n 'triggering_actor',\n 'run_number',\n 'run_attempt',\n 'updated_at',\n 'cancel_url',\n 'rerun_url',\n 'check_suite_node_id',\n 'pull_requests',\n 'id',\n 'node_id',\n 'status',\n 'repository',\n 'jobs_url',\n 'previous_attempt_url',\n 'artifacts_url',\n 'html_url',\n 'head_sha',\n 'head_repository',\n 'run_started_at',\n 'head_branch',\n 'url',\n 'event',\n 'name',\n 'actor',\n 'created_at',\n 'check_suite_url',\n 'check_suite_id',\n 'conclusion',\n 'head_commit',\n 'logs_url']\n\n\nWe don’t care about many of these for the purposes of this analysis, however we need the id and a few values derived from the run_started_at column.\n\nid: the unique identifier of the workflow run\nrun_started_at: the time the workflow run started\n\nWe compute the date the run started at so we can later compare it to the dates where we added poetry and switched to the team plan.\n\nworkflows = workflows.select(\n    _.id, _.run_started_at, started_date=_.run_started_at.date()\n)\nworkflows\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n┃ id        ┃ run_started_at      ┃ started_date ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n│ int64     │ timestamp           │ date         │\n├───────────┼─────────────────────┼──────────────┤\n│ 195478382 │ 2020-08-04 23:54:29 │ 2020-08-04   │\n│ 195476517 │ 2020-08-04 23:51:44 │ 2020-08-04   │\n│ 195475525 │ 2020-08-04 23:50:11 │ 2020-08-04   │\n│ 195468677 │ 2020-08-04 23:39:51 │ 2020-08-04   │\n│ 195465343 │ 2020-08-04 23:34:11 │ 2020-08-04   │\n│ 195460611 │ 2020-08-04 23:29:07 │ 2020-08-04   │\n│ 195452505 │ 2020-08-04 23:17:29 │ 2020-08-04   │\n│ 195447886 │ 2020-08-04 23:11:35 │ 2020-08-04   │\n│ 195435521 │ 2020-08-04 23:02:34 │ 2020-08-04   │\n│ 195433385 │ 2020-08-04 23:01:00 │ 2020-08-04   │\n│         … │ …                   │ …            │\n└───────────┴─────────────────────┴──────────────┘\n\n\n\nWe need to associate jobs and workflows somehow, so let’s join them on the relevant key fields.\n\njoined = jobs.join(workflows, jobs.run_id == workflows.id)\njoined\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ run_id    ┃ job_duration ┃ last_job_started_at ┃ last_job_completed_at ┃ id        ┃ run_started_at      ┃ … ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ int64     │ int64        │ timestamp           │ timestamp             │ int64     │ timestamp           │ … │\n├───────────┼──────────────┼─────────────────────┼───────────────────────┼───────────┼─────────────────────┼───┤\n│ 637414690 │            0 │ 2021-03-09 23:40:16 │ 2021-03-09 23:40:16   │ 637414690 │ 2021-03-09 22:59:31 │ … │\n│ 637412930 │   2146000000 │ 2021-03-09 23:38:33 │ 2021-03-10 00:17:30   │ 637412930 │ 2021-03-09 22:58:27 │ … │\n│ 637412930 │   1329000000 │ 2021-03-09 23:38:33 │ 2021-03-10 00:17:30   │ 637412930 │ 2021-03-09 22:58:27 │ … │\n│ 637412930 │   2979000000 │ 2021-03-09 23:38:33 │ 2021-03-10 00:17:30   │ 637412930 │ 2021-03-09 22:58:27 │ … │\n│ 637412930 │   1527000000 │ 2021-03-09 23:38:33 │ 2021-03-10 00:17:30   │ 637412930 │ 2021-03-09 22:58:27 │ … │\n│ 637412930 │   1585000000 │ 2021-03-09 23:38:33 │ 2021-03-10 00:17:30   │ 637412930 │ 2021-03-09 22:58:27 │ … │\n│ 637412930 │    985000000 │ 2021-03-09 23:38:33 │ 2021-03-10 00:17:30   │ 637412930 │ 2021-03-09 22:58:27 │ … │\n│ 637412930 │   2455000000 │ 2021-03-09 23:38:33 │ 2021-03-10 00:17:30   │ 637412930 │ 2021-03-09 22:58:27 │ … │\n│ 637412930 │   1305000000 │ 2021-03-09 23:38:33 │ 2021-03-10 00:17:30   │ 637412930 │ 2021-03-09 22:58:27 │ … │\n│ 637412930 │   1015000000 │ 2021-03-09 23:38:33 │ 2021-03-10 00:17:30   │ 637412930 │ 2021-03-09 22:58:27 │ … │\n│         … │            … │ …                   │ …                     │         … │ …                   │ … │\n└───────────┴──────────────┴─────────────────────┴───────────────────────┴───────────┴─────────────────────┴───┘\n\n\n\nSweet! Now we have workflow runs and job runs together in the same table, let’s start exploring summarization.\nLet’s encode our knowledge about when the poetry move happened and also when we moved to the team plan.\n\nfrom datetime import date\n\nPOETRY_MERGED_DATE = date(2021, 10, 15)\nTEAMIZATION_DATE = date(2022, 11, 28)\n\nLet’s compute some indicator variables indicating whether a given row contains data after poetry changes occurred, and do the same for the team plan.\nLet’s also compute queueing time and workflow duration.\n\nstats = joined.select(\n    _.started_date,\n    _.job_duration,\n    has_poetry=_.started_date &gt; POETRY_MERGED_DATE,\n    has_team=_.started_date &gt; TEAMIZATION_DATE,\n    queueing_time=_.last_job_started_at.cast(\"int\")\n    - _.run_started_at.cast(\"int\"),\n    workflow_duration=_.last_job_completed_at.cast(\"int\")\n    - _.run_started_at.cast(\"int\"),\n)\nstats\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ started_date ┃ job_duration ┃ has_poetry ┃ has_team ┃ queueing_time ┃ workflow_duration ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ date         │ int64        │ boolean    │ boolean  │ int64         │ int64             │\n├──────────────┼──────────────┼────────────┼──────────┼───────────────┼───────────────────┤\n│ 2021-08-02   │    286000000 │ False      │ False    │      12000000 │         823000000 │\n│ 2021-08-02   │    274000000 │ False      │ False    │      12000000 │         823000000 │\n│ 2021-08-02   │    397000000 │ False      │ False    │      12000000 │         823000000 │\n│ 2021-08-02   │    394000000 │ False      │ False    │      12000000 │         823000000 │\n│ 2021-08-02   │    709000000 │ False      │ False    │      12000000 │         823000000 │\n│ 2021-08-02   │    760000000 │ False      │ False    │      12000000 │         823000000 │\n│ 2021-08-02   │    717000000 │ False      │ False    │      12000000 │         823000000 │\n│ 2021-08-02   │    419000000 │ False      │ False    │      12000000 │         823000000 │\n│ 2021-08-02   │    503000000 │ False      │ False    │      12000000 │         823000000 │\n│ 2021-08-02   │    811000000 │ False      │ False    │      12000000 │         823000000 │\n│ …            │            … │ …          │ …        │             … │                 … │\n└──────────────┴──────────────┴────────────┴──────────┴───────────────┴───────────────────┘\n\n\n\nLet’s create a column ranging from 0 to 2 inclusive where:\n\n0: no improvements\n1: just poetry\n2: poetry and the team plan\n\nLet’s also give them some names that’ll look nice on our plots.\n\nstats = stats.mutate(\n    raw_improvements=_.has_poetry.cast(\"int\") + _.has_team.cast(\"int\")\n).mutate(\n    improvements=(\n        _.raw_improvements.case()\n        .when(0, \"None\")\n        .when(1, \"Poetry\")\n        .when(2, \"Poetry + Team Plan\")\n        .else_(\"NA\")\n        .end()\n    ),\n    team_plan=ibis.where(_.raw_improvements &gt; 1, \"Poetry + Team Plan\", \"None\"),\n)\nstats\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ started_date ┃ job_duration ┃ has_poetry ┃ has_team ┃ queueing_time ┃ workflow_duration ┃ raw_improvements ┃ … ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━┩\n│ date         │ int64        │ boolean    │ boolean  │ int64         │ int64             │ int64            │ … │\n├──────────────┼──────────────┼────────────┼──────────┼───────────────┼───────────────────┼──────────────────┼───┤\n│ 2021-08-22   │    388000000 │ False      │ False    │      21000000 │         850000000 │                0 │ … │\n│ 2021-08-22   │    392000000 │ False      │ False    │      21000000 │         850000000 │                0 │ … │\n│ 2021-08-22   │    502000000 │ False      │ False    │      21000000 │         850000000 │                0 │ … │\n│ 2021-08-22   │    515000000 │ False      │ False    │      21000000 │         850000000 │                0 │ … │\n│ 2021-08-22   │    392000000 │ False      │ False    │      21000000 │         850000000 │                0 │ … │\n│ 2021-08-22   │    341000000 │ False      │ False    │      21000000 │         850000000 │                0 │ … │\n│ 2021-08-22   │    743000000 │ False      │ False    │      21000000 │         850000000 │                0 │ … │\n│ 2021-08-22   │    630000000 │ False      │ False    │      21000000 │         850000000 │                0 │ … │\n│ 2021-08-22   │    750000000 │ False      │ False    │      21000000 │         850000000 │                0 │ … │\n│ 2021-08-22   │    777000000 │ False      │ False    │      21000000 │         850000000 │                0 │ … │\n│ …            │            … │ …          │ …        │             … │                 … │                … │ … │\n└──────────────┴──────────────┴────────────┴──────────┴───────────────┴───────────────────┴──────────────────┴───┘\n\n\n\nFinally, we can summarize by averaging the different durations, grouping on the variables of interest.\n\nUSECS_PER_MIN = 60_000_000\n\nagged = stats.group_by([_.started_date, _.improvements, _.team_plan]).agg(\n    job=_.job_duration.div(USECS_PER_MIN).mean(),\n    workflow=_.workflow_duration.div(USECS_PER_MIN).mean(),\n    queueing_time=_.queueing_time.div(USECS_PER_MIN).mean(),\n)\nagged\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ started_date ┃ improvements ┃ team_plan ┃ job       ┃ workflow  ┃ queueing_time ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ date         │ string       │ string    │ float64   │ float64   │ float64       │\n├──────────────┼──────────────┼───────────┼───────────┼───────────┼───────────────┤\n│ 2021-11-08   │ Poetry       │ None      │  5.076520 │ 25.813687 │     17.095093 │\n│ 2022-05-09   │ Poetry       │ None      │  3.645870 │ 17.563438 │     15.305701 │\n│ 2022-06-04   │ Poetry       │ None      │  3.641251 │ 12.041645 │     10.759055 │\n│ 2022-07-21   │ Poetry       │ None      │  3.236880 │ 13.001932 │     12.147463 │\n│ 2020-12-15   │ None         │ None      │ 13.788721 │ 59.767340 │     59.767340 │\n│ 2021-08-17   │ None         │ None      │  8.777027 │ 22.233333 │      7.607057 │\n│ 2021-09-23   │ None         │ None      │ 11.531302 │ 93.434379 │     93.059946 │\n│ 2020-09-01   │ None         │ None      │ 15.639095 │ 59.154527 │     58.680453 │\n│ 2021-06-24   │ None         │ None      │  8.442157 │ 13.697059 │      0.197059 │\n│ 2021-01-07   │ None         │ None      │ 12.555856 │ 48.162613 │     47.651351 │\n│ …            │ …            │ …         │         … │         … │             … │\n└──────────────┴──────────────┴───────────┴───────────┴───────────┴───────────────┘\n\n\n\nIf at any point you want to inspect the SQL you’ll be running, ibis has you covered with ibis.to_sql.\n\nsql = ibis.to_sql(agged)\nsql\n\nSELECT\n  t0.`started_date`,\n  t0.`improvements`,\n  t0.`team_plan`,\n  AVG(IEEE_DIVIDE(t0.`job_duration`, 60000000)) AS `job`,\n  AVG(IEEE_DIVIDE(t0.`workflow_duration`, 60000000)) AS `workflow`,\n  AVG(IEEE_DIVIDE(t0.`queueing_time`, 60000000)) AS `queueing_time`\nFROM (\n  SELECT\n    t1.*,\n    CASE t1.`raw_improvements`\n      WHEN 0\n      THEN 'None'\n      WHEN 1\n      THEN 'Poetry'\n      WHEN 2\n      THEN 'Poetry + Team Plan'\n      ELSE 'NA'\n    END AS `improvements`,\n    CASE WHEN t1.`raw_improvements` &gt; 1 THEN 'Poetry + Team Plan' ELSE 'None' END AS `team_plan`\n  FROM (\n    SELECT\n      t2.*,\n      CAST(t2.`has_poetry` AS INT64) + CAST(t2.`has_team` AS INT64) AS `raw_improvements`\n    FROM (\n      SELECT\n        `started_date`,\n        `job_duration`,\n        `started_date` &gt; CAST('2021-10-15' AS DATE) AS `has_poetry`,\n        `started_date` &gt; CAST('2022-11-28' AS DATE) AS `has_team`,\n        UNIX_MICROS(`last_job_started_at`) - UNIX_MICROS(`run_started_at`) AS `queueing_time`,\n        UNIX_MICROS(`last_job_completed_at`) - UNIX_MICROS(`run_started_at`) AS `workflow_duration`\n      FROM (\n        SELECT\n          t5.`run_id`,\n          UNIX_MICROS(t5.`completed_at`) - UNIX_MICROS(t5.`started_at`) AS `job_duration`,\n          MAX(t5.`started_at`) OVER (PARTITION BY t5.`run_id`) AS `last_job_started_at`,\n          MAX(t5.`completed_at`) OVER (PARTITION BY t5.`run_id`) AS `last_job_completed_at`\n        FROM `ibis-gbq.workflows.jobs` AS t5\n      ) AS t3\n      INNER JOIN (\n        SELECT\n          t5.`id`,\n          t5.`run_started_at`,\n          DATE(t5.`run_started_at`) AS `started_date`\n        FROM `ibis-gbq.workflows.workflows` AS t5\n      ) AS t4\n        ON t3.`run_id` = t4.`id`\n    ) AS t2\n  ) AS t1\n) AS t0\nGROUP BY\n  1,\n  2,\n  3"
  },
  {
    "objectID": "blog/rendered/ci-analysis.html#result-1-job-duration",
    "href": "blog/rendered/ci-analysis.html#result-1-job-duration",
    "title": "Analysis of Ibis’s CI Performance",
    "section": "Result #1: Job Duration",
    "text": "Result #1: Job Duration\nThis result is pretty interesting.\nA few things pop out to me right away:\n\nThe move to poetry decreased the average job run duration by quite a bit. No, I’m not going to do any statistical tests.\nThe variability of job run durations also decreased by quite a bit after introducing poetry.\nMoving to the team plan had little to no effect on job run duration.\n\n\n(\n    ggplot(\n        df.loc[df.entity != \"job\"].reset_index(drop=True),\n        aes(x=\"started_date\", y=\"duration\", color=\"factor(improvements)\"),\n    )\n    + facet_wrap(\"entity\", ncol=1)\n    + geom_point()\n    + geom_vline(\n        xintercept=[TEAMIZATION_DATE, POETRY_MERGED_DATE],\n        linetype=\"dashed\",\n    )\n    + scale_color_brewer(\n        palette=7,\n        type='qual',\n        limits=[\"None\", \"Poetry\", \"Poetry + Team Plan\"],\n    )\n    + geom_text(x=POETRY_MERGED_DATE, label=poetry_label, y=75, color=\"blue\")\n    + geom_text(x=TEAMIZATION_DATE, label=team_label, y=50, color=\"blue\")\n    + stat_smooth(method=\"lm\")\n    + labs(x=\"Date\", y=\"Duration (minutes)\")\n    + ggtitle(\"Workflow Duration\")\n    + theme(\n        figure_size=(22, 13),\n        legend_position=(0.68, 0.75),\n        legend_direction=\"vertical\",\n    )\n)"
  },
  {
    "objectID": "blog/rendered/ci-analysis.html#result-2-workflow-duration-and-queueing-time",
    "href": "blog/rendered/ci-analysis.html#result-2-workflow-duration-and-queueing-time",
    "title": "Analysis of Ibis’s CI Performance",
    "section": "Result #2: Workflow Duration and Queueing Time",
    "text": "Result #2: Workflow Duration and Queueing Time\nAnother interesting result.\n\nQueueing Time\n\nIt almost looks like moving to poetry made average queueing time worse. This is probably due to our perception that faster jobs means faster ci. As we see here that isn’t the case\nMoving to the team plan cut down the queueing time by quite a bit\n\n\n\nWorkflow Duration\n\nOverall workflow duration appears to be strongly influenced by moving to the team plan, which is almost certainly due to the drop in queueing time since we are no longer limited by slow job durations.\nPerhaps it’s obvious, but queueing time and workflow duration appear to be highly correlated.\n\nIn the next plot we’ll look at that correlation.\n\n(\n    ggplot(raw_df, aes(x=\"workflow\", y=\"queueing_time\"))\n    + geom_point()\n    + geom_rug()\n    + facet_grid(\". ~ team_plan\")\n    + labs(x=\"Workflow Duration (minutes)\", y=\"Queueing Time (minutes)\")\n    + ggtitle(\"Workflow Duration vs. Queueing Time\")\n    + theme(figure_size=(22, 6))\n)"
  },
  {
    "objectID": "blog/rendered/ci-analysis.html#result-3-workflow-duration-and-queueing-duration-are-correlated",
    "href": "blog/rendered/ci-analysis.html#result-3-workflow-duration-and-queueing-duration-are-correlated",
    "title": "Analysis of Ibis’s CI Performance",
    "section": "Result #3: Workflow Duration and Queueing Duration are correlated",
    "text": "Result #3: Workflow Duration and Queueing Duration are correlated\nIt also seems that moving to the team plan (though also the move to poetry might be related here) reduced the variability of both metrics.\nWe’re lacking data compared to the past so we should wait for more to come in."
  },
  {
    "objectID": "blog/rendered/ci-analysis.html#conclusions",
    "href": "blog/rendered/ci-analysis.html#conclusions",
    "title": "Analysis of Ibis’s CI Performance",
    "section": "Conclusions",
    "text": "Conclusions\nIt appears that you need both a short queue time and fast individual jobs to minimize time spent in CI.\nIf you have a short queue time, but long job runs then you’ll be bottlenecked on individual jobs, and if you have more jobs than queue slots then you’ll be blocked on queueing time.\nI think we can sum this up nicely:\n\nslow jobs, slow queue: 🤷 blocked by jobs or queue\nslow jobs, fast queue: ❓ blocked by jobs, if jobs are slow enough\nfast jobs, slow queue: ❗ blocked by queue, with enough jobs\nfast jobs, fast queue: ✅"
  },
  {
    "objectID": "blog/rendered/torch.html",
    "href": "blog/rendered/torch.html",
    "title": "Ibis on 🔥: Supercharge Your Workflow with DuckDB and PyTorch",
    "section": "",
    "text": "In this blog post we show how to leverage ecosystem tools to build an end-to-end ML pipeline using Ibis, DuckDB and PyTorch.\nCheck out the live stream of this notebook here: https://www.youtube.com/live/L4_deAdStKs?feature=share\nLet’s get started!\nimport ibis\nimport ibis.expr.datatypes as dt\n\nfrom ibis import _, selectors as s, udf\nibis.options.interactive = True"
  },
  {
    "objectID": "blog/rendered/torch.html#define-a-function-to-clean-inputs",
    "href": "blog/rendered/torch.html#define-a-function-to-clean-inputs",
    "title": "Ibis on 🔥: Supercharge Your Workflow with DuckDB and PyTorch",
    "section": "Define a Function to Clean Inputs",
    "text": "Define a Function to Clean Inputs\nLet’s define a function to clean the data in a few different ways:\n\nRemove outliers (Z-score based)\nRemove negative trip distances and negative fare amounts\nCast inputs to float32, since that’s what PyTorch wants\n\nWe use a function here to ensure that we can run the same code on the test data set before prediction.\n\ndef clean_input(path):\n    return (\n        # load parquet\n        ibis.read_parquet(path)\n        # compute fare_amount_zscore and trip_distance_zscore\n        .mutate(s.across([\"fare_amount\", \"trip_distance\"], dict(zscore=(_ - _.mean()) / _.std())))\n        # filter out negative trip distance and bizarre transactions\n        .filter([_.trip_distance &gt; 0.0, _.fare_amount &gt;= 0.0])\n        # keep values that within 2 standard deviations\n        .filter(s.if_all(s.endswith(\"_zscore\"), _.abs() &lt;= 2))\n        # drop columns that aren't necessary for further analysis\n        .drop(s.endswith(\"_zscore\"))\n        # select the columns we care about\n        .select(s.across([\"fare_amount\", \"trip_distance\"], _.cast(\"float32\")))\n    )\n\n\ntraining_data = clean_input(\"/home/cloud/data/trip-data/yellow_tripdata_2016-01.parquet\")\n\n\ntraining_data\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ trip_distance ┃ fare_amount ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n│ float32       │ float32     │\n├───────────────┼─────────────┤\n│          3.20 │        14.0 │\n│          1.00 │         9.5 │\n│          0.90 │         6.0 │\n│          0.80 │         5.0 │\n│          1.80 │        11.0 │\n│          2.30 │        11.0 │\n│         13.80 │        43.0 │\n│          3.46 │        20.0 │\n│          0.83 │         5.5 │\n│          0.87 │         7.0 │\n│             … │           … │\n└───────────────┴─────────────┘"
  },
  {
    "objectID": "blog/rendered/torch.html#execute-the-query-and-convert-to-torch-tensors",
    "href": "blog/rendered/torch.html#execute-the-query-and-convert-to-torch-tensors",
    "title": "Ibis on 🔥: Supercharge Your Workflow with DuckDB and PyTorch",
    "section": "Execute the Query and Convert to Torch Tensors",
    "text": "Execute the Query and Convert to Torch Tensors\nNew in Ibis 6.0 is the to_torch method, which executes a query and returns the results as a dictionary of torch.Tensors keyed by column names.\nWe’ll use that to get our input data for training.\n\ntorch_training_data = training_data.to_torch()  # type: dict[str, Tensor]\n\n\ntorch_training_data\n\n{'trip_distance': tensor([3.2000, 1.0000, 0.9000,  ..., 5.6300, 0.7700, 1.2600]),\n 'fare_amount': tensor([14.0000,  9.5000,  6.0000,  ..., 18.5000,  5.0000,  6.5000])}"
  },
  {
    "objectID": "blog/rendered/torch.html#train-the-model",
    "href": "blog/rendered/torch.html#train-the-model",
    "title": "Ibis on 🔥: Supercharge Your Workflow with DuckDB and PyTorch",
    "section": "Train the Model",
    "text": "Train the Model\nLet’s assume for now we don’t have access to the model code. Maybe your co-worker wrote the model or it’s part of an API that you don’t control. Either way, it’s a black box to us.\nThe API looks like this:\nclass PredictCabFare:\n    def __init__(self, data: dict[str, torch.Tensor]) -&gt; None:\n        \"\"\"Initialize the model with training data.\"\"\"\n\n    def train(self) -&gt; None:\n        \"\"\"Train the model.\"\"\"\n\n    def __call__(self, input: pyarrow.ChunkedArray) -&gt; pyarrow.Array:\n        \"\"\"Invoke the trained model on unseen input.\"\"\"\n\nfrom model import PredictCabFare\n\n\nmodel = PredictCabFare(torch_training_data)\nmodel.train()\n\n100%|███████████████████████████████████████████████████████████████████████████████| 100/100 [00:04&lt;00:00, 22.46it/s]"
  },
  {
    "objectID": "blog/rendered/torch.html#define-an-ibis-udf-that-predicts-fares",
    "href": "blog/rendered/torch.html#define-an-ibis-udf-that-predicts-fares",
    "title": "Ibis on 🔥: Supercharge Your Workflow with DuckDB and PyTorch",
    "section": "Define an Ibis UDF that predicts fares",
    "text": "Define an Ibis UDF that predicts fares\nNow we get to the meaty part: defining an Ibis UDF (user-defined function) that invokes our model on unseen data!\n\nfrom ibis.expr.operations import udf\n\n\n@udf.scalar.pyarrow\ndef predict_fare(distance: dt.float64) -&gt; dt.float32:\n    return model(distance)\n\n\n## Visualize the comparison between the predicted cab fares and the actual cab fares\nprediction = (\n    clean_input(\"/home/cloud/data/trip-data/yellow_tripdata_2016-02.parquet\")\n    .limit(10_000)\n    .mutate(predicted_fare=lambda t: predict_fare(t.trip_distance.cast(\"float32\")))\n)\nprediction\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ trip_distance ┃ fare_amount ┃ predicted_fare ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ float32       │ float32     │ float32        │\n├───────────────┼─────────────┼────────────────┤\n│           9.1 │        27.0 │      29.008099 │\n│           3.3 │        11.5 │      12.776804 │\n│           0.5 │         4.0 │       4.941007 │\n│           7.4 │        26.5 │      24.250650 │\n│           1.6 │         7.5 │       8.019356 │\n│           3.8 │        16.0 │      14.176053 │\n│           1.1 │         6.0 │       6.620107 │\n│           6.8 │        21.0 │      22.571550 │\n│           2.9 │        12.0 │      11.657405 │\n│           1.2 │         6.5 │       6.899957 │\n│             … │           … │              … │\n└───────────────┴─────────────┴────────────────┘"
  },
  {
    "objectID": "blog/rendered/torch.html#prepare-the-data-for-plotting",
    "href": "blog/rendered/torch.html#prepare-the-data-for-plotting",
    "title": "Ibis on 🔥: Supercharge Your Workflow with DuckDB and PyTorch",
    "section": "Prepare the Data for Plotting",
    "text": "Prepare the Data for Plotting\nHere we tidy up our data to make it easier to adjust plotting style based on the data.\nIn this case, we’re interested in visually distinguishing the model’s predicted fare amount from the actual fare amount so we pivot the data into a longer form which adds a string column metric that indicates the kind of fare a given row contains.\n\npivoted_prediction = prediction.pivot_longer(\n    s.contains(\"fare\"),\n    values_to=\"fare\",\n    names_to=\"metric\",\n)\n\n\npivoted_prediction\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ trip_distance ┃ metric         ┃ fare      ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ float32       │ string         │ float32   │\n├───────────────┼────────────────┼───────────┤\n│           9.1 │ fare_amount    │ 27.000000 │\n│           9.1 │ predicted_fare │ 29.008099 │\n│           3.3 │ fare_amount    │ 11.500000 │\n│           3.3 │ predicted_fare │ 12.776804 │\n│           0.5 │ fare_amount    │  4.000000 │\n│           0.5 │ predicted_fare │  4.941007 │\n│           7.4 │ fare_amount    │ 26.500000 │\n│           7.4 │ predicted_fare │ 24.250650 │\n│           1.6 │ fare_amount    │  7.500000 │\n│           1.6 │ predicted_fare │  8.019356 │\n│             … │ …              │         … │\n└───────────────┴────────────────┴───────────┘"
  },
  {
    "objectID": "blog/rendered/torch.html#plot-the-results",
    "href": "blog/rendered/torch.html#plot-the-results",
    "title": "Ibis on 🔥: Supercharge Your Workflow with DuckDB and PyTorch",
    "section": "Plot the Results",
    "text": "Plot the Results\nThere are a bunch of strange and interesting data points and observations that don’t have an obvious explanation:\n\nThere seem to be a good number of \\$50-ish rides regardless of distance. What’s going on there?\nWhat’s going on with the extreme outliers? For instance, the 50 mile ride that only cost about \\$60 or the 25 mile ride that cost about \\$140.\n\n\nfrom plotnine import aes, ggtitle, ggplot, geom_point, xlab, ylab\n\n(\n    ggplot(pivoted_prediction, aes(x=\"trip_distance\", y=\"fare\", color=\"metric\"))\n    + geom_point()\n    + xlab(\"Trip Distance\")\n    + ylab(\"Fare\")\n    + ggtitle(\"Predicted Fare vs Actual Fare by Trip Distance\")\n)\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;"
  },
  {
    "objectID": "blog/rendered/ibis-version-6.0.0-release.html",
    "href": "blog/rendered/ibis-version-6.0.0-release.html",
    "title": "Ibis v6.0.0",
    "section": "",
    "text": "03 July 2023"
  },
  {
    "objectID": "blog/rendered/ibis-version-6.0.0-release.html#overview",
    "href": "blog/rendered/ibis-version-6.0.0-release.html#overview",
    "title": "Ibis v6.0.0",
    "section": "Overview",
    "text": "Overview\nIbis 6.0.0 adds the Oracle backend, revamped UDF support, and many new features. This release also includes a number of refactors, bug fixes, and performance improvements. You can view the full changelog in the release notes.\nIf you’re new to Ibis, see how to install and the getting started tutorial.\nTo follow along with this blog, ensure you’re on 'ibis-framework&gt;=6,&lt;7'. First, we’ll setup Ibis and fetch some sample data to use.\n\nimport ibis\nibis.__version__\n\n'6.0.0'\n\n\n\nimport ibis.selectors as s\n\nibis.options.interactive = True\n\nt = ibis.examples.penguins.fetch()\nt.limit(3)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "blog/rendered/ibis-version-6.0.0-release.html#breaking-changes",
    "href": "blog/rendered/ibis-version-6.0.0-release.html#breaking-changes",
    "title": "Ibis v6.0.0",
    "section": "Breaking changes",
    "text": "Breaking changes\n\nJoin duplicate column names\nPreviously when joining tables with duplicate column names, _x and _y suffixes would be appended by default to the left and right tables respectively. You could override this with the suffix argument, which is now removed in favor of lname and rname arguments. The default is changed to no suffix for the left table and _right for the right table.\n\nt.join(t, \"island\").select(s.startswith(\"species\")).limit(3)\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ species ┃ species_right ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string  │ string        │\n├─────────┼───────────────┤\n│ Adelie  │ Adelie        │\n│ Adelie  │ Adelie        │\n│ Adelie  │ Adelie        │\n└─────────┴───────────────┘\n\n\n\nTo replicate the previous behavior:\n\nt.join(t, \"island\", lname=\"{name}_x\", rname=\"{name}_y\").select(\n    s.startswith(\"species\")\n).limit(3)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ species_x ┃ species_y ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string    │ string    │\n├───────────┼───────────┤\n│ Adelie    │ Adelie    │\n│ Adelie    │ Adelie    │\n│ Adelie    │ Adelie    │\n└───────────┴───────────┘\n\n\n\n\n\n.count() column names no longer named count automatically\nColumns created with the .count() aggregation are no longer automatically named count. This is to follow convention with other aggregations and reduce the likelihood of name collisions.\n\nt.group_by(\"species\").agg(ibis._.species.count())\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ species   ┃ Count(species) ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string    │ int64          │\n├───────────┼────────────────┤\n│ Adelie    │            152 │\n│ Gentoo    │            124 │\n│ Chinstrap │             68 │\n└───────────┴────────────────┘\n\n\n\nTo reproduce the old behavior, you can rename the column to count with:\n\nt.group_by(\"species\").agg(count=ibis._.species.count())\n\n┏━━━━━━━━━━━┳━━━━━━━┓\n┃ species   ┃ count ┃\n┡━━━━━━━━━━━╇━━━━━━━┩\n│ string    │ int64 │\n├───────────┼───────┤\n│ Adelie    │   152 │\n│ Gentoo    │   124 │\n│ Chinstrap │    68 │\n└───────────┴───────┘"
  },
  {
    "objectID": "blog/rendered/ibis-version-6.0.0-release.html#backends",
    "href": "blog/rendered/ibis-version-6.0.0-release.html#backends",
    "title": "Ibis v6.0.0",
    "section": "Backends",
    "text": "Backends\n\nOracle\nThe Oracle backend was added! See the Voltron Data blog for more details.\n\nibis.connect(f\"oracle://user:password@host\")\n\n&lt;ibis.backends.oracle.Backend at 0x14638a350&gt;\n\n\n\n\nDuckDB\nThere were various DuckDB improvements, but one notable new feature is the ability to attach to a SQLite database through DuckDB. This allows you to run OLAP queries via DuckDB significantly faster on source data from SQLite.\nFirst we’ll create a DuckDB connection and show it has no tables:\n\nduckdb_con = ibis.connect(\"duckdb://\")\nduckdb_con.list_tables()\n\n[]\n\n\nThen create a SQLite database with a table:\n\nsqlite_con = ibis.connect(\"sqlite://penguins.db\")\nsqlite_con.create_table(\"penguins\", t.to_pyarrow(), overwrite=True);\n\nAnd attach it:\n\nduckdb_con.attach_sqlite(\"./penguins.db\")\nduckdb_con.list_tables()\n\n['penguins']\n\n\n\nMotherDuck support!\nMotherDuck launched recently and is now supported in Ibis!\nSimply connect with the DuckDB backend using md: or motherduck: as the database.\n\nibis.connect(\"duckdb://md:\")\n\n&lt;ibis.backends.duckdb.Backend at 0x146154fd0&gt;\n\n\n\n\n\nPolars\nThe Polars backend received many improvements from community members @alexander-beedie and @mesejo, with plenty of operations now supported.\nSome additions in this version include:\n\nany and all reductions\nargmin and argmax\nidentical_to\ncorr\nsupport for .sql()\n\nGive it a try by setting your backend to Polars with ibis.set_backend(\"polars\")."
  },
  {
    "objectID": "blog/rendered/ibis-version-6.0.0-release.html#functionality",
    "href": "blog/rendered/ibis-version-6.0.0-release.html#functionality",
    "title": "Ibis v6.0.0",
    "section": "Functionality",
    "text": "Functionality\n\nUDFs\nUser-defined functions (UDFs) have been revamped with a new syntax and new backends added. To get started, import the decorator:\n\nfrom ibis import udf\n\nDefine a UDF:\n\n@udf.scalar.python\ndef num_vowels(s: str, include_y: bool = False) -&gt; int:\n    return sum(map(s.lower().count, \"aeiou\" + (\"y\" * include_y)))\n\nAnd call it:\n\nnum_vowels(t[:1].species.execute()[0])\n\n\n\n\n\n4\n\n\n\n\nt.group_by(num_vowels=num_vowels(t.species)).agg(\n    num_vowels_island_count=t.island.count()\n)\n\n┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ num_vowels ┃ num_vowels_island_count ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64      │ int64                   │\n├────────────┼─────────────────────────┤\n│          4 │                     152 │\n│          3 │                     124 │\n│          2 │                      68 │\n└────────────┴─────────────────────────┘\n\n\n\n\nt.filter(num_vowels(t.species) &lt; 4).limit(3)\n\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Gentoo  │ Biscoe │           46.1 │          13.2 │               211 │        4500 │ female │  2007 │\n│ Gentoo  │ Biscoe │           50.0 │          16.3 │               230 │        5700 │ male   │  2007 │\n│ Gentoo  │ Biscoe │           48.7 │          14.1 │               210 │        4450 │ female │  2007 │\n└─────────┴────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\nto_torch API\nA new to_torch output method was added. Combined with UDFs, this brings powerful ML capabilities into Ibis. See a complete example in the Ibis + DuckDB + PyTorch blog.\n\nimport torch\n\ntorch.set_printoptions(threshold=10)\n\n\nt.select(s.numeric()).to_torch()\n\n{'bill_length_mm': tensor([39.1000, 39.5000, 40.3000,  ..., 49.6000, 50.8000, 50.2000],\n        dtype=torch.float64),\n 'bill_depth_mm': tensor([18.7000, 17.4000, 18.0000,  ..., 18.2000, 19.0000, 18.7000],\n        dtype=torch.float64),\n 'flipper_length_mm': tensor([181, 186, 195,  ..., 193, 210, 198]),\n 'body_mass_g': tensor([3750, 3800, 3250,  ..., 3775, 4100, 3775]),\n 'year': tensor([2007, 2007, 2007,  ..., 2009, 2009, 2009])}\n\n\n\n\nArray zip support\nA new zip operation was added on array data types, allowing you to zip together multiple arrays.\n\narrays = ibis.memtable(\n    {\"numbers\": [[3, 2], [], None], \"strings\": [[\"a\", \"c\"], None, [\"e\"]]}\n)\narrays\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ numbers      ┃ strings       ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt; │ array&lt;string&gt; │\n├──────────────┼───────────────┤\n│ [3, 2]       │ ['a', 'c']    │\n│ []           │ NULL          │\n│ NULL         │ ['e']         │\n└──────────────┴───────────────┘\n\n\n\n\narrays.numbers.zip(arrays.strings).execute()\n\n0    [{'f1': 3, 'f2': 'a'}, {'f1': 2, 'f2': 'c'}]\n1                                              []\n2                       [{'f1': None, 'f2': 'e'}]\nName: ArrayZip(), dtype: object\n\n\n\narrays.numbers.zip(arrays.strings).unnest()\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayZip()                    ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ struct&lt;f1: int64, f2: string&gt; │\n├───────────────────────────────┤\n│ {'f1': 3, 'f2': 'a'}          │\n│ {'f1': 2, 'f2': 'c'}          │\n│ {'f1': None, 'f2': 'e'}       │\n└───────────────────────────────┘\n\n\n\n\n\nTry cast support\nA new try_cast() operation was added that allows you to cast a column to a type, but return null if the cast fails.\n\nibis.literal(\"a\").try_cast(\"int\")\n\n\n\n\n\nNone\n\n\n\n\nibis.literal(0).try_cast(\"float\")\n\n\n\n\n\n0.0\n\n\n\n\n\n__dataframe__ support\nIbis now supports the dataframe interchange protocol, allowing Ibis expressions to be used in any framework that supports it. Adoption of the protocol is still in its early stages, but we expect this to enable Ibis to be used in many new places going forward.\n\nt.__dataframe__()\n\n&lt;pyarrow.interchange.dataframe._PyArrowDataFrame at 0x29a2bfd50&gt;\n\n\n\n\nStreamlit experimental connection interface\nA new experimental connection interface was added for Streamlit. See how-to write a Streamlit app with Ibis.\n\n\nSQL dialect parameter\nIn SQL methods, you can now pass the dialect parameter to specify the SQL dialect used. This leverages sqlglot under the hood.\n\nbigquery_sql = \"\"\"SELECT\n  t0.`species`,\n  COUNT(t0.`species`) AS `count`,\n  CAST(COUNT(DISTINCT t0.`island`) AS FLOAT64) AS `islands`\nFROM penguins AS t0\nGROUP BY\n  1\n\"\"\"\n\nduckdb_con.sql(bigquery_sql, dialect=\"bigquery\")\n\n┏━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━┓\n┃ species   ┃ count ┃ islands ┃\n┡━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━┩\n│ string    │ int64 │ float64 │\n├───────────┼───────┼─────────┤\n│ Adelie    │   152 │     3.0 │\n│ Gentoo    │   124 │     1.0 │\n│ Chinstrap │    68 │     1.0 │\n└───────────┴───────┴─────────┘\n\n\n\n\n\nDelta Lake read/write support for some backends\nDelta Lake tables are supported through the deltalake package with read_delta() implemented for DuckDB, Polars, and DataFusion.\n\nt.to_delta(\"penguins.delta\", mode=\"overwrite\")\n\n\nt = ibis.read_delta(\"penguins.delta\")\nt.limit(3)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\nSelectors\nSome minor selectors improvements were added including the ability to use abstract type names and lists of strings.\n\nt.select(s.of_type(\"string\")).limit(3)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┓\n┃ species ┃ island    ┃ sex    ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━┩\n│ string  │ string    │ string │\n├─────────┼───────────┼────────┤\n│ Adelie  │ Torgersen │ male   │\n│ Adelie  │ Torgersen │ female │\n│ Adelie  │ Torgersen │ female │\n└─────────┴───────────┴────────┘\n\n\n\n\nt.agg(s.across([\"species\", \"island\"], ibis._.count()))\n\n┏━━━━━━━━━┳━━━━━━━━┓\n┃ species ┃ island ┃\n┡━━━━━━━━━╇━━━━━━━━┩\n│ int64   │ int64  │\n├─────────┼────────┤\n│     344 │    344 │\n└─────────┴────────┘"
  },
  {
    "objectID": "blog/rendered/ibis-version-6.0.0-release.html#refactors",
    "href": "blog/rendered/ibis-version-6.0.0-release.html#refactors",
    "title": "Ibis v6.0.0",
    "section": "Refactors",
    "text": "Refactors\nSeveral internal refactors that shouldn’t affect normal usage were made. See the release notes for more details."
  },
  {
    "objectID": "blog/rendered/ibis-version-6.0.0-release.html#wrapping-up",
    "href": "blog/rendered/ibis-version-6.0.0-release.html#wrapping-up",
    "title": "Ibis v6.0.0",
    "section": "Wrapping up",
    "text": "Wrapping up\nIbis v6.0.0 brings exciting new features that enable future support for ML and streaming workloads.\nAs always, try Ibis by installing and getting started.\nIf you run into any issues or find support is lacking for your backend, open an issue or discussion and let us know!"
  },
  {
    "objectID": "blog/rendered/campaign-finance.html",
    "href": "blog/rendered/campaign-finance.html",
    "title": "Exploring Campaign Finance Data",
    "section": "",
    "text": "Hi! My name is Nick Crews, and I’m a data engineer that looks at public campaign finance data.\nIn this post, I’ll walk through how I use Ibis to explore public campaign contribution data from the Federal Election Commission (FEC). We’ll do some loading, cleaning, featurizing, and visualization. There will be filtering, sorting, grouping, and aggregation."
  },
  {
    "objectID": "blog/rendered/campaign-finance.html#downloading-the-data",
    "href": "blog/rendered/campaign-finance.html#downloading-the-data",
    "title": "Exploring Campaign Finance Data",
    "section": "Downloading The Data",
    "text": "Downloading The Data\n\nfrom pathlib import Path\nfrom zipfile import ZipFile\nfrom urllib.request import urlretrieve\n\n# Download and unzip the 2018 individual contributions data\nurl = \"https://cg-519a459a-0ea3-42c2-b7bc-fa1143481f74.s3-us-gov-west-1.amazonaws.com/bulk-downloads/2018/indiv18.zip\"\nzip_path = Path(\"indiv18.zip\")\ncsv_path = Path(\"indiv18.csv\")\n\nif not zip_path.exists():\n    urlretrieve(url, zip_path)\n\nif not csv_path.exists():\n    with ZipFile(zip_path) as zip_file, csv_path.open(\"w\") as csv_file:\n        for line in zip_file.open(\"itcont.txt\"):\n            csv_file.write(line.decode())"
  },
  {
    "objectID": "blog/rendered/campaign-finance.html#loading-the-data",
    "href": "blog/rendered/campaign-finance.html#loading-the-data",
    "title": "Exploring Campaign Finance Data",
    "section": "Loading the data",
    "text": "Loading the data\nNow that we have our raw data in a .csv format, let’s load it into Ibis, using the duckdb backend.\nNote that a 4.3 GB .csv would be near the limit of what pandas could handle on my laptop with 16GB of RAM. In pandas, typically every time you perform a transformation on the data, a copy of the data is made. I could only do a few transformations before I ran out of memory.\nWith Ibis, this problem is solved in two different ways.\nFirst, because they are designed to work with very large datasets, many (all?) SQL backends support out of core operations. The data lives on disk, and are only loaded in a streaming fashion when needed, and then written back to disk as the operation is performed.\nSecond, unless you explicitly ask for it, Ibis makes use of lazy evaluation. This means that when you ask for a result, the result is not persisted in memory. Only the original source data is persisted. Everything else is derived from this on the fly.\n\nimport ibis\nfrom ibis import _\n\nibis.options.interactive = True\n\n# The raw .csv file doesn't have column names, so we will add them in the next step.\nraw = ibis.read_csv(csv_path)\nraw\n\n┏━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ column00  ┃ column01 ┃ column02 ┃ column03 ┃ column04           ┃ column05 ┃ column06 ┃ column07          ┃ column08     ┃ column09 ┃ column10  ┃ column11          ┃ column12                ┃ column13 ┃ column14 ┃ column15  ┃ column16        ┃ column17 ┃ column18 ┃ column19                                                        ┃ … ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string    │ string   │ string   │ string   │ int64              │ string   │ string   │ string            │ string       │ string   │ string    │ string            │ string                  │ string   │ int64    │ string    │ string          │ int64    │ string   │ string                                                          │ … │\n├───────────┼──────────┼──────────┼──────────┼────────────────────┼──────────┼──────────┼───────────────────┼──────────────┼──────────┼───────────┼───────────────────┼─────────────────────────┼──────────┼──────────┼───────────┼─────────────────┼──────────┼──────────┼─────────────────────────────────────────────────────────────────┼───┤\n│ C00401224 │ A        │ M6       │ P        │ 201804059101866001 │ 24T      │ IND      │ STOUFFER, LEIGH   │ AMSTELVEEN   │ ZZ       │ 1187RC    │ MYSELF            │ SELF EMPLOYED           │ 05172017 │       10 │ C00458000 │ SA11AI_81445687 │  1217152 │ ∅        │ EARMARKED FOR PROGRESSIVE CHANGE CAMPAIGN COMMITTEE (C00458000) │ … │\n│ C00401224 │ A        │ M6       │ P        │ 201804059101867748 │ 24T      │ IND      │ STRAWS, JOYCE     │ OCOEE        │ FL       │ 34761     │ SILVERSEA CRUISES │ RESERVATIONS SUPERVISOR │ 05182017 │       10 │ C00000935 │ SA11AI_81592336 │  1217152 │ ∅        │ EARMARKED FOR DCCC (C00000935)                                  │ … │\n│ C00401224 │ A        │ M6       │ P        │ 201804059101867748 │ 24T      │ IND      │ STRAWS, JOYCE     │ OCOEE        │ FL       │ 34761     │ SILVERSEA CRUISES │ RESERVATIONS SUPERVISOR │ 05192017 │       15 │ C00000935 │ SA11AI_81627562 │  1217152 │ ∅        │ EARMARKED FOR DCCC (C00000935)                                  │ … │\n│ C00401224 │ A        │ M6       │ P        │ 201804059101865942 │ 24T      │ IND      │ STOTT, JIM        │ CAPE NEDDICK │ ME       │ 039020760 │ NONE              │ NONE                    │ 05132017 │       35 │ C00000935 │ SA11AI_81047921 │  1217152 │ ∅        │ EARMARKED FOR DCCC (C00000935)                                  │ … │\n│ C00401224 │ A        │ M6       │ P        │ 201804059101865942 │ 24T      │ IND      │ STOTT, JIM        │ CAPE NEDDICK │ ME       │ 039020760 │ NONE              │ NONE                    │ 05152017 │       35 │ C00000935 │ SA11AI_81209209 │  1217152 │ ∅        │ EARMARKED FOR DCCC (C00000935)                                  │ … │\n│ C00401224 │ A        │ M6       │ P        │ 201804059101865942 │ 24T      │ IND      │ STOTT, JIM        │ CAPE NEDDICK │ ME       │ 039020760 │ NONE              │ NONE                    │ 05192017 │        5 │ C00000935 │ SA11AI_81605223 │  1217152 │ ∅        │ EARMARKED FOR DCCC (C00000935)                                  │ … │\n│ C00401224 │ A        │ M6       │ P        │ 201804059101865943 │ 24T      │ IND      │ STOTT, JIM        │ CAPE NEDDICK │ ME       │ 039020760 │ NONE              │ NONE                    │ 05242017 │       15 │ C00000935 │ SA11AI_82200022 │  1217152 │ ∅        │ EARMARKED FOR DCCC (C00000935)                                  │ … │\n│ C00401224 │ A        │ M6       │ P        │ 201804059101865943 │ 24T      │ IND      │ STOTT, JIM        │ CAPE NEDDICK │ ME       │ 03902     │ NOT EMPLOYED      │ NOT EMPLOYED            │ 05292017 │      100 │ C00213512 │ SA11AI_82589834 │  1217152 │ ∅        │ EARMARKED FOR NANCY PELOSI FOR CONGRESS (C00213512)             │ … │\n│ C00401224 │ A        │ M6       │ P        │ 201804059101865944 │ 24T      │ IND      │ STOTT, JIM        │ CAPE NEDDICK │ ME       │ 039020760 │ NONE              │ NONE                    │ 05302017 │       35 │ C00000935 │ SA11AI_82643727 │  1217152 │ ∅        │ EARMARKED FOR DCCC (C00000935)                                  │ … │\n│ C00401224 │ A        │ M6       │ P        │ 201804059101867050 │ 24T      │ IND      │ STRANGE, WINIFRED │ ANNA MSRIA   │ FL       │ 34216     │ NOT EMPLOYED      │ NOT EMPLOYED            │ 05162017 │       25 │ C00000935 │ SA11AI_81325918 │  1217152 │ ∅        │ EARMARKED FOR DCCC (C00000935)                                  │ … │\n│ …         │ …        │ …        │ …        │                  … │ …        │ …        │ …                 │ …            │ …        │ …         │ …                 │ …                       │ …        │        … │ …         │ …               │        … │ …        │ …                                                               │ … │\n└───────────┴──────────┴──────────┴──────────┴────────────────────┴──────────┴──────────┴───────────────────┴──────────────┴──────────┴───────────┴───────────────────┴─────────────────────────┴──────────┴──────────┴───────────┴─────────────────┴──────────┴──────────┴─────────────────────────────────────────────────────────────────┴───┘\n\n\n\n\n# For a more comprehesive description of the columns and their meaning, see\n# https://www.fec.gov/campaign-finance-data/contributions-individuals-file-description/\ncolumns = {\n    \"CMTE_ID\": \"keep\",  # Committee ID\n    \"AMNDT_IND\": \"drop\",  # Amendment indicator. A = amendment, N = new, T = termination\n    \"RPT_TP\": \"drop\",  # Report type (monthly, quarterly, etc)\n    \"TRANSACTION_PGI\": \"keep\",  # Primary/general indicator\n    \"IMAGE_NUM\": \"drop\",  # Image number\n    \"TRANSACTION_TP\": \"drop\",  # Transaction type\n    \"ENTITY_TP\": \"keep\",  # Entity type\n    \"NAME\": \"drop\",  # Contributor name\n    \"CITY\": \"keep\",  # Contributor city\n    \"STATE\": \"keep\",  # Contributor state\n    \"ZIP_CODE\": \"drop\",  # Contributor zip code\n    \"EMPLOYER\": \"drop\",  # Contributor employer\n    \"OCCUPATION\": \"drop\",  # Contributor occupation\n    \"TRANSACTION_DT\": \"keep\",  # Transaction date\n    \"TRANSACTION_AMT\": \"keep\",  # Transaction amount\n    # Other ID. For individual contributions will be null. For contributions from\n    # other FEC committees, will be the committee ID of the other committee.\n    \"OTHER_ID\": \"drop\",\n    \"TRAN_ID\": \"drop\",  # Transaction ID\n    \"FILE_NUM\": \"drop\",  # File number, unique number assigned to each report filed with the FEC\n    \"MEMO_CD\": \"drop\",  # Memo code\n    \"MEMO_TEXT\": \"drop\",  # Memo text\n    \"SUB_ID\": \"drop\",  # Submission ID. Unique number assigned to each transaction.\n}\n\nrenaming = {old: new for old, new in zip(raw.columns, columns.keys())}\nto_keep = [k for k, v in columns.items() if v == \"keep\"]\nkept = raw.relabel(renaming)[to_keep]\nkept\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ CMTE_ID   ┃ TRANSACTION_PGI ┃ ENTITY_TP ┃ CITY         ┃ STATE  ┃ TRANSACTION_DT ┃ TRANSACTION_AMT ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string    │ string          │ string    │ string       │ string │ string         │ int64           │\n├───────────┼─────────────────┼───────────┼──────────────┼────────┼────────────────┼─────────────────┤\n│ C00401224 │ P               │ IND       │ AMSTELVEEN   │ ZZ     │ 05172017       │              10 │\n│ C00401224 │ P               │ IND       │ OCOEE        │ FL     │ 05182017       │              10 │\n│ C00401224 │ P               │ IND       │ OCOEE        │ FL     │ 05192017       │              15 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05132017       │              35 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05152017       │              35 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05192017       │               5 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05242017       │              15 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05292017       │             100 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05302017       │              35 │\n│ C00401224 │ P               │ IND       │ ANNA MSRIA   │ FL     │ 05162017       │              25 │\n│ …         │ …               │ …         │ …            │ …      │ …              │               … │\n└───────────┴─────────────────┴───────────┴──────────────┴────────┴────────────────┴─────────────────┘\n\n\n\n\n# 21 million rows\nkept.count()\n\n\n\n\n\n21730731\n\n\n\nHuh, what’s up with those timings? Previewing the head only took a fraction of a second, but finding the number of rows took 10 seconds.\nThat’s because duckdb is scanning the .csv file on the fly every time we access it. So we only have to read the first few lines to get that preview, but we have to read the whole file to get the number of rows.\nNote that this isn’t a feature of Ibis, but a feature of Duckdb. This what I think is one of the strengths of Ibis: Ibis itself doesn’t have to implement any of the optimimizations or features of the backends. Those backends can focus on what they do best, and Ibis can get those things for free.\nSo, let’s tell duckdb to actually read in the file to its native format so later accesses will be faster. This will be a ~20 seconds that we’ll only have to pay once.\n\nkept = kept.cache()\nkept\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ CMTE_ID   ┃ TRANSACTION_PGI ┃ ENTITY_TP ┃ CITY         ┃ STATE  ┃ TRANSACTION_DT ┃ TRANSACTION_AMT ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string    │ string          │ string    │ string       │ string │ string         │ int64           │\n├───────────┼─────────────────┼───────────┼──────────────┼────────┼────────────────┼─────────────────┤\n│ C00401224 │ P               │ IND       │ AMSTELVEEN   │ ZZ     │ 05172017       │              10 │\n│ C00401224 │ P               │ IND       │ OCOEE        │ FL     │ 05182017       │              10 │\n│ C00401224 │ P               │ IND       │ OCOEE        │ FL     │ 05192017       │              15 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05132017       │              35 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05152017       │              35 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05192017       │               5 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05242017       │              15 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05292017       │             100 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05302017       │              35 │\n│ C00401224 │ P               │ IND       │ ANNA MSRIA   │ FL     │ 05162017       │              25 │\n│ …         │ …               │ …         │ …            │ …      │ …              │               … │\n└───────────┴─────────────────┴───────────┴──────────────┴────────┴────────────────┴─────────────────┘\n\n\n\nLook, now accessing it only takes a fraction of a second!\n\nkept.count()\n\n\n\n\n\n21730731\n\n\n\n\nCommittees Data\nThe contributions only list an opaque CMTE_ID column. We want to know which actual committee this is. Let’s load the committees table so we can lookup from committee ID to committee name.\n\ndef read_committees():\n    committees_url = \"https://cg-519a459a-0ea3-42c2-b7bc-fa1143481f74.s3-us-gov-west-1.amazonaws.com/bulk-downloads/2018/committee_summary_2018.csv\"\n    # This just creates a view, it doesn't actually fetch the data yet\n    tmp = ibis.read_csv(committees_url)\n    tmp = tmp[\"CMTE_ID\", \"CMTE_NM\"]\n    # The raw table contains multiple rows for each committee id, so lets pick\n    # an arbitrary row for each committee id as the representative name.\n    deduped = tmp.group_by(\"CMTE_ID\").agg(CMTE_NM=_.CMTE_NM.arbitrary())\n    return deduped\n\n\ncomms = read_committees().cache()\ncomms\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ CMTE_ID   ┃ CMTE_NM                                  ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string                                   │\n├───────────┼──────────────────────────────────────────┤\n│ C00097238 │ VIAD CORP GOOD GOVERNMENT PROJECT        │\n│ C00497859 │ HALL FOR CONGRESS                        │\n│ C00639914 │ TOM K. PAPPAS FOR CONGRESS               │\n│ C00663401 │ ENGAGING LEADERS OF THE FUTURE PAC       │\n│ C00020453 │ CASS COUNTY REPUBLICAN CENTRAL COMMITTEE │\n│ C00591909 │ RODNEY FRELINGHUYSEN VICTORY FUND        │\n│ C00635094 │ CAPITO VICTORY COMMITTEE                 │\n│ C00658310 │ ROBERT BARR FOR CONGRESS                 │\n│ C00657965 │ NEW ELECTORATE                           │\n│ C00485045 │ DISTRICT COUNCIL 9 POLITICAL ACTION FUND │\n│ …         │ …                                        │\n└───────────┴──────────────────────────────────────────┘\n\n\n\nNow add a the committee name to the contributions table:\n\ntogether = kept.left_join(comms, \"CMTE_ID\").drop(\"CMTE_ID_x\", \"CMTE_ID_y\")\ntogether\n\n┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n┃ TRANSACTION_PGI ┃ ENTITY_TP ┃ CITY         ┃ STATE  ┃ TRANSACTION_DT ┃ TRANSACTION_AMT ┃ CMTE_NM ┃\n┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n│ string          │ string    │ string       │ string │ string         │ int64           │ string  │\n├─────────────────┼───────────┼──────────────┼────────┼────────────────┼─────────────────┼─────────┤\n│ P               │ IND       │ AMSTELVEEN   │ ZZ     │ 05172017       │              10 │ ACTBLUE │\n│ P               │ IND       │ OCOEE        │ FL     │ 05182017       │              10 │ ACTBLUE │\n│ P               │ IND       │ OCOEE        │ FL     │ 05192017       │              15 │ ACTBLUE │\n│ P               │ IND       │ CAPE NEDDICK │ ME     │ 05132017       │              35 │ ACTBLUE │\n│ P               │ IND       │ CAPE NEDDICK │ ME     │ 05152017       │              35 │ ACTBLUE │\n│ P               │ IND       │ CAPE NEDDICK │ ME     │ 05192017       │               5 │ ACTBLUE │\n│ P               │ IND       │ CAPE NEDDICK │ ME     │ 05242017       │              15 │ ACTBLUE │\n│ P               │ IND       │ CAPE NEDDICK │ ME     │ 05292017       │             100 │ ACTBLUE │\n│ P               │ IND       │ CAPE NEDDICK │ ME     │ 05302017       │              35 │ ACTBLUE │\n│ P               │ IND       │ ANNA MSRIA   │ FL     │ 05162017       │              25 │ ACTBLUE │\n│ …               │ …         │ …            │ …      │ …              │               … │ …       │\n└─────────────────┴───────────┴──────────────┴────────┴────────────────┴─────────────────┴─────────┘"
  },
  {
    "objectID": "blog/rendered/campaign-finance.html#cleaning",
    "href": "blog/rendered/campaign-finance.html#cleaning",
    "title": "Exploring Campaign Finance Data",
    "section": "Cleaning",
    "text": "Cleaning\nFirst, let’s drop any contributions that don’t have a committee name. There are only 6 of them.\n\n# We can do this fearlessly, no .copy() needed, because\n# everything in Ibis is immutable. If we did this in pandas,\n# we might start modifying the original DataFrame accidentally!\ncleaned = together\n\nhas_name = cleaned.CMTE_NM.notnull()\ncleaned = cleaned[has_name]\nhas_name.value_counts()\n\n┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ NotNull(CMTE_NM) ┃ NotNull(CMTE_NM)_count ┃\n┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean          │ int64                  │\n├──────────────────┼────────────────────────┤\n│ True             │               21730725 │\n│ False            │                      6 │\n└──────────────────┴────────────────────────┘\n\n\n\nLet’s look at the ENTITY_TP column. This represents the type of entity that made the contribution:\n\ntogether.ENTITY_TP.value_counts()\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ ENTITY_TP ┃ ENTITY_TP_count ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string    │ int64           │\n├───────────┼─────────────────┤\n│ IND       │        21687993 │\n│ ORG       │           18555 │\n│ CAN       │           13659 │\n│ PAC       │            3621 │\n│ ∅         │            5289 │\n│ COM       │             867 │\n│ PTY       │              49 │\n│ CCM       │             698 │\n└───────────┴─────────────────┘\n\n\n\nWe only care about contributions from individuals.\nOnce we filter on this column, the contents of it are irrelevant, so let’s drop it.\n\ncleaned = together[_.ENTITY_TP == \"IND\"].drop(\"ENTITY_TP\")\n\nIt looks like the TRANSACTION_DT column was a raw string like “MMDDYYYY”, so let’s convert that to a proper date type.\n\nfrom ibis.expr.types import StringValue, DateValue\n\n\ndef mmddyyyy_to_date(val: StringValue) -&gt; DateValue:\n    return val.cast(str).lpad(8, \"0\").to_timestamp(\"%m%d%Y\").date()\n\n\ncleaned = cleaned.mutate(date=mmddyyyy_to_date(_.TRANSACTION_DT)).drop(\"TRANSACTION_DT\")\ncleaned\n\n┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ TRANSACTION_PGI ┃ CITY         ┃ STATE  ┃ TRANSACTION_AMT ┃ CMTE_NM ┃ date       ┃\n┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━┩\n│ string          │ string       │ string │ int64           │ string  │ date       │\n├─────────────────┼──────────────┼────────┼─────────────────┼─────────┼────────────┤\n│ P               │ AMSTELVEEN   │ ZZ     │              10 │ ACTBLUE │ 2017-05-17 │\n│ P               │ OCOEE        │ FL     │              10 │ ACTBLUE │ 2017-05-18 │\n│ P               │ OCOEE        │ FL     │              15 │ ACTBLUE │ 2017-05-19 │\n│ P               │ CAPE NEDDICK │ ME     │              35 │ ACTBLUE │ 2017-05-13 │\n│ P               │ CAPE NEDDICK │ ME     │              35 │ ACTBLUE │ 2017-05-15 │\n│ P               │ CAPE NEDDICK │ ME     │               5 │ ACTBLUE │ 2017-05-19 │\n│ P               │ CAPE NEDDICK │ ME     │              15 │ ACTBLUE │ 2017-05-24 │\n│ P               │ CAPE NEDDICK │ ME     │             100 │ ACTBLUE │ 2017-05-29 │\n│ P               │ CAPE NEDDICK │ ME     │              35 │ ACTBLUE │ 2017-05-30 │\n│ P               │ ANNA MSRIA   │ FL     │              25 │ ACTBLUE │ 2017-05-16 │\n│ …               │ …            │ …      │               … │ …       │ …          │\n└─────────────────┴──────────────┴────────┴─────────────────┴─────────┴────────────┘\n\n\n\nThe TRANSACTION_PGI column represents the type (primary, general, etc) of election, and the year. But it seems to be not very consistent:\n\ncleaned.TRANSACTION_PGI.topk(10)\n\n┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ TRANSACTION_PGI ┃ count    ┃\n┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string          │ int64    │\n├─────────────────┼──────────┤\n│ P               │ 17013597 │\n│ G2018           │  2095123 │\n│ P2018           │  1677183 │\n│ P2020           │   208501 │\n│ O2018           │   161874 │\n│ S2017           │   124336 │\n│ G2017           │    98401 │\n│ P2022           │    91136 │\n│ P2017           │    61153 │\n│ R2017           │    54281 │\n└─────────────────┴──────────┘\n\n\n\n\ndef get_election_type(pgi: StringValue) -&gt; StringValue:\n    \"\"\"Use the first letter of the TRANSACTION_PGI column to determine the election type\n\n    If the first letter is not one of the known election stage, then return null.\n    \"\"\"\n    election_types = {\n        \"P\": \"primary\",\n        \"G\": \"general\",\n        \"O\": \"other\",\n        \"C\": \"convention\",\n        \"R\": \"runoff\",\n        \"S\": \"special\",\n        \"E\": \"recount\",\n    }\n    first_letter = pgi[0]\n    return first_letter.substitute(election_types, else_=ibis.NA)\n\n\ncleaned = cleaned.mutate(election_type=get_election_type(_.TRANSACTION_PGI)).drop(\n    \"TRANSACTION_PGI\"\n)\ncleaned\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ CITY         ┃ STATE  ┃ TRANSACTION_AMT ┃ CMTE_NM ┃ date       ┃ election_type ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string       │ string │ int64           │ string  │ date       │ string        │\n├──────────────┼────────┼─────────────────┼─────────┼────────────┼───────────────┤\n│ AMSTELVEEN   │ ZZ     │              10 │ ACTBLUE │ 2017-05-17 │ primary       │\n│ OCOEE        │ FL     │              10 │ ACTBLUE │ 2017-05-18 │ primary       │\n│ OCOEE        │ FL     │              15 │ ACTBLUE │ 2017-05-19 │ primary       │\n│ CAPE NEDDICK │ ME     │              35 │ ACTBLUE │ 2017-05-13 │ primary       │\n│ CAPE NEDDICK │ ME     │              35 │ ACTBLUE │ 2017-05-15 │ primary       │\n│ CAPE NEDDICK │ ME     │               5 │ ACTBLUE │ 2017-05-19 │ primary       │\n│ CAPE NEDDICK │ ME     │              15 │ ACTBLUE │ 2017-05-24 │ primary       │\n│ CAPE NEDDICK │ ME     │             100 │ ACTBLUE │ 2017-05-29 │ primary       │\n│ CAPE NEDDICK │ ME     │              35 │ ACTBLUE │ 2017-05-30 │ primary       │\n│ ANNA MSRIA   │ FL     │              25 │ ACTBLUE │ 2017-05-16 │ primary       │\n│ …            │ …      │               … │ …       │ …          │ …             │\n└──────────────┴────────┴─────────────────┴─────────┴────────────┴───────────────┘\n\n\n\nThat worked well! There are 0 nulls in the resulting column, so we always were able to determine the election type.\n\ncleaned.election_type.topk(10)\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ election_type ┃ count    ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string        │ int64    │\n├───────────────┼──────────┤\n│ primary       │ 19061954 │\n│ general       │  2216685 │\n│ other         │   161965 │\n│ special       │   149572 │\n│ runoff        │    69637 │\n│ convention    │    22453 │\n│ recount       │     5063 │\n│ ∅             │        0 │\n└───────────────┴──────────┘\n\n\n\nAbout 1/20 of transactions are negative. These could represent refunds, or they could be data entry errors. Let’s simply drop them to keep it simple.\n\nabove_zero = cleaned.TRANSACTION_AMT &gt; 0\ncleaned = cleaned[above_zero]\nabove_zero.value_counts()\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Greater(TRANSACTION_AMT, 0) ┃ Greater(TRANSACTION_AMT, 0)_count ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                     │ int64                             │\n├─────────────────────────────┼───────────────────────────────────┤\n│ True                        │                          20669810 │\n│ False                       │                           1018183 │\n└─────────────────────────────┴───────────────────────────────────┘"
  },
  {
    "objectID": "blog/rendered/campaign-finance.html#adding-features",
    "href": "blog/rendered/campaign-finance.html#adding-features",
    "title": "Exploring Campaign Finance Data",
    "section": "Adding Features",
    "text": "Adding Features\nNow that the data is cleaned up to a usable format, let’s add some features.\nFirst, it’s useful to categorize donations by size, placing them into buckets of small, medium, large, etc.\n\nedges = [\n    10,\n    50,\n    100,\n    500,\n    1000,\n    5000,\n]\nlabels = [\n    \"&lt;10\",\n    \"10-50\",\n    \"50-100\",\n    \"100-500\",\n    \"500-1000\",\n    \"1000-5000\",\n    \"5000+\",\n]\n\n\ndef bucketize(vals, edges, str_labels):\n    # Uses Ibis's .bucket() method to create a categorical column\n    int_labels = vals.bucket(edges, include_under=True, include_over=True)\n    # Map the integer labels to the string labels\n    int_to_str = {str(i): s for i, s in enumerate(str_labels)}\n    return int_labels.cast(str).substitute(int_to_str)\n\n\nfeatured = cleaned.mutate(amount_bucket=bucketize(_.TRANSACTION_AMT, edges, labels))\nfeatured\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ CITY         ┃ STATE  ┃ TRANSACTION_AMT ┃ CMTE_NM ┃ date       ┃ election_type ┃ amount_bucket ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string       │ string │ int64           │ string  │ date       │ string        │ string        │\n├──────────────┼────────┼─────────────────┼─────────┼────────────┼───────────────┼───────────────┤\n│ AMSTELVEEN   │ ZZ     │              10 │ ACTBLUE │ 2017-05-17 │ primary       │ 10-50         │\n│ OCOEE        │ FL     │              10 │ ACTBLUE │ 2017-05-18 │ primary       │ 10-50         │\n│ OCOEE        │ FL     │              15 │ ACTBLUE │ 2017-05-19 │ primary       │ 10-50         │\n│ CAPE NEDDICK │ ME     │              35 │ ACTBLUE │ 2017-05-13 │ primary       │ 10-50         │\n│ CAPE NEDDICK │ ME     │              35 │ ACTBLUE │ 2017-05-15 │ primary       │ 10-50         │\n│ CAPE NEDDICK │ ME     │               5 │ ACTBLUE │ 2017-05-19 │ primary       │ &lt;10           │\n│ CAPE NEDDICK │ ME     │              15 │ ACTBLUE │ 2017-05-24 │ primary       │ 10-50         │\n│ CAPE NEDDICK │ ME     │             100 │ ACTBLUE │ 2017-05-29 │ primary       │ 100-500       │\n│ CAPE NEDDICK │ ME     │              35 │ ACTBLUE │ 2017-05-30 │ primary       │ 10-50         │\n│ ANNA MSRIA   │ FL     │              25 │ ACTBLUE │ 2017-05-16 │ primary       │ 10-50         │\n│ …            │ …      │               … │ …       │ …          │ …             │ …             │\n└──────────────┴────────┴─────────────────┴─────────┴────────────┴───────────────┴───────────────┘"
  },
  {
    "objectID": "blog/rendered/campaign-finance.html#analysis",
    "href": "blog/rendered/campaign-finance.html#analysis",
    "title": "Exploring Campaign Finance Data",
    "section": "Analysis",
    "text": "Analysis\n\nBy donation size\nOne thing we can look at is the donation breakdown by size: - Are most donations small or large? - Where do politicians/committees get most of their money from? Large or small donations?\nWe also will compare performance of Ibis vs pandas during this groupby.\n\ndef summary_by(table, by):\n    return table.group_by(by).agg(\n        n_donations=_.count(),\n        total_amount=_.TRANSACTION_AMT.sum(),\n        mean_amount=_.TRANSACTION_AMT.mean(),\n        median_amount=_.TRANSACTION_AMT.approx_median(),\n    )\n\n\ndef summary_by_pandas(df, by):\n    return df.groupby(by, as_index=False).agg(\n        n_donations=(\"election_type\", \"count\"),\n        total_amount=(\"TRANSACTION_AMT\", \"sum\"),\n        mean_amount=(\"TRANSACTION_AMT\", \"mean\"),\n        median_amount=(\"TRANSACTION_AMT\", \"median\"),\n    )\n\n\n# persist the input data so the following timings of the group_by are accurate.\nsubset = featured[\"election_type\", \"amount_bucket\", \"TRANSACTION_AMT\"]\nsubset = subset.cache()\npandas_subset = subset.execute()\n\nLet’s take a look at what we are actually computing:\n\nby_type_and_bucket = summary_by(subset, [\"election_type\", \"amount_bucket\"])\nby_type_and_bucket\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ election_type ┃ amount_bucket ┃ n_donations ┃ total_amount ┃ mean_amount  ┃ median_amount ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string        │ string        │ int64       │ int64        │ float64      │ int64         │\n├───────────────┼───────────────┼─────────────┼──────────────┼──────────────┼───────────────┤\n│ primary       │ 10-50         │     8115404 │    187666261 │    23.124697 │            25 │\n│ primary       │ 50-100        │     2663933 │    155426540 │    58.344763 │            50 │\n│ primary       │ 100-500       │     3636287 │    637353634 │   175.275943 │           150 │\n│ primary       │ &lt;10           │     2423728 │     10080721 │     4.159180 │             5 │\n│ primary       │ 500-1000      │      634677 │    334630687 │   527.245649 │           500 │\n│ primary       │ 1000-5000     │      684755 │   1231394874 │  1798.299938 │          1005 │\n│ general       │ 1000-5000     │      246101 │    460025242 │  1869.253851 │          1965 │\n│ general       │ 100-500       │      700821 │    123174568 │   175.757530 │           150 │\n│ general       │ 500-1000      │      174182 │     91015697 │   522.532162 │           500 │\n│ primary       │ 5000+         │       44085 │   1558371116 │ 35349.237065 │         10000 │\n│ …             │ …             │           … │            … │            … │             … │\n└───────────────┴───────────────┴─────────────┴──────────────┴──────────────┴───────────────┘\n\n\n\nOK, now let’s do our timings.\nOne interesting thing to pay attention to here is the execution time for the following groupby. Before, we could get away with lazy execution: because we only wanted to preview the first few rows, we only had to compute the first few rows, so all our previews were very fast.\nBut now, as soon as we do a groupby, we have to actually go through the whole dataset in order to compute the aggregate per group. So this is going to be slower. BUT, duckdb is still quite fast. It only takes .4 seconds to groupby-agg all 20 million rows!\n\n%timeit summary_by(subset, [\"election_type\", \"amount_bucket\"]).execute()  # .execute() so we actually fetch the data\n\n532 ms ± 22.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nNow let’s try the same thing in pandas:\n\n%timeit summary_by_pandas(pandas_subset, [\"election_type\", \"amount_bucket\"])\n\n5.03 s ± 222 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nIt takes about 4 seconds, which is about 10 times slower than duckdb. At this scale, it again doesn’t matter, but you could imagine with a dataset much larger than this, it would matter.\nLet’s also think about memory usage:\n\npandas_subset.memory_usage(deep=True).sum() / 1e9  # GB\n\n2.782586797\n\n\nThe source dataframe is couple gigabytes, so probably during the groupby, the peak memory usage is going to be a bit higher than this. You could use a profiler such as FIL if you wanted an exact number, I was too lazy to use that here.\nAgain, this works on my laptop at this dataset size, but much larger than this and I’d start having problems. Duckdb on the other hand is designed around working out of core so it should scale to datasets into the hundreds of gigabytes, much larger than your computer’s RAM.\n\n\nBack to analysis\nOK, let’s plot the result of that groupby.\nSurprise! (Or maybe not…) Most donations are small. But most of the money comes from donations larger than $1000.\nWell if that’s the case, why do politicians spend so much time soliciting small donations? One explanation is that they can use the number of donations as a marketing pitch, to show how popular they are, and thus how viable of a candidate they are.\nThis also might explain whose interests are being served by our politicians.\n\nimport altair as alt\n\n# Save as .svg so we can embed in the docs, normally not needed if in a notebook.\n# This import is needed to add chromedriver to PATH so that altair_saver can find it.\nimport chromedriver_binary\nalt.renderers.enable(\"png\")\nalt.renderers.set_embed_options(width=500, height=600)\n\n# Do some bookkeeping so the buckets are displayed smallest to largest on the charts\nbucket_col = alt.Column(\"amount_bucket:N\", sort=labels)\n\nn_by_bucket = (\n    alt.Chart(by_type_and_bucket.execute())\n    .mark_bar()\n    .encode(\n        x=bucket_col,\n        y=\"n_donations:Q\",\n        color=\"election_type:N\",\n    )\n)\ntotal_by_bucket = (\n    alt.Chart(by_type_and_bucket.execute())\n    .mark_bar()\n    .encode(\n        x=bucket_col,\n        y=\"total_amount:Q\",\n        color=\"election_type:N\",\n    )\n)\nn_by_bucket | total_by_bucket\n\n\n\n\n\n\nBy election stage\nLet’s look at how donations break down by election stage. Do people donate differently for primary elections vs general elections?\nLet’s ignore everything but primary and general elections, since they are the most common, and arguably the most important.\n\ngb2 = by_type_and_bucket[_.election_type.isin((\"primary\", \"general\"))]\nn_donations_per_election_type = _.n_donations.sum().over(group_by=\"election_type\")\nfrac = _.n_donations / n_donations_per_election_type\ngb2 = gb2.mutate(frac_n_donations_per_election_type=frac)\ngb2\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ election_type ┃ amount_bucket ┃ n_donations ┃ total_amount ┃ mean_amount  ┃ median_amount ┃ frac_n_donations_per_election_type ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string        │ string        │ int64       │ int64        │ float64      │ int64         │ float64                            │\n├───────────────┼───────────────┼─────────────┼──────────────┼──────────────┼───────────────┼────────────────────────────────────┤\n│ primary       │ &lt;10           │     2423728 │     10080721 │     4.159180 │             5 │                           0.133151 │\n│ primary       │ 1000-5000     │      684755 │   1231394874 │  1798.299938 │          1007 │                           0.037618 │\n│ primary       │ 10-50         │     8115404 │    187666261 │    23.124697 │            25 │                           0.445831 │\n│ primary       │ 50-100        │     2663933 │    155426540 │    58.344763 │            50 │                           0.146347 │\n│ primary       │ 100-500       │     3636287 │    637353634 │   175.275943 │           150 │                           0.199764 │\n│ primary       │ 500-1000      │      634677 │    334630687 │   527.245649 │           500 │                           0.034867 │\n│ primary       │ 5000+         │       44085 │   1558371116 │ 35349.237065 │         10000 │                           0.002422 │\n│ general       │ 100-500       │      700821 │    123174568 │   175.757530 │           150 │                           0.317796 │\n│ general       │ &lt;10           │      115873 │       536742 │     4.632158 │             5 │                           0.052544 │\n│ general       │ 10-50         │      660787 │     14411588 │    21.809733 │            25 │                           0.299642 │\n│ …             │ …             │           … │            … │            … │             … │                                  … │\n└───────────────┴───────────────┴─────────────┴──────────────┴──────────────┴───────────────┴────────────────────────────────────┘\n\n\n\nIt looks like primary elections get a larger proportion of small donations.\n\nalt.Chart(gb2.execute()).mark_bar().encode(\n    x=\"election_type:O\",\n    y=\"frac_n_donations_per_election_type:Q\",\n    color=bucket_col,\n)\n\n\n\n\n\n\nBy recipient\nLet’s look at the top players. Who gets the most donations?\nFar and away it is ActBlue, which acts as a conduit for donations to Democratic interests.\nBeto O’Rourke is the top individual politician, hats off to him!\n\nby_recip = summary_by(featured, \"CMTE_NM\")\nby_recip\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ CMTE_NM                                                            ┃ n_donations ┃ total_amount ┃ mean_amount ┃ median_amount ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string                                                             │ int64       │ int64        │ float64     │ int64         │\n├────────────────────────────────────────────────────────────────────┼─────────────┼──────────────┼─────────────┼───────────────┤\n│ BRIDGESTONE AMERICAS INC. PAC                                      │         384 │        42632 │  111.020833 │            50 │\n│ CORELOGIC INC PAC                                                  │           4 │        11000 │ 2750.000000 │          2750 │\n│ CROWLEY MARITIME CORPORATION FEDERAL PAC                           │        1137 │       192839 │  169.603342 │           100 │\n│ NATIONAL PROPANE GAS ASSOCIATION POLITICAL ACTION COMMITTEE        │         475 │       198445 │  417.778947 │           152 │\n│ BWX TECHNOLOGIES, INC POLITICAL ACTION COMMITTEE                   │        4839 │       431450 │   89.160984 │            64 │\n│ CHEVRON EMPLOYEES POLITICAL ACTION COMMITTEE - CHEVRON CORPORATION │       12356 │       883848 │   71.531887 │            34 │\n│ EMILY'S LIST                                                       │      174381 │     29222927 │  167.580912 │            50 │\n│ OKLAHOMA STRONG LEADERSHIP PAC                                     │           2 │         5500 │ 2750.000000 │          2750 │\n│ AMERICANS UNITED IN SUPPORT OF DEMOCRACY                           │          64 │       256300 │ 4004.687500 │          5000 │\n│ AMERICAN SENIORS HOUSING ASSOCIATION (SENIORS HOUSING PAC)         │         341 │       805502 │ 2362.175953 │          2000 │\n│ …                                                                  │           … │            … │           … │             … │\n└────────────────────────────────────────────────────────────────────┴─────────────┴──────────────┴─────────────┴───────────────┘\n\n\n\n\ntop_recip = by_recip.order_by(ibis.desc(\"n_donations\")).head(10)\nalt.Chart(top_recip.execute()).mark_bar().encode(\n    x=alt.X(\"CMTE_NM:O\", sort=\"-y\"),\n    y=\"n_donations:Q\",\n)\n\n\n\n\n\n\nBy Location\nWhere are the largest donations coming from?\n\nf2 = featured.mutate(loc=_.CITY + \", \" + _.STATE).drop(\"CITY\", \"STATE\")\nby_loc = summary_by(f2, \"loc\")\n# Drop the places with a small number of donations so we're\n# resistant to outliers for the mean\nby_loc = by_loc[_.n_donations &gt; 1000]\nby_loc\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ loc                 ┃ n_donations ┃ total_amount ┃ mean_amount ┃ median_amount ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string              │ int64       │ int64        │ float64     │ int64         │\n├─────────────────────┼─────────────┼──────────────┼─────────────┼───────────────┤\n│ NORTHVILLE, MI      │        7926 │      1377925 │  173.848726 │            50 │\n│ MISSOULA, MT        │       19647 │      3628034 │  184.660966 │            27 │\n│ MIDDLETOWN, DE      │        1565 │       515802 │  329.585942 │            25 │\n│ RALEIGH, NC         │       36381 │      6424836 │  176.598664 │            25 │\n│ LOUISVILLE, KY      │       40812 │      8840210 │  216.608105 │            37 │\n│ JACKSON HEIGHTS, NY │        2555 │       266927 │  104.472407 │            34 │\n│ BROKEN ARROW, OK    │        3844 │       409471 │  106.522112 │            35 │\n│ ARLINGTON, MA       │       12807 │      1967560 │  153.631608 │            38 │\n│ CORALVILLE, IA      │        3318 │       284634 │   85.784810 │            25 │\n│ CORNISH, NH         │        3026 │       140696 │   46.495704 │            25 │\n│ …                   │           … │            … │           … │             … │\n└─────────────────────┴─────────────┴──────────────┴─────────────┴───────────────┘\n\n\n\n\ndef top_by(col):\n    top = by_loc.order_by(ibis.desc(col)).head(10)\n    return (\n        alt.Chart(top.execute())\n        .mark_bar()\n        .encode(\n            x=alt.X('loc:O', sort=\"-y\"),\n            y=col,\n        )\n    )\n\n\ntop_by(\"n_donations\") | top_by(\"total_amount\") | top_by(\"mean_amount\") | top_by(\n    \"median_amount\"\n)\n\n\n\n\n\n\nBy month\nWhen do the donations come in?\n\nby_month = summary_by(featured, _.date.month().name(\"month_int\"))\n# Sorta hacky, .substritute doesn't work to change dtypes (yet?)\n# so we cast to string and then do our mapping\nmonth_map = {\n    \"1\": \"Jan\",\n    \"2\": \"Feb\",\n    \"3\": \"Mar\",\n    \"4\": \"Apr\",\n    \"5\": \"May\",\n    \"6\": \"Jun\",\n    \"7\": \"Jul\",\n    \"8\": \"Aug\",\n    \"9\": \"Sep\",\n    \"10\": \"Oct\",\n    \"11\": \"Nov\",\n    \"12\": \"Dec\",\n}\nby_month = by_month.mutate(month_str=_.month_int.cast(str).substitute(month_map))\nby_month\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ month_int ┃ n_donations ┃ total_amount ┃ mean_amount ┃ median_amount ┃ month_str ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ int32     │ int64       │ int64        │ float64     │ int64         │ string    │\n├───────────┼─────────────┼──────────────┼─────────────┼───────────────┼───────────┤\n│         ∅ │        1514 │       250297 │  165.321664 │           100 │ ∅         │\n│         1 │      348979 │    174837854 │  500.998209 │           121 │ Jan       │\n│         2 │      581646 │    255997655 │  440.126219 │           100 │ Feb       │\n│         3 │     1042577 │    430906797 │  413.309326 │            81 │ Mar       │\n│         4 │     1088244 │    299252692 │  274.986760 │            50 │ Apr       │\n│         5 │     1374248 │    387317202 │  281.839378 │            48 │ May       │\n│         6 │     1667285 │    465305247 │  279.079610 │            44 │ Jun       │\n│         7 │     1607053 │    320528605 │  199.451172 │            35 │ Jul       │\n│         8 │     2023466 │    473544182 │  234.026261 │            35 │ Aug       │\n│         9 │     2583847 │    697888624 │  270.096729 │            38 │ Sep       │\n│         … │           … │            … │           … │             … │ …         │\n└───────────┴─────────────┴──────────────┴─────────────┴───────────────┴───────────┘\n\n\n\n\nmonths_in_order = list(month_map.values())\nalt.Chart(by_month.execute()).mark_bar().encode(\n    x=alt.X(\"month_str:O\", sort=months_in_order),\n    y=\"n_donations:Q\",\n)"
  },
  {
    "objectID": "blog/rendered/campaign-finance.html#conclusion",
    "href": "blog/rendered/campaign-finance.html#conclusion",
    "title": "Exploring Campaign Finance Data",
    "section": "Conclusion",
    "text": "Conclusion\nThanks for following along! I hope you’ve learned something about Ibis, and maybe even about campaign finance.\nIbis is a great tool for exploring data. I now find myself reaching for it when in the past I would have reached for pandas.\nSome of the highlights for me:\n\nFast, lazy execution, a great display format, and good type hinting/editor support for a great REPL experience.\nVery well thought-out API and semantics (e.g. isinstance(val, NumericValue)?? That’s beautiful!)\nFast and fairly complete string support, since I work with a lot of text data.\nExtremely responsive maintainers. Sometimes I’ve submitted multiple feature requests and bug reports in a single day, and a PR has been merged by the next day.\nEscape hatch to SQL. I didn’t have to use that here, but if something isn’t supported, you can always fall back to SQL.\n\nCheck out The Ibis Website for more information."
  },
  {
    "objectID": "blog/Ibis-version-3.1.0-release.html",
    "href": "blog/Ibis-version-3.1.0-release.html",
    "title": "Ibis v3.1.0",
    "section": "",
    "text": "by Marlene Mhangami\n25 July 2022\n\n\nIbis 3.1 has officially been released as the latest version of the package. With this release comes new convenience features, increased backend operation coverage and a plethora of bug fixes. As usual, a full list of the changes can be found in the project release notes here Let’s talk about some of the new changes 3.1 brings for Ibis users.\n\n\n\nThe first significant change to note is that, Ibis now provides a more convenient way to connect to a backend using the ibis.connect method. You can now use this function to connect to an appropriate backend using a connection string.\nHere are some examples:\n\n=== “DuckDB”\nInitialize a DuckDB instance using `'duckdb://:memory:'`\n~~~python\nconn = ibis.connect('duckdb://:memory:')\n~~~\nAnd begin registering your tables:\n~~~python\nconn.register('csv://farm_data/dates.csv', 'dates')\nconn.register('csv://farm_data/farmer_groups.csv', 'farmer_groups')\nconn.register('csv://farm_data/crops.csv', 'crops')\nconn.register('csv://farm_data/farms.csv', 'farms')\nconn.register('csv://farm_data/harvest.csv', 'harvest')\nconn.register('csv://farm_data/farmers.csv', 'farmers')\nconn.register('csv://farm_data/tracts.csv', 'tracts')\nconn.register('csv://farm_data/fields.csv', 'fields')\n~~~\nYou can also do this programmatically:\n~~~python\nfiles = glob.glob('farm_data/*.csv')\n\nfor file in files:\n    fname = 'csv://' + file\n    tname = file.replace('farm_data/', '').replace('.csv', '')\n    conn.register(fname, tname)\n~~~\nThis method isn’t limited to `csv://`.  It works with `parquet://` and `csv.gz://` as well.\nGive it a try!\n=== “Postgres”\n~~~python\nconn = ibis.connect('postgres://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;')\n~~~\nOr, using a [.pgpass file](https://www.postgresql.org/docs/9.3/libpq-pgpass.html):\n~~~python\nconn = ibis.connect('postgres://&lt;username&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;')\n~~~\n\n\n\n\nOne of the trickier parts about working with data is that it doesn’t usually come organized in neat, predictable rows and columns. Instead data often consists of rows that could contain a single bit of data or arrays of it. When data is organized in layers, as with arrays, it can sometimes be difficult to work with. Ibis 3.1 introduces the unnest function as a way to flatten arrays of data.\nUnnest takes a column containing an array of values and separates the individual values into rows as shown:\nBefore Unnest:\n    | col    |\n    | ------ |\n    | [1, 2] |\nAfter Unnest:\n    | col |\n    | --- |\n    |  1  |\n    |  2  |\nHere is a self-contained example of creating a dataset with an array and then unnesting it:\n\n=== “DuckDB”\n~~~python\nimport ibis\nimport pandas as pd\n\n# Parquet save path\nfname = 'array_data.parquet'\n\n# Mock Data\ndata = [\n    ['array_id', 'array_value']\n    ,[1, [1, 3, 4]]\n    ,[2, [2, 4, 5]]\n    ,[3, [6, 8]]\n    ,[4, [1, 6]]\n]\n\n# Save as parquet\npd.DataFrame(data[1:], columns=data[0]).to_parquet(fname)\n\n# Connect to the file using a DuckDB backend\nconn = ibis.connect(f\"duckdb://{fname}\")\n\n# Create a table expression for your loaded data\narray_data = conn.table(\"array_data\")\n\n# Optionally execute the array data to preview\narray_data.execute()\n\n# select the unnested values with their corresponding IDs\narray_data.select(['array_id', array_data['array_value'].unnest()]).execute()\n~~~\n=== “Postgres”\n~~~python\nimport ibis\nimport pandas as pd\n\n# Postgres connection string for user 'ibistutorials' with a valid .pgpass file in ~/\n# See https://www.postgresql.org/docs/9.3/libpq-pgpass.html for details on ~/.pgpass\ncstring = 'postgres://ibistutorials@localhost:5432/pg-ibis'\n\n# Mock Data\ndata = [\n    ['array_id', 'array_value']\n    ,[1, [1, 3, 4]]\n    ,[2, [2, 4, 5]]\n    ,[3, [6, 8]]\n    ,[4, [1, 6]]\n]\n\n# Create a dataframe for easy loading\ndf = pd.DataFrame(data[1:], columns=data[0])\n\n# Postgres backend connection\nconn = ibis.connect(cstring)\n\n# SQLAlchemy Types\n# Integer type\nint_type = ibis.backends.postgres.sa.types.INT()\n# Array type function\narr_f = ibis.backends.postgres.sa.types.ARRAY\n\n# Load data to table using pd.DataFrame.to_sql\ndf.to_sql(\n    name='array_data'\n    ,con=conn.con.connect()\n    ,if_exists='replace'\n    ,index=False\n    ,dtype={\n        'array_id': int_type\n        ,'array_value': arr_f(int_type)\n    }\n)\n\n# Array Data Table Expression\narray_data = conn.table(\"array_data\")\n\n# Optionally execute to preview entire table\n# array_data.execute()\n\n# Unnest\narray_data.select(['array_id', array_data['array_value'].unnest()]).execute()\n~~~\n\n\n\n\nThere is now a shorthand for lambda functions using underscore (_). This is useful for chaining expressions to one another and helps reduce total line characters and appearances of lambdas.\nFor example, let’s use array_data from above. We will unnest array_value, find the weighted average, and then sum in one expression:\nfrom ibis import _\n\n(\n    array_data\n    .select([\n        'array_id'\n        # array_data returns a TableExpr, `_` here is shorthand\n        # for that returned expression\n        ,_['array_value'].unnest().name('arval')\n        # we can use it instead of saying `array_data`\n        ,(_['array_value'].length().cast('float')\n          / _['array_value'].length().sum().cast('float')).name('wgt')\n    ])\n    # Since the above `select` statement returns a TableExpr, we can use\n    # `_` to reference that one as well:\n    .mutate(wgt_prod=_.arval * _.wgt)\n    # And again:\n    .aggregate(vsum=_.wgt_prod.sum(), vcount=_.wgt_prod.count())\n    # And again:\n    .mutate(wgt_mean=_.vsum / _.vcount)\n).execute()\nNote that if you import _ directly from ibis (from ibis import _), the default _ object will lose its functionality, so be mindful if you have a habit of using it outside of Ibis.\n\n\n\nAlong with these changes, the operation matrix has had a few more holes filled. Contributors should note that backend test data is now loaded dynamically. Most users won’t be exposed to this update, but it should make contribution a bit more streamlined.\nTo see the full patch notes, go to the patch notes page\nAs always, Ibis is free and open source. Contributions are welcome and encouraged–drop into the discussions, raise an issue, or put in a pull request.\nDownload ibis 3.1 today!"
  },
  {
    "objectID": "blog/Ibis-version-3.1.0-release.html#introduction",
    "href": "blog/Ibis-version-3.1.0-release.html#introduction",
    "title": "Ibis v3.1.0",
    "section": "",
    "text": "Ibis 3.1 has officially been released as the latest version of the package. With this release comes new convenience features, increased backend operation coverage and a plethora of bug fixes. As usual, a full list of the changes can be found in the project release notes here Let’s talk about some of the new changes 3.1 brings for Ibis users."
  },
  {
    "objectID": "blog/Ibis-version-3.1.0-release.html#ibis.connect",
    "href": "blog/Ibis-version-3.1.0-release.html#ibis.connect",
    "title": "Ibis v3.1.0",
    "section": "",
    "text": "The first significant change to note is that, Ibis now provides a more convenient way to connect to a backend using the ibis.connect method. You can now use this function to connect to an appropriate backend using a connection string.\nHere are some examples:\n\n=== “DuckDB”\nInitialize a DuckDB instance using `'duckdb://:memory:'`\n~~~python\nconn = ibis.connect('duckdb://:memory:')\n~~~\nAnd begin registering your tables:\n~~~python\nconn.register('csv://farm_data/dates.csv', 'dates')\nconn.register('csv://farm_data/farmer_groups.csv', 'farmer_groups')\nconn.register('csv://farm_data/crops.csv', 'crops')\nconn.register('csv://farm_data/farms.csv', 'farms')\nconn.register('csv://farm_data/harvest.csv', 'harvest')\nconn.register('csv://farm_data/farmers.csv', 'farmers')\nconn.register('csv://farm_data/tracts.csv', 'tracts')\nconn.register('csv://farm_data/fields.csv', 'fields')\n~~~\nYou can also do this programmatically:\n~~~python\nfiles = glob.glob('farm_data/*.csv')\n\nfor file in files:\n    fname = 'csv://' + file\n    tname = file.replace('farm_data/', '').replace('.csv', '')\n    conn.register(fname, tname)\n~~~\nThis method isn’t limited to `csv://`.  It works with `parquet://` and `csv.gz://` as well.\nGive it a try!\n=== “Postgres”\n~~~python\nconn = ibis.connect('postgres://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;')\n~~~\nOr, using a [.pgpass file](https://www.postgresql.org/docs/9.3/libpq-pgpass.html):\n~~~python\nconn = ibis.connect('postgres://&lt;username&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;')\n~~~"
  },
  {
    "objectID": "blog/Ibis-version-3.1.0-release.html#unnest-support",
    "href": "blog/Ibis-version-3.1.0-release.html#unnest-support",
    "title": "Ibis v3.1.0",
    "section": "",
    "text": "One of the trickier parts about working with data is that it doesn’t usually come organized in neat, predictable rows and columns. Instead data often consists of rows that could contain a single bit of data or arrays of it. When data is organized in layers, as with arrays, it can sometimes be difficult to work with. Ibis 3.1 introduces the unnest function as a way to flatten arrays of data.\nUnnest takes a column containing an array of values and separates the individual values into rows as shown:\nBefore Unnest:\n    | col    |\n    | ------ |\n    | [1, 2] |\nAfter Unnest:\n    | col |\n    | --- |\n    |  1  |\n    |  2  |\nHere is a self-contained example of creating a dataset with an array and then unnesting it:\n\n=== “DuckDB”\n~~~python\nimport ibis\nimport pandas as pd\n\n# Parquet save path\nfname = 'array_data.parquet'\n\n# Mock Data\ndata = [\n    ['array_id', 'array_value']\n    ,[1, [1, 3, 4]]\n    ,[2, [2, 4, 5]]\n    ,[3, [6, 8]]\n    ,[4, [1, 6]]\n]\n\n# Save as parquet\npd.DataFrame(data[1:], columns=data[0]).to_parquet(fname)\n\n# Connect to the file using a DuckDB backend\nconn = ibis.connect(f\"duckdb://{fname}\")\n\n# Create a table expression for your loaded data\narray_data = conn.table(\"array_data\")\n\n# Optionally execute the array data to preview\narray_data.execute()\n\n# select the unnested values with their corresponding IDs\narray_data.select(['array_id', array_data['array_value'].unnest()]).execute()\n~~~\n=== “Postgres”\n~~~python\nimport ibis\nimport pandas as pd\n\n# Postgres connection string for user 'ibistutorials' with a valid .pgpass file in ~/\n# See https://www.postgresql.org/docs/9.3/libpq-pgpass.html for details on ~/.pgpass\ncstring = 'postgres://ibistutorials@localhost:5432/pg-ibis'\n\n# Mock Data\ndata = [\n    ['array_id', 'array_value']\n    ,[1, [1, 3, 4]]\n    ,[2, [2, 4, 5]]\n    ,[3, [6, 8]]\n    ,[4, [1, 6]]\n]\n\n# Create a dataframe for easy loading\ndf = pd.DataFrame(data[1:], columns=data[0])\n\n# Postgres backend connection\nconn = ibis.connect(cstring)\n\n# SQLAlchemy Types\n# Integer type\nint_type = ibis.backends.postgres.sa.types.INT()\n# Array type function\narr_f = ibis.backends.postgres.sa.types.ARRAY\n\n# Load data to table using pd.DataFrame.to_sql\ndf.to_sql(\n    name='array_data'\n    ,con=conn.con.connect()\n    ,if_exists='replace'\n    ,index=False\n    ,dtype={\n        'array_id': int_type\n        ,'array_value': arr_f(int_type)\n    }\n)\n\n# Array Data Table Expression\narray_data = conn.table(\"array_data\")\n\n# Optionally execute to preview entire table\n# array_data.execute()\n\n# Unnest\narray_data.select(['array_id', array_data['array_value'].unnest()]).execute()\n~~~"
  },
  {
    "objectID": "blog/Ibis-version-3.1.0-release.html#api",
    "href": "blog/Ibis-version-3.1.0-release.html#api",
    "title": "Ibis v3.1.0",
    "section": "",
    "text": "There is now a shorthand for lambda functions using underscore (_). This is useful for chaining expressions to one another and helps reduce total line characters and appearances of lambdas.\nFor example, let’s use array_data from above. We will unnest array_value, find the weighted average, and then sum in one expression:\nfrom ibis import _\n\n(\n    array_data\n    .select([\n        'array_id'\n        # array_data returns a TableExpr, `_` here is shorthand\n        # for that returned expression\n        ,_['array_value'].unnest().name('arval')\n        # we can use it instead of saying `array_data`\n        ,(_['array_value'].length().cast('float')\n          / _['array_value'].length().sum().cast('float')).name('wgt')\n    ])\n    # Since the above `select` statement returns a TableExpr, we can use\n    # `_` to reference that one as well:\n    .mutate(wgt_prod=_.arval * _.wgt)\n    # And again:\n    .aggregate(vsum=_.wgt_prod.sum(), vcount=_.wgt_prod.count())\n    # And again:\n    .mutate(wgt_mean=_.vsum / _.vcount)\n).execute()\nNote that if you import _ directly from ibis (from ibis import _), the default _ object will lose its functionality, so be mindful if you have a habit of using it outside of Ibis."
  },
  {
    "objectID": "blog/Ibis-version-3.1.0-release.html#additional-changes",
    "href": "blog/Ibis-version-3.1.0-release.html#additional-changes",
    "title": "Ibis v3.1.0",
    "section": "",
    "text": "Along with these changes, the operation matrix has had a few more holes filled. Contributors should note that backend test data is now loaded dynamically. Most users won’t be exposed to this update, but it should make contribution a bit more streamlined.\nTo see the full patch notes, go to the patch notes page\nAs always, Ibis is free and open source. Contributions are welcome and encouraged–drop into the discussions, raise an issue, or put in a pull request.\nDownload ibis 3.1 today!"
  },
  {
    "objectID": "blog/selectors.html",
    "href": "blog/selectors.html",
    "title": "Maximizing Productivity with Selectors",
    "section": "",
    "text": "Before Ibis 5.0 it’s been challenging to concisely express whole-table operations with ibis. Happily this is no longer the case in ibis 5.0.\nLet’s jump right in!\nWe’ll look at selectors examples using the palmerpenguins data set with the DuckDB backend.\n\n\nIn [8]: from ibis.interactive import *\n\nIn [11]: t = ex.penguins.fetch()\n\nIn [12]: t\nOut[12]:\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\nLet’s say you want to compute the z-score of every numeric column and replace the existing data with that normalized value. Here’s how you’d do that with selectors:\nIn [13]: t.mutate(s.across(s.numeric(), (_ - _.mean()) / _.std()))\nOut[13]:\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year      ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ float64     │ string │ float64   │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────────┤\n│ Adelie  │ Torgersen │      -0.883205 │      0.784300 │         -1.416272 │   -0.563317 │ male   │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.809939 │      0.126003 │         -1.060696 │   -0.500969 │ female │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.663408 │      0.429833 │         -0.420660 │   -1.186793 │ female │ -1.257484 │\n│ Adelie  │ Torgersen │            nan │           nan │               nan │         nan │ NULL   │ -1.257484 │\n│ Adelie  │ Torgersen │      -1.322799 │      1.088129 │         -0.562890 │   -0.937403 │ female │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.846572 │      1.746426 │         -0.776236 │   -0.688012 │ male   │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.919837 │      0.328556 │         -1.416272 │   -0.719186 │ female │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.864888 │      1.240044 │         -0.420660 │    0.590115 │ male   │ -1.257484 │\n│ Adelie  │ Torgersen │      -1.799025 │      0.480471 │         -0.562890 │   -0.906229 │ NULL   │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.352029 │      1.543873 │         -0.776236 │    0.060160 │ NULL   │ -1.257484 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │         … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────────┘\n\n\n\nWhoops, looks like we included year in our normalization because it’s an int64 column (and therefore numeric) but normalizing the year doesn’t make sense.\nWe can exclude year from the normalization using another selector:\nIn [14]: t.mutate(s.across(s.numeric() & ~s.c(\"year\"), (_ - _.mean()) / _.std()))\nOut[14]:\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ float64     │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │      -0.883205 │      0.784300 │         -1.416272 │   -0.563317 │ male   │  2007 │\n│ Adelie  │ Torgersen │      -0.809939 │      0.126003 │         -1.060696 │   -0.500969 │ female │  2007 │\n│ Adelie  │ Torgersen │      -0.663408 │      0.429833 │         -0.420660 │   -1.186793 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │               nan │         nan │ NULL   │  2007 │\n│ Adelie  │ Torgersen │      -1.322799 │      1.088129 │         -0.562890 │   -0.937403 │ female │  2007 │\n│ Adelie  │ Torgersen │      -0.846572 │      1.746426 │         -0.776236 │   -0.688012 │ male   │  2007 │\n│ Adelie  │ Torgersen │      -0.919837 │      0.328556 │         -1.416272 │   -0.719186 │ female │  2007 │\n│ Adelie  │ Torgersen │      -0.864888 │      1.240044 │         -0.420660 │    0.590115 │ male   │  2007 │\n│ Adelie  │ Torgersen │      -1.799025 │      0.480471 │         -0.562890 │   -0.906229 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │      -0.352029 │      1.543873 │         -0.776236 │    0.060160 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\nc is short for “column” and the ~ means “negate”. Combining those we get “not the year column”!\nPretty neat right?\n\n\n\nThe power of this approach comes in when you want the grouped version. Perhaps we think some of these columns vary by species.\nWith selectors, all you need to do is slap a .group_by(\"species\") onto t:\nIn [18]: t.group_by(\"species\").mutate(\n    ...:     s.across(s.numeric() & ~s.c(\"year\"), (_ - _.mean()) / _.std())\n    ...: )\nOut[18]:\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string │ float64        │ float64       │ float64           │ float64     │ string │ int64 │\n├─────────┼────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Gentoo  │ Biscoe │      -0.455854 │     -1.816223 │         -0.954050 │   -1.142626 │ female │  2007 │\n│ Gentoo  │ Biscoe │      -0.975022 │     -0.287513 │         -0.491442 │   -0.448342 │ female │  2009 │\n│ Gentoo  │ Biscoe │       0.387793 │     -0.898997 │         -1.108253 │   -1.241809 │ female │  2007 │\n│ Gentoo  │ Biscoe │       0.809616 │      0.222056 │          0.125368 │    1.237778 │ male   │  2007 │\n│ Gentoo  │ Biscoe │       0.030865 │     -0.491341 │         -0.337240 │    0.642677 │ male   │  2007 │\n│ Gentoo  │ Biscoe │      -0.326062 │     -1.510481 │         -1.108253 │   -1.043442 │ female │  2007 │\n│ Gentoo  │ Biscoe │      -0.682990 │     -0.389427 │         -0.954050 │   -0.547525 │ female │  2007 │\n│ Gentoo  │ Biscoe │      -0.261167 │      0.323970 │          0.279571 │    0.245943 │ male   │  2007 │\n│ Gentoo  │ Biscoe │      -1.364397 │     -1.612395 │         -1.262455 │   -1.340993 │ female │  2007 │\n│ Gentoo  │ Biscoe │      -0.228719 │      0.425884 │         -0.337240 │    0.146759 │ male   │  2007 │\n│ …       │ …      │              … │             … │                 … │           … │ …      │     … │\n└─────────┴────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\nSince ibis translates this into a run-of-the-mill selection as if you had called select or mutate without selectors, nothing special is needed for a backend to work with these new constructs.\nLet’s look at some more examples.\n\n\n\nGrouped min/max normalization? Easy:\nIn [22]: t.group_by(\"species\").mutate(\n    ...:     s.across(s.numeric() & ~s.c(\"year\"), (_ - _.min()) / (_.max() - _.min()))\n    ...: )\nOut[22]:\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string │ float64        │ float64       │ float64           │ float64     │ string │ int64 │\n├─────────┼────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Gentoo  │ Biscoe │       0.278075 │      0.023810 │          0.285714 │    0.234043 │ female │  2007 │\n│ Gentoo  │ Biscoe │       0.192513 │      0.380952 │          0.392857 │    0.382979 │ female │  2009 │\n│ Gentoo  │ Biscoe │       0.417112 │      0.238095 │          0.250000 │    0.212766 │ female │  2007 │\n│ Gentoo  │ Biscoe │       0.486631 │      0.500000 │          0.535714 │    0.744681 │ male   │  2007 │\n│ Gentoo  │ Biscoe │       0.358289 │      0.333333 │          0.428571 │    0.617021 │ male   │  2007 │\n│ Gentoo  │ Biscoe │       0.299465 │      0.095238 │          0.250000 │    0.255319 │ female │  2007 │\n│ Gentoo  │ Biscoe │       0.240642 │      0.357143 │          0.285714 │    0.361702 │ female │  2007 │\n│ Gentoo  │ Biscoe │       0.310160 │      0.523810 │          0.571429 │    0.531915 │ male   │  2007 │\n│ Gentoo  │ Biscoe │       0.128342 │      0.071429 │          0.214286 │    0.191489 │ female │  2007 │\n│ Gentoo  │ Biscoe │       0.315508 │      0.547619 │          0.428571 │    0.510638 │ male   │  2007 │\n│ …       │ …      │              … │             … │                 … │           … │ …      │     … │\n└─────────┴────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nHow about casting every column whose name ends with any of the strings \"mm\" or \"g\" to a float32? No problem!\nIn [23]: t.mutate(s.across(s.endswith((\"mm\", \"g\")), _.cast(\"float32\")))\nOut[23]:\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float32        │ float32       │ float32           │ float32     │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │      39.099998 │     18.700001 │             181.0 │      3750.0 │ male   │  2007 │\n│ Adelie  │ Torgersen │      39.500000 │     17.400000 │             186.0 │      3800.0 │ female │  2007 │\n│ Adelie  │ Torgersen │      40.299999 │     18.000000 │             195.0 │      3250.0 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │               nan │         nan │ NULL   │  2007 │\n│ Adelie  │ Torgersen │      36.700001 │     19.299999 │             193.0 │      3450.0 │ female │  2007 │\n│ Adelie  │ Torgersen │      39.299999 │     20.600000 │             190.0 │      3650.0 │ male   │  2007 │\n│ Adelie  │ Torgersen │      38.900002 │     17.799999 │             181.0 │      3625.0 │ female │  2007 │\n│ Adelie  │ Torgersen │      39.200001 │     19.600000 │             195.0 │      4675.0 │ male   │  2007 │\n│ Adelie  │ Torgersen │      34.099998 │     18.100000 │             193.0 │      3475.0 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │      42.000000 │     20.200001 │             190.0 │      4250.0 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\nWe can make all string columns have the same case too!\nIn [35]: t.mutate(s.across(s.of_type(\"string\"), _.lower()))\nOut[35]:\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ adelie  │ torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ adelie  │ torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ adelie  │ torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ adelie  │ torgersen │            nan │           nan │              NULL │        NULL │ NULL   │  2007 │\n│ adelie  │ torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ adelie  │ torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ adelie  │ torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ adelie  │ torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ adelie  │ torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ adelie  │ torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nWhat if I want to compute multiple things? Heck yeah!\nIn [9]: t.group_by(\"sex\").mutate(\n   ...:     s.across(\n   ...:         s.numeric() & ~s.c(\"year\"),\n   ...:         dict(centered=_ - _.mean(), zscore=(_ - _.mean()) / _.std()),\n   ...:     )\n   ...: ).select(\"sex\", s.endswith((\"_centered\", \"_zscore\")))\nOut[9]:\n┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ sex    ┃ bill_length_mm_centered ┃ bill_depth_mm_centered ┃ flipper_length_mm_centered ┃ body_mass_g_centered ┃ bill_length_mm_zscore ┃ bill_depth_mm_zscore ┃ flipper_length_mm_zscore ┃ … ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string │ float64                 │ float64                │ float64                    │ float64              │ float64               │ float64              │ float64                  │ … │\n├────────┼─────────────────────────┼────────────────────────┼────────────────────────────┼──────────────────────┼───────────────────────┼──────────────────────┼──────────────────────────┼───┤\n│ male   │                0.445238 │              -2.091071 │                  10.494048 │           504.315476 │              0.082960 │            -1.122210 │                 0.721346 │ … │\n│ male   │                2.245238 │              -2.791071 │                   4.494048 │           954.315476 │              0.418349 │            -1.497878 │                 0.308914 │ … │\n│ male   │               -6.254762 │               0.208929 │                 -18.505952 │           -95.684524 │             -1.165434 │             0.112125 │                -1.272072 │ … │\n│ male   │               -5.054762 │               1.008929 │                   3.494048 │          -245.684524 │             -0.941841 │             0.541459 │                 0.240176 │ … │\n│ male   │              -11.254762 │               3.208929 │                  -6.505952 │          -145.684524 │             -2.097071 │             1.722128 │                -0.447210 │ … │\n│ male   │               -3.354762 │               2.808929 │                  -7.505952 │           -45.684524 │             -0.625084 │             1.507461 │                -0.515948 │ … │\n│ male   │                0.145238 │               3.608929 │                 -10.505952 │          -345.684524 │              0.027062 │             1.936795 │                -0.722164 │ … │\n│ male   │               -8.154762 │               0.808929 │                 -24.505952 │          -945.684524 │             -1.519456 │             0.434126 │                -1.684504 │ … │\n│ male   │               -7.654762 │               0.208929 │                 -19.505952 │          -595.684524 │             -1.426292 │             0.112125 │                -1.340811 │ … │\n│ male   │               -7.054762 │              -0.691071 │                 -24.505952 │          -745.684524 │             -1.314496 │            -0.370876 │                -1.684504 │ … │\n│ …      │                       … │                      … │                          … │                    … │                     … │                    … │                        … │ … │\n└────────┴─────────────────────────┴────────────────────────┴────────────────────────────┴──────────────────────┴───────────────────────┴──────────────────────┴──────────────────────────┴───┘\nDon’t like the naming convention?\nPass a function to make your own name!\nIn [12]: t.select(s.startswith(\"bill\")).mutate(\n    ...:     s.across(\n    ...:         s.all(),\n    ...:         dict(x=_ - _.mean(), y=_.max()),\n    ...:         names=lambda col, fn: f\"{col}_{fn}_improved\",\n    ...:     )\n    ...: )\nOut[12]:\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ bill_length_mm ┃ bill_depth_mm ┃ bill_length_mm_x_improved ┃ bill_depth_mm_x_improved ┃ bill_length_mm_y_improved ┃ bill_depth_mm_y_improved ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ float64        │ float64       │ float64                   │ float64                  │ float64                   │ float64                  │\n├────────────────┼───────────────┼───────────────────────────┼──────────────────────────┼───────────────────────────┼──────────────────────────┤\n│           39.1 │          18.7 │                  -4.82193 │                  1.54883 │                      59.6 │                     21.5 │\n│           39.5 │          17.4 │                  -4.42193 │                  0.24883 │                      59.6 │                     21.5 │\n│           40.3 │          18.0 │                  -3.62193 │                  0.84883 │                      59.6 │                     21.5 │\n│            nan │           nan │                       nan │                      nan │                      59.6 │                     21.5 │\n│           36.7 │          19.3 │                  -7.22193 │                  2.14883 │                      59.6 │                     21.5 │\n│           39.3 │          20.6 │                  -4.62193 │                  3.44883 │                      59.6 │                     21.5 │\n│           38.9 │          17.8 │                  -5.02193 │                  0.64883 │                      59.6 │                     21.5 │\n│           39.2 │          19.6 │                  -4.72193 │                  2.44883 │                      59.6 │                     21.5 │\n│           34.1 │          18.1 │                  -9.82193 │                  0.94883 │                      59.6 │                     21.5 │\n│           42.0 │          20.2 │                  -1.92193 │                  3.04883 │                      59.6 │                     21.5 │\n│              … │             … │                         … │                        … │                         … │                        … │\n└────────────────┴───────────────┴───────────────────────────┴──────────────────────────┴───────────────────────────┴──────────────────────────┘\nDon’t like lambda functions? We support a format string too!\nIn [5]: t.select(s.startswith(\"bill\")).mutate(\n   ...:     s.across(\n   ...:         s.all(),\n   ...:         func=dict(x=_ - _.mean(), y=_.max()),\n   ...:         names=\"{col}_{fn}_improved\",\n   ...:     )\n   ...: ).head(2)\nOut[5]:\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ bill_length_mm ┃ bill_depth_mm ┃ bill_length_mm_x_improved ┃ bill_depth_mm_x_improved ┃ bill_length_mm_y_improved ┃ bill_depth_mm_y_improved ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ float64        │ float64       │ float64                   │ float64                  │ float64                   │ float64                  │\n├────────────────┼───────────────┼───────────────────────────┼──────────────────────────┼───────────────────────────┼──────────────────────────┤\n│           39.1 │          18.7 │                  -4.82193 │                  1.54883 │                      59.6 │                     21.5 │\n│           39.5 │          17.4 │                  -4.42193 │                  0.24883 │                      59.6 │                     21.5 │\n└────────────────┴───────────────┴───────────────────────────┴──────────────────────────┴───────────────────────────┴──────────────────────────┘\n\n\n\nWe’ve seen lots of mutate use, but selectors also work with .agg:\nIn [31]: t.group_by(\"year\").agg(s.across(s.numeric() & ~s.c(\"year\"), _.mean())).order_by(\"year\")\nOut[31]:\n┏━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ year  ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃\n┡━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n│ int64 │ float64        │ float64       │ float64           │ float64     │\n├───────┼────────────────┼───────────────┼───────────────────┼─────────────┤\n│  2007 │      43.740367 │     17.427523 │        196.880734 │ 4124.541284 │\n│  2008 │      43.541228 │     16.914035 │        202.798246 │ 4266.666667 │\n│  2009 │      44.452941 │     17.125210 │        202.806723 │ 4210.294118 │\n└───────┴────────────────┴───────────────┴───────────────────┴─────────────┘\nNaturally, selectors work in grouping keys too, for even more convenience:\nIn [12]: t.group_by(~s.numeric() | s.c(\"year\")).mutate(\n    ...:     s.across(s.numeric() & ~s.c(\"year\"), dict(centered=_ - _.mean(), std=_.std()))\n    ...: ).select(\"species\", s.endswith((\"_centered\", \"_std\")))\nOut[12]:\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ species ┃ bill_length_mm_centered ┃ bill_depth_mm_centered ┃ flipper_length_mm_centered ┃ body_mass_g_centered ┃ bill_length_mm_std ┃ bill_depth_mm_std ┃ flipper_length_mm_std ┃ body_mass_g_std ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string  │ float64                 │ float64                │ float64                    │ float64              │ float64            │ float64           │ float64               │ float64         │\n├─────────┼─────────────────────────┼────────────────────────┼────────────────────────────┼──────────────────────┼────────────────────┼───────────────────┼───────────────────────┼─────────────────┤\n│ Adelie  │                1.187500 │               1.412500 │                       -1.0 │          -550.000000 │           2.087676 │          0.756755 │              7.764388 │      311.677489 │\n│ Adelie  │               -3.812500 │               0.612500 │                       -5.0 │          -300.000000 │           2.087676 │          0.756755 │              7.764388 │      311.677489 │\n│ Adelie  │               -1.812500 │               0.312500 │                       -6.0 │          -150.000000 │           2.087676 │          0.756755 │              7.764388 │      311.677489 │\n│ Adelie  │                0.987500 │              -0.787500 │                       10.0 │           200.000000 │           2.087676 │          0.756755 │              7.764388 │      311.677489 │\n│ Adelie  │               -0.512500 │              -0.787500 │                       -9.0 │           350.000000 │           2.087676 │          0.756755 │              7.764388 │      311.677489 │\n│ Adelie  │                0.687500 │               0.012500 │                       13.0 │           200.000000 │           2.087676 │          0.756755 │              7.764388 │      311.677489 │\n│ Adelie  │                0.187500 │              -0.387500 │                        1.0 │           250.000000 │           2.087676 │          0.756755 │              7.764388 │      311.677489 │\n│ Adelie  │                3.087500 │              -0.387500 │                       -3.0 │             0.000000 │           2.087676 │          0.756755 │              7.764388 │      311.677489 │\n│ Adelie  │                1.644444 │              -1.144444 │                       -7.0 │           -19.444444 │           2.119028 │          0.860394 │              5.408327 │      170.375403 │\n│ Adelie  │                1.644444 │              -0.044444 │                        3.0 │            30.555556 │           2.119028 │          0.860394 │              5.408327 │      170.375403 │\n│ …       │                       … │                      … │                          … │                    … │                  … │                 … │                     … │               … │\n└─────────┴─────────────────────────┴────────────────────────┴────────────────────────────┴──────────────────────┴────────────────────┴───────────────────┴───────────────────────┴─────────────────┘\n\n\n\nYou can also express complex filters more concisely.\nLet’s say we only want to keep rows where all the bill size z-score related columns’ absolute values are greater than 2.\n\nIn [78]: t.drop(\"year\").group_by(\"species\").mutate(\n    ...:     s.across(s.numeric(), dict(zscore=(_ - _.mean()) / _.std()))\n    ...: ).filter(s.if_all(s.startswith(\"bill\") & s.endswith(\"_zscore\"), _.abs() &gt; 2))\nOut[78]:\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ bill_length_mm_zscore ┃ bill_depth_mm_zscore ┃ flipper_length_mm_zscore ┃ body_mass_g_zscore ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ float64               │ float64              │ float64                  │ float64            │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────────────────────┼──────────────────────┼──────────────────────────┼────────────────────┤\n│ Gentoo  │ Biscoe    │           59.6 │          17.0 │               230 │        6050 │ male   │              3.924621 │             2.056508 │                 1.975799 │           1.932062 │\n│ Gentoo  │ Biscoe    │           55.9 │          17.0 │               228 │        5600 │ male   │              2.724046 │             2.056508 │                 1.667394 │           1.039411 │\n│ Adelie  │ Torgersen │           46.0 │          21.5 │               194 │        4200 │ male   │              2.706539 │             2.592071 │                 0.618760 │           1.088911 │\n│ Adelie  │ Dream     │           32.1 │          15.5 │               188 │        3050 │ female │             -2.512345 │            -2.339505 │                -0.298747 │          -1.418906 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────────────────────┴──────────────────────┴──────────────────────────┴────────────────────┘```\n\n\n\n\nThe SQL for that last expression is pretty gnarly:\nIn [79]: ibis.show_sql(\n    ...:     t.drop(\"year\")\n    ...:     .group_by(\"species\")\n    ...:     .mutate(s.across(s.numeric(), dict(zscore=(_ - _.mean()) / _.std())))\n    ...:     .filter(s.if_all(s.startswith(\"bill\") & s.endswith(\"_zscore\"), _.abs() &gt; 2))\n    ...: )\nWITH t0 AS (\n  SELECT\n    t2.species AS species,\n    t2.island AS island,\n    t2.bill_length_mm AS bill_length_mm,\n    t2.bill_depth_mm AS bill_depth_mm,\n    t2.flipper_length_mm AS flipper_length_mm,\n    t2.body_mass_g AS body_mass_g,\n    t2.sex AS sex\n  FROM ibis_read_csv_3 AS t2\n), t1 AS (\n  SELECT\n    t0.species AS species,\n    t0.island AS island,\n    t0.bill_length_mm AS bill_length_mm,\n    t0.bill_depth_mm AS bill_depth_mm,\n    t0.flipper_length_mm AS flipper_length_mm,\n    t0.body_mass_g AS body_mass_g,\n    t0.sex AS sex,\n    (\n      t0.bill_length_mm - AVG(t0.bill_length_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)\n    ) / STDDEV_SAMP(t0.bill_length_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS bill_length_mm_zscore,\n    (\n      t0.bill_depth_mm - AVG(t0.bill_depth_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)\n    ) / STDDEV_SAMP(t0.bill_depth_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS bill_depth_mm_zscore,\n    (\n      t0.flipper_length_mm - AVG(t0.flipper_length_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)\n    ) / STDDEV_SAMP(t0.flipper_length_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS flipper_length_mm_zscore,\n    (\n      t0.body_mass_g - AVG(t0.body_mass_g) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)\n    ) / STDDEV_SAMP(t0.body_mass_g) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS body_mass_g_zscore\n  FROM t0\n)\nSELECT\n  t1.species,\n  t1.island,\n  t1.bill_length_mm,\n  t1.bill_depth_mm,\n  t1.flipper_length_mm,\n  t1.body_mass_g,\n  t1.sex,\n  t1.bill_length_mm_zscore,\n  t1.bill_depth_mm_zscore,\n  t1.flipper_length_mm_zscore,\n  t1.body_mass_g_zscore\nFROM t1\nWHERE\n  ABS(t1.bill_length_mm_zscore) &gt; CAST(2 AS SMALLINT)\n  AND ABS(t1.bill_depth_mm_zscore) &gt; CAST(2 AS SMALLINT)\nGood thing you didn’t have to write that by hand!\n\n\n\n\nThis blog post illustrates the ability to apply computations to many columns at once and the power of ibis as a composable, expressive library for analytics.\n\nGet involved!\nReport issues!"
  },
  {
    "objectID": "blog/selectors.html#setup",
    "href": "blog/selectors.html#setup",
    "title": "Maximizing Productivity with Selectors",
    "section": "",
    "text": "In [8]: from ibis.interactive import *\n\nIn [11]: t = ex.penguins.fetch()\n\nIn [12]: t\nOut[12]:\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "blog/selectors.html#examples",
    "href": "blog/selectors.html#examples",
    "title": "Maximizing Productivity with Selectors",
    "section": "",
    "text": "Let’s say you want to compute the z-score of every numeric column and replace the existing data with that normalized value. Here’s how you’d do that with selectors:\nIn [13]: t.mutate(s.across(s.numeric(), (_ - _.mean()) / _.std()))\nOut[13]:\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year      ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ float64     │ string │ float64   │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────────┤\n│ Adelie  │ Torgersen │      -0.883205 │      0.784300 │         -1.416272 │   -0.563317 │ male   │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.809939 │      0.126003 │         -1.060696 │   -0.500969 │ female │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.663408 │      0.429833 │         -0.420660 │   -1.186793 │ female │ -1.257484 │\n│ Adelie  │ Torgersen │            nan │           nan │               nan │         nan │ NULL   │ -1.257484 │\n│ Adelie  │ Torgersen │      -1.322799 │      1.088129 │         -0.562890 │   -0.937403 │ female │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.846572 │      1.746426 │         -0.776236 │   -0.688012 │ male   │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.919837 │      0.328556 │         -1.416272 │   -0.719186 │ female │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.864888 │      1.240044 │         -0.420660 │    0.590115 │ male   │ -1.257484 │\n│ Adelie  │ Torgersen │      -1.799025 │      0.480471 │         -0.562890 │   -0.906229 │ NULL   │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.352029 │      1.543873 │         -0.776236 │    0.060160 │ NULL   │ -1.257484 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │         … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────────┘\n\n\n\nWhoops, looks like we included year in our normalization because it’s an int64 column (and therefore numeric) but normalizing the year doesn’t make sense.\nWe can exclude year from the normalization using another selector:\nIn [14]: t.mutate(s.across(s.numeric() & ~s.c(\"year\"), (_ - _.mean()) / _.std()))\nOut[14]:\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ float64     │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │      -0.883205 │      0.784300 │         -1.416272 │   -0.563317 │ male   │  2007 │\n│ Adelie  │ Torgersen │      -0.809939 │      0.126003 │         -1.060696 │   -0.500969 │ female │  2007 │\n│ Adelie  │ Torgersen │      -0.663408 │      0.429833 │         -0.420660 │   -1.186793 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │               nan │         nan │ NULL   │  2007 │\n│ Adelie  │ Torgersen │      -1.322799 │      1.088129 │         -0.562890 │   -0.937403 │ female │  2007 │\n│ Adelie  │ Torgersen │      -0.846572 │      1.746426 │         -0.776236 │   -0.688012 │ male   │  2007 │\n│ Adelie  │ Torgersen │      -0.919837 │      0.328556 │         -1.416272 │   -0.719186 │ female │  2007 │\n│ Adelie  │ Torgersen │      -0.864888 │      1.240044 │         -0.420660 │    0.590115 │ male   │  2007 │\n│ Adelie  │ Torgersen │      -1.799025 │      0.480471 │         -0.562890 │   -0.906229 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │      -0.352029 │      1.543873 │         -0.776236 │    0.060160 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\nc is short for “column” and the ~ means “negate”. Combining those we get “not the year column”!\nPretty neat right?\n\n\n\nThe power of this approach comes in when you want the grouped version. Perhaps we think some of these columns vary by species.\nWith selectors, all you need to do is slap a .group_by(\"species\") onto t:\nIn [18]: t.group_by(\"species\").mutate(\n    ...:     s.across(s.numeric() & ~s.c(\"year\"), (_ - _.mean()) / _.std())\n    ...: )\nOut[18]:\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string │ float64        │ float64       │ float64           │ float64     │ string │ int64 │\n├─────────┼────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Gentoo  │ Biscoe │      -0.455854 │     -1.816223 │         -0.954050 │   -1.142626 │ female │  2007 │\n│ Gentoo  │ Biscoe │      -0.975022 │     -0.287513 │         -0.491442 │   -0.448342 │ female │  2009 │\n│ Gentoo  │ Biscoe │       0.387793 │     -0.898997 │         -1.108253 │   -1.241809 │ female │  2007 │\n│ Gentoo  │ Biscoe │       0.809616 │      0.222056 │          0.125368 │    1.237778 │ male   │  2007 │\n│ Gentoo  │ Biscoe │       0.030865 │     -0.491341 │         -0.337240 │    0.642677 │ male   │  2007 │\n│ Gentoo  │ Biscoe │      -0.326062 │     -1.510481 │         -1.108253 │   -1.043442 │ female │  2007 │\n│ Gentoo  │ Biscoe │      -0.682990 │     -0.389427 │         -0.954050 │   -0.547525 │ female │  2007 │\n│ Gentoo  │ Biscoe │      -0.261167 │      0.323970 │          0.279571 │    0.245943 │ male   │  2007 │\n│ Gentoo  │ Biscoe │      -1.364397 │     -1.612395 │         -1.262455 │   -1.340993 │ female │  2007 │\n│ Gentoo  │ Biscoe │      -0.228719 │      0.425884 │         -0.337240 │    0.146759 │ male   │  2007 │\n│ …       │ …      │              … │             … │                 … │           … │ …      │     … │\n└─────────┴────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\nSince ibis translates this into a run-of-the-mill selection as if you had called select or mutate without selectors, nothing special is needed for a backend to work with these new constructs.\nLet’s look at some more examples.\n\n\n\nGrouped min/max normalization? Easy:\nIn [22]: t.group_by(\"species\").mutate(\n    ...:     s.across(s.numeric() & ~s.c(\"year\"), (_ - _.min()) / (_.max() - _.min()))\n    ...: )\nOut[22]:\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string │ float64        │ float64       │ float64           │ float64     │ string │ int64 │\n├─────────┼────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Gentoo  │ Biscoe │       0.278075 │      0.023810 │          0.285714 │    0.234043 │ female │  2007 │\n│ Gentoo  │ Biscoe │       0.192513 │      0.380952 │          0.392857 │    0.382979 │ female │  2009 │\n│ Gentoo  │ Biscoe │       0.417112 │      0.238095 │          0.250000 │    0.212766 │ female │  2007 │\n│ Gentoo  │ Biscoe │       0.486631 │      0.500000 │          0.535714 │    0.744681 │ male   │  2007 │\n│ Gentoo  │ Biscoe │       0.358289 │      0.333333 │          0.428571 │    0.617021 │ male   │  2007 │\n│ Gentoo  │ Biscoe │       0.299465 │      0.095238 │          0.250000 │    0.255319 │ female │  2007 │\n│ Gentoo  │ Biscoe │       0.240642 │      0.357143 │          0.285714 │    0.361702 │ female │  2007 │\n│ Gentoo  │ Biscoe │       0.310160 │      0.523810 │          0.571429 │    0.531915 │ male   │  2007 │\n│ Gentoo  │ Biscoe │       0.128342 │      0.071429 │          0.214286 │    0.191489 │ female │  2007 │\n│ Gentoo  │ Biscoe │       0.315508 │      0.547619 │          0.428571 │    0.510638 │ male   │  2007 │\n│ …       │ …      │              … │             … │                 … │           … │ …      │     … │\n└─────────┴────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nHow about casting every column whose name ends with any of the strings \"mm\" or \"g\" to a float32? No problem!\nIn [23]: t.mutate(s.across(s.endswith((\"mm\", \"g\")), _.cast(\"float32\")))\nOut[23]:\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float32        │ float32       │ float32           │ float32     │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │      39.099998 │     18.700001 │             181.0 │      3750.0 │ male   │  2007 │\n│ Adelie  │ Torgersen │      39.500000 │     17.400000 │             186.0 │      3800.0 │ female │  2007 │\n│ Adelie  │ Torgersen │      40.299999 │     18.000000 │             195.0 │      3250.0 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │               nan │         nan │ NULL   │  2007 │\n│ Adelie  │ Torgersen │      36.700001 │     19.299999 │             193.0 │      3450.0 │ female │  2007 │\n│ Adelie  │ Torgersen │      39.299999 │     20.600000 │             190.0 │      3650.0 │ male   │  2007 │\n│ Adelie  │ Torgersen │      38.900002 │     17.799999 │             181.0 │      3625.0 │ female │  2007 │\n│ Adelie  │ Torgersen │      39.200001 │     19.600000 │             195.0 │      4675.0 │ male   │  2007 │\n│ Adelie  │ Torgersen │      34.099998 │     18.100000 │             193.0 │      3475.0 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │      42.000000 │     20.200001 │             190.0 │      4250.0 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\nWe can make all string columns have the same case too!\nIn [35]: t.mutate(s.across(s.of_type(\"string\"), _.lower()))\nOut[35]:\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ adelie  │ torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ adelie  │ torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ adelie  │ torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ adelie  │ torgersen │            nan │           nan │              NULL │        NULL │ NULL   │  2007 │\n│ adelie  │ torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ adelie  │ torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ adelie  │ torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ adelie  │ torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ adelie  │ torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ adelie  │ torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nWhat if I want to compute multiple things? Heck yeah!\nIn [9]: t.group_by(\"sex\").mutate(\n   ...:     s.across(\n   ...:         s.numeric() & ~s.c(\"year\"),\n   ...:         dict(centered=_ - _.mean(), zscore=(_ - _.mean()) / _.std()),\n   ...:     )\n   ...: ).select(\"sex\", s.endswith((\"_centered\", \"_zscore\")))\nOut[9]:\n┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━┓\n┃ sex    ┃ bill_length_mm_centered ┃ bill_depth_mm_centered ┃ flipper_length_mm_centered ┃ body_mass_g_centered ┃ bill_length_mm_zscore ┃ bill_depth_mm_zscore ┃ flipper_length_mm_zscore ┃ … ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━┩\n│ string │ float64                 │ float64                │ float64                    │ float64              │ float64               │ float64              │ float64                  │ … │\n├────────┼─────────────────────────┼────────────────────────┼────────────────────────────┼──────────────────────┼───────────────────────┼──────────────────────┼──────────────────────────┼───┤\n│ male   │                0.445238 │              -2.091071 │                  10.494048 │           504.315476 │              0.082960 │            -1.122210 │                 0.721346 │ … │\n│ male   │                2.245238 │              -2.791071 │                   4.494048 │           954.315476 │              0.418349 │            -1.497878 │                 0.308914 │ … │\n│ male   │               -6.254762 │               0.208929 │                 -18.505952 │           -95.684524 │             -1.165434 │             0.112125 │                -1.272072 │ … │\n│ male   │               -5.054762 │               1.008929 │                   3.494048 │          -245.684524 │             -0.941841 │             0.541459 │                 0.240176 │ … │\n│ male   │              -11.254762 │               3.208929 │                  -6.505952 │          -145.684524 │             -2.097071 │             1.722128 │                -0.447210 │ … │\n│ male   │               -3.354762 │               2.808929 │                  -7.505952 │           -45.684524 │             -0.625084 │             1.507461 │                -0.515948 │ … │\n│ male   │                0.145238 │               3.608929 │                 -10.505952 │          -345.684524 │              0.027062 │             1.936795 │                -0.722164 │ … │\n│ male   │               -8.154762 │               0.808929 │                 -24.505952 │          -945.684524 │             -1.519456 │             0.434126 │                -1.684504 │ … │\n│ male   │               -7.654762 │               0.208929 │                 -19.505952 │          -595.684524 │             -1.426292 │             0.112125 │                -1.340811 │ … │\n│ male   │               -7.054762 │              -0.691071 │                 -24.505952 │          -745.684524 │             -1.314496 │            -0.370876 │                -1.684504 │ … │\n│ …      │                       … │                      … │                          … │                    … │                     … │                    … │                        … │ … │\n└────────┴─────────────────────────┴────────────────────────┴────────────────────────────┴──────────────────────┴───────────────────────┴──────────────────────┴──────────────────────────┴───┘\nDon’t like the naming convention?\nPass a function to make your own name!\nIn [12]: t.select(s.startswith(\"bill\")).mutate(\n    ...:     s.across(\n    ...:         s.all(),\n    ...:         dict(x=_ - _.mean(), y=_.max()),\n    ...:         names=lambda col, fn: f\"{col}_{fn}_improved\",\n    ...:     )\n    ...: )\nOut[12]:\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ bill_length_mm ┃ bill_depth_mm ┃ bill_length_mm_x_improved ┃ bill_depth_mm_x_improved ┃ bill_length_mm_y_improved ┃ bill_depth_mm_y_improved ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ float64        │ float64       │ float64                   │ float64                  │ float64                   │ float64                  │\n├────────────────┼───────────────┼───────────────────────────┼──────────────────────────┼───────────────────────────┼──────────────────────────┤\n│           39.1 │          18.7 │                  -4.82193 │                  1.54883 │                      59.6 │                     21.5 │\n│           39.5 │          17.4 │                  -4.42193 │                  0.24883 │                      59.6 │                     21.5 │\n│           40.3 │          18.0 │                  -3.62193 │                  0.84883 │                      59.6 │                     21.5 │\n│            nan │           nan │                       nan │                      nan │                      59.6 │                     21.5 │\n│           36.7 │          19.3 │                  -7.22193 │                  2.14883 │                      59.6 │                     21.5 │\n│           39.3 │          20.6 │                  -4.62193 │                  3.44883 │                      59.6 │                     21.5 │\n│           38.9 │          17.8 │                  -5.02193 │                  0.64883 │                      59.6 │                     21.5 │\n│           39.2 │          19.6 │                  -4.72193 │                  2.44883 │                      59.6 │                     21.5 │\n│           34.1 │          18.1 │                  -9.82193 │                  0.94883 │                      59.6 │                     21.5 │\n│           42.0 │          20.2 │                  -1.92193 │                  3.04883 │                      59.6 │                     21.5 │\n│              … │             … │                         … │                        … │                         … │                        … │\n└────────────────┴───────────────┴───────────────────────────┴──────────────────────────┴───────────────────────────┴──────────────────────────┘\nDon’t like lambda functions? We support a format string too!\nIn [5]: t.select(s.startswith(\"bill\")).mutate(\n   ...:     s.across(\n   ...:         s.all(),\n   ...:         func=dict(x=_ - _.mean(), y=_.max()),\n   ...:         names=\"{col}_{fn}_improved\",\n   ...:     )\n   ...: ).head(2)\nOut[5]:\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ bill_length_mm ┃ bill_depth_mm ┃ bill_length_mm_x_improved ┃ bill_depth_mm_x_improved ┃ bill_length_mm_y_improved ┃ bill_depth_mm_y_improved ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ float64        │ float64       │ float64                   │ float64                  │ float64                   │ float64                  │\n├────────────────┼───────────────┼───────────────────────────┼──────────────────────────┼───────────────────────────┼──────────────────────────┤\n│           39.1 │          18.7 │                  -4.82193 │                  1.54883 │                      59.6 │                     21.5 │\n│           39.5 │          17.4 │                  -4.42193 │                  0.24883 │                      59.6 │                     21.5 │\n└────────────────┴───────────────┴───────────────────────────┴──────────────────────────┴───────────────────────────┴──────────────────────────┘\n\n\n\nWe’ve seen lots of mutate use, but selectors also work with .agg:\nIn [31]: t.group_by(\"year\").agg(s.across(s.numeric() & ~s.c(\"year\"), _.mean())).order_by(\"year\")\nOut[31]:\n┏━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ year  ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃\n┡━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n│ int64 │ float64        │ float64       │ float64           │ float64     │\n├───────┼────────────────┼───────────────┼───────────────────┼─────────────┤\n│  2007 │      43.740367 │     17.427523 │        196.880734 │ 4124.541284 │\n│  2008 │      43.541228 │     16.914035 │        202.798246 │ 4266.666667 │\n│  2009 │      44.452941 │     17.125210 │        202.806723 │ 4210.294118 │\n└───────┴────────────────┴───────────────┴───────────────────┴─────────────┘\nNaturally, selectors work in grouping keys too, for even more convenience:\nIn [12]: t.group_by(~s.numeric() | s.c(\"year\")).mutate(\n    ...:     s.across(s.numeric() & ~s.c(\"year\"), dict(centered=_ - _.mean(), std=_.std()))\n    ...: ).select(\"species\", s.endswith((\"_centered\", \"_std\")))\nOut[12]:\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ species ┃ bill_length_mm_centered ┃ bill_depth_mm_centered ┃ flipper_length_mm_centered ┃ body_mass_g_centered ┃ bill_length_mm_std ┃ bill_depth_mm_std ┃ flipper_length_mm_std ┃ body_mass_g_std ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string  │ float64                 │ float64                │ float64                    │ float64              │ float64            │ float64           │ float64               │ float64         │\n├─────────┼─────────────────────────┼────────────────────────┼────────────────────────────┼──────────────────────┼────────────────────┼───────────────────┼───────────────────────┼─────────────────┤\n│ Adelie  │                1.187500 │               1.412500 │                       -1.0 │          -550.000000 │           2.087676 │          0.756755 │              7.764388 │      311.677489 │\n│ Adelie  │               -3.812500 │               0.612500 │                       -5.0 │          -300.000000 │           2.087676 │          0.756755 │              7.764388 │      311.677489 │\n│ Adelie  │               -1.812500 │               0.312500 │                       -6.0 │          -150.000000 │           2.087676 │          0.756755 │              7.764388 │      311.677489 │\n│ Adelie  │                0.987500 │              -0.787500 │                       10.0 │           200.000000 │           2.087676 │          0.756755 │              7.764388 │      311.677489 │\n│ Adelie  │               -0.512500 │              -0.787500 │                       -9.0 │           350.000000 │           2.087676 │          0.756755 │              7.764388 │      311.677489 │\n│ Adelie  │                0.687500 │               0.012500 │                       13.0 │           200.000000 │           2.087676 │          0.756755 │              7.764388 │      311.677489 │\n│ Adelie  │                0.187500 │              -0.387500 │                        1.0 │           250.000000 │           2.087676 │          0.756755 │              7.764388 │      311.677489 │\n│ Adelie  │                3.087500 │              -0.387500 │                       -3.0 │             0.000000 │           2.087676 │          0.756755 │              7.764388 │      311.677489 │\n│ Adelie  │                1.644444 │              -1.144444 │                       -7.0 │           -19.444444 │           2.119028 │          0.860394 │              5.408327 │      170.375403 │\n│ Adelie  │                1.644444 │              -0.044444 │                        3.0 │            30.555556 │           2.119028 │          0.860394 │              5.408327 │      170.375403 │\n│ …       │                       … │                      … │                          … │                    … │                  … │                 … │                     … │               … │\n└─────────┴─────────────────────────┴────────────────────────┴────────────────────────────┴──────────────────────┴────────────────────┴───────────────────┴───────────────────────┴─────────────────┘\n\n\n\nYou can also express complex filters more concisely.\nLet’s say we only want to keep rows where all the bill size z-score related columns’ absolute values are greater than 2.\n\nIn [78]: t.drop(\"year\").group_by(\"species\").mutate(\n    ...:     s.across(s.numeric(), dict(zscore=(_ - _.mean()) / _.std()))\n    ...: ).filter(s.if_all(s.startswith(\"bill\") & s.endswith(\"_zscore\"), _.abs() &gt; 2))\nOut[78]:\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ bill_length_mm_zscore ┃ bill_depth_mm_zscore ┃ flipper_length_mm_zscore ┃ body_mass_g_zscore ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ float64               │ float64              │ float64                  │ float64            │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────────────────────┼──────────────────────┼──────────────────────────┼────────────────────┤\n│ Gentoo  │ Biscoe    │           59.6 │          17.0 │               230 │        6050 │ male   │              3.924621 │             2.056508 │                 1.975799 │           1.932062 │\n│ Gentoo  │ Biscoe    │           55.9 │          17.0 │               228 │        5600 │ male   │              2.724046 │             2.056508 │                 1.667394 │           1.039411 │\n│ Adelie  │ Torgersen │           46.0 │          21.5 │               194 │        4200 │ male   │              2.706539 │             2.592071 │                 0.618760 │           1.088911 │\n│ Adelie  │ Dream     │           32.1 │          15.5 │               188 │        3050 │ female │             -2.512345 │            -2.339505 │                -0.298747 │          -1.418906 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────────────────────┴──────────────────────┴──────────────────────────┴────────────────────┘```\n\n\n\n\nThe SQL for that last expression is pretty gnarly:\nIn [79]: ibis.show_sql(\n    ...:     t.drop(\"year\")\n    ...:     .group_by(\"species\")\n    ...:     .mutate(s.across(s.numeric(), dict(zscore=(_ - _.mean()) / _.std())))\n    ...:     .filter(s.if_all(s.startswith(\"bill\") & s.endswith(\"_zscore\"), _.abs() &gt; 2))\n    ...: )\nWITH t0 AS (\n  SELECT\n    t2.species AS species,\n    t2.island AS island,\n    t2.bill_length_mm AS bill_length_mm,\n    t2.bill_depth_mm AS bill_depth_mm,\n    t2.flipper_length_mm AS flipper_length_mm,\n    t2.body_mass_g AS body_mass_g,\n    t2.sex AS sex\n  FROM ibis_read_csv_3 AS t2\n), t1 AS (\n  SELECT\n    t0.species AS species,\n    t0.island AS island,\n    t0.bill_length_mm AS bill_length_mm,\n    t0.bill_depth_mm AS bill_depth_mm,\n    t0.flipper_length_mm AS flipper_length_mm,\n    t0.body_mass_g AS body_mass_g,\n    t0.sex AS sex,\n    (\n      t0.bill_length_mm - AVG(t0.bill_length_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)\n    ) / STDDEV_SAMP(t0.bill_length_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS bill_length_mm_zscore,\n    (\n      t0.bill_depth_mm - AVG(t0.bill_depth_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)\n    ) / STDDEV_SAMP(t0.bill_depth_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS bill_depth_mm_zscore,\n    (\n      t0.flipper_length_mm - AVG(t0.flipper_length_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)\n    ) / STDDEV_SAMP(t0.flipper_length_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS flipper_length_mm_zscore,\n    (\n      t0.body_mass_g - AVG(t0.body_mass_g) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)\n    ) / STDDEV_SAMP(t0.body_mass_g) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS body_mass_g_zscore\n  FROM t0\n)\nSELECT\n  t1.species,\n  t1.island,\n  t1.bill_length_mm,\n  t1.bill_depth_mm,\n  t1.flipper_length_mm,\n  t1.body_mass_g,\n  t1.sex,\n  t1.bill_length_mm_zscore,\n  t1.bill_depth_mm_zscore,\n  t1.flipper_length_mm_zscore,\n  t1.body_mass_g_zscore\nFROM t1\nWHERE\n  ABS(t1.bill_length_mm_zscore) &gt; CAST(2 AS SMALLINT)\n  AND ABS(t1.bill_depth_mm_zscore) &gt; CAST(2 AS SMALLINT)\nGood thing you didn’t have to write that by hand!"
  },
  {
    "objectID": "blog/selectors.html#summary",
    "href": "blog/selectors.html#summary",
    "title": "Maximizing Productivity with Selectors",
    "section": "",
    "text": "This blog post illustrates the ability to apply computations to many columns at once and the power of ibis as a composable, expressive library for analytics.\n\nGet involved!\nReport issues!"
  },
  {
    "objectID": "concept/backends.html",
    "href": "concept/backends.html",
    "title": "Backends",
    "section": "",
    "text": "A backend is where execution of Ibis table expressions occur after compiling into some intermediate representation. A backend is often a database and the intermediate representation often SQL, but several types of backends exist. See the backends page for specific documentation on each.\n\n\nThe first category of backends translate Ibis table expressions into query strings.\nThe compiler turns each table expression into a query string and passes that query to the database through a driver API for execution.\n\nApache Impala\nClickHouse\nGoogle BigQuery\nHeavyAI\n\n\n\n\nThe next category of backends translates Ibis table expressions into another system’s table expression objects, for example, SQLAlchemy.\nInstead of generating a query string for each table expression, these backends produce another kind of table expression object and typically have high-level APIs for execution.\n\nApache Arrow Datafusion\nApache Druid\nApache PySpark\nDask\nDuckDB\nMS SQL Server\nMySQL\nOracle\nPolars\nPostgreSQL\nSQLite\nSnowflake\nTrino\n\n\n\n\nThe pandas backend is the only direct execution backend. A full description of the implementation can be found in the module docstring of the pandas backend located in ibis/backends/pandas/core.py.\n\npandas"
  },
  {
    "objectID": "concept/backends.html#string-generating-backends",
    "href": "concept/backends.html#string-generating-backends",
    "title": "Backends",
    "section": "",
    "text": "The first category of backends translate Ibis table expressions into query strings.\nThe compiler turns each table expression into a query string and passes that query to the database through a driver API for execution.\n\nApache Impala\nClickHouse\nGoogle BigQuery\nHeavyAI"
  },
  {
    "objectID": "concept/backends.html#expression-generating-backends",
    "href": "concept/backends.html#expression-generating-backends",
    "title": "Backends",
    "section": "",
    "text": "The next category of backends translates Ibis table expressions into another system’s table expression objects, for example, SQLAlchemy.\nInstead of generating a query string for each table expression, these backends produce another kind of table expression object and typically have high-level APIs for execution.\n\nApache Arrow Datafusion\nApache Druid\nApache PySpark\nDask\nDuckDB\nMS SQL Server\nMySQL\nOracle\nPolars\nPostgreSQL\nSQLite\nSnowflake\nTrino"
  },
  {
    "objectID": "concept/backends.html#direct-execution-backends",
    "href": "concept/backends.html#direct-execution-backends",
    "title": "Backends",
    "section": "",
    "text": "The pandas backend is the only direct execution backend. A full description of the implementation can be found in the module docstring of the pandas backend located in ibis/backends/pandas/core.py.\n\npandas"
  },
  {
    "objectID": "concept/why_ibis.html",
    "href": "concept/why_ibis.html",
    "title": "Why Ibis?",
    "section": "",
    "text": "Ibis is a dataframe interface to execution engines with support for 15+ backends. Ibis doesn’t replace your existing execution engine, it extends it with powerful abstractions and intuitive syntax.\nIbis works with what you already have, so why not check out our getting started guide?"
  },
  {
    "objectID": "concept/why_ibis.html#big-data-engines-like-bigquery-snowflake-spark",
    "href": "concept/why_ibis.html#big-data-engines-like-bigquery-snowflake-spark",
    "title": "Why Ibis?",
    "section": "Big Data engines like BigQuery, Snowflake, Spark, …",
    "text": "Big Data engines like BigQuery, Snowflake, Spark, …\nSee above. Ibis works with your existing execution engine, it doesn’t replace it."
  },
  {
    "objectID": "concept/why_ibis.html#sql",
    "href": "concept/why_ibis.html#sql",
    "title": "Why Ibis?",
    "section": "SQL",
    "text": "SQL\nSQL is the 800 lb gorilla in the room. One of our developers gave a whole talk comparing Ibis and SQL, but we can summarize some key points:\n\nSQL fails at runtime, Ibis validates expressions as you construct them\nIbis is written in Python and features some pretty killer tab-completion\nIbis lets you use SQL when you want to (for our SQL-based backends)\n\nIf your SQL-fu is strong, we might not convince you to leave it all behind, but check out our Ibis for SQL users guide and see if it whets your appetite."
  },
  {
    "objectID": "concept/why_ibis.html#pandas",
    "href": "concept/why_ibis.html#pandas",
    "title": "Why Ibis?",
    "section": "pandas",
    "text": "pandas\npandas is the 800 lb panda in the room. Ibis, like every dataframe API in the PyData ecosystem, takes a fair bit of inspiration from pandas.\nAnd like the other engine comparisons above, Ibis doesn’t replace pandas, it works with pandas.\npandas is an in-memory analysis engine – if your data are bigger than the amount of RAM you have, things will go poorly.\nIbis defers execution, and is agnostic to the backend that runs a given query. If your analysis is causing pandas to hit an out-of-memory error, you can use Ibis to quickly and easily switch to a different backend that supports out-of-core execution.\nIbis syntax is similar to pandas syntax, but it isn’t a drop-in replacement. Check out our Ibis for pandas Users guide if you’d like to give Ibis a try!"
  },
  {
    "objectID": "concept/why_ibis.html#sqlalchemy-and-sqlglot",
    "href": "concept/why_ibis.html#sqlalchemy-and-sqlglot",
    "title": "Why Ibis?",
    "section": "sqlalchemy and sqlglot",
    "text": "sqlalchemy and sqlglot\nsqlalchemy and sqlglot are amazing tools and we are big fans. Ibis uses both of these heavily to validate and generate SQL to send to our SQL backends.\nIf you need super-fine-grained control over which SQL primitives are used to construct a query and you are using Python, SQLAlchemy is definitely the tool for you.\nIf you are looking for a Python-based SQL transpiler, we strongly recommend using SQLGlot.\nIf you are looking for a dataframe API to construct and execute your analytics queries against a large collection of powerful execution engines, then allow us point you at the Ibis Getting Started guide."
  },
  {
    "objectID": "concept/design.html",
    "href": "concept/design.html",
    "title": "Design",
    "section": "",
    "text": "Type safety\nExpressiveness\nComposability\nFamiliarity\n\n\n\n\n\nUser writes expression\nEach method or function call builds a new expression\nExpressions are type checked as you create them\nExpressions have some optimizations that happen as the user builds them\nBackend specific rewrites\nExpressions are compiled\nThe SQL string that generated by the compiler is sent to the database and executed (this step is skipped for the pandas backend)\nThe database returns some data that is then turned into a pandas DataFrame by Ibis\n\n\n\n\nThe main user-facing component of Ibis is expressions. The base class of all expressions in Ibis is the [ibis.expr.types.Expr][] class.\nExpressions provide the user facing API, most of which is defined in ibis/expr/api.py.\n\n\nIbis’s type system consists of a set of rules for specifying the types of inputs to ibis.expr.types.Node subclasses. Upon construction of a Node subclass, Ibis performs validation of every input to the node based on the rule that was used to declare the input.\nRules are defined in ibis.expr.rules\n\n\n\n\n\nExpressions are a thin but important abstraction over operations, containing only type information and shape information, i.e., whether they are tables, columns, or scalars.\n\nExamples of expression types include [StringValue][ibis.expr.types.StringValue] and [Table][ibis.expr.types.Table]. \n\n\n\n\n\nNode subclasses make up the core set of operations of Ibis. Each node corresponds to a particular operation.\nMost nodes are defined in the ibis.expr.operations module.\nExamples of nodes include ibis.expr.operations.Add and ibis.expr.operations.Sum.\nNodes (transitively) inherit from a class that allows node authors to define their node’s input arguments directly in the class body.\nAdditionally the output_type member of the class is a rule or method that defines the shape (scalar or column) and element type of the operation.\nAn example of usage is a node that representats a logarithm operation:\n\nimport ibis.expr.rules as rlz\nfrom ibis.expr.operations import Value\n\nclass Log(Value):\n   # A double scalar or column\n   arg = rlz.double\n   # Optional argument, defaults to None\n   base = rlz.optional(rlz.double)\n   # Output expression's datatype will correspond to arg's datatype\n   output_dtype = rlz.dtype_like('arg')\n   # Output expression will be scalar if arg is scalar, column otherwise\n   output_shape = rlz.shape_like('arg')\nThis class describes an operation called Log that takes one required argument: a double scalar or column, and one optional argument: a double scalar or column named base that defaults to nothing if not provided. The base argument is None by default so that the expression will behave as the underlying database does.\nSimilar objects are instantiated when you use Ibis APIs:\nimport ibis\nt = ibis.table([('a', 'float')], name='t')\nlog_1p = (1 + t.a).log()  # an Add and a Log are instantiated here\n\n\n\nSeparating expressions from their underlying operations makes it easy to generically describe and validate the inputs to particular nodes. In the log example, it doesn’t matter what operation (node) the double-valued arguments are coming from, they must only satisfy the requirement denoted by the rule.\nSeparation of the ibis.expr.types.Node and ibis.expr.types.Expr classes also allows the API to be tied to the physical type of the expression rather than the particular operation, making it easy to define the API in terms of types rather than specific operations.\nFurthermore, operations often have an output type that depends on the input type. An example of this is the greatest function, which takes the maximum of all of its arguments. Another example is CASE statements, whose THEN expressions determine the output type of the expression.\nThis allows Ibis to provide only the APIs that make sense for a particular type, even when an operation yields a different output type depending on its input. Concretely, this means that you cannot perform operations that don’t make sense, like computing the average of a string column.\n\n\n\n\nThe next major component of Ibis is the compilers.\nThe first few versions of Ibis directly generated strings, but the compiler infrastructure was generalized to support compilation of SQLAlchemy based expressions.\nThe compiler works by translating the different pieces of SQL expression into a string or SQLAlchemy expression.\nThe main pieces of a SELECT statement are:\n!. The set of column expressions (select_set) !. WHERE clauses (where) !. GROUP BY clauses (group_by) !. HAVING clauses (having) !. LIMIT clauses (limit) !. ORDER BY clauses (order_by) !. DISTINCT clauses (distinct)\nEach of these pieces is translated into a SQL string and finally assembled by the instance of the ibis.sql.compiler.ExprTranslator subclass specific to the backend being compiled. For example, the ibis.impala.compiler.ImpalaExprTranslator is one of the subclasses that will perform this translation.\n!!! note “Ibis can target other systems besides SQL”\nWhile Ibis was designed with an explicit goal of first-class SQL support,\nIbis can target other systems such as pandas.\n\n\n\nPresumably we want to do something with our compiled expressions. This is where execution comes in.\nThis is least complex part of Ibis, mostly only requiring Ibis to correctly handle whatever the database hands back.\nBy and large, the execution of compiled SQL is handled by the database to which SQL is sent from Ibis.\nHowever, once the data arrives from the database we need to convert that data to a pandas DataFrame.\nThe Query class, with its ibis.sql.client.Query._fetch method, provides a way for Ibis ibis.sql.client.SQLClient objects to do any additional processing necessary after the database returns results to the client."
  },
  {
    "objectID": "concept/design.html#primary-goals",
    "href": "concept/design.html#primary-goals",
    "title": "Design",
    "section": "",
    "text": "Type safety\nExpressiveness\nComposability\nFamiliarity"
  },
  {
    "objectID": "concept/design.html#flow-of-execution",
    "href": "concept/design.html#flow-of-execution",
    "title": "Design",
    "section": "",
    "text": "User writes expression\nEach method or function call builds a new expression\nExpressions are type checked as you create them\nExpressions have some optimizations that happen as the user builds them\nBackend specific rewrites\nExpressions are compiled\nThe SQL string that generated by the compiler is sent to the database and executed (this step is skipped for the pandas backend)\nThe database returns some data that is then turned into a pandas DataFrame by Ibis"
  },
  {
    "objectID": "concept/design.html#expressions",
    "href": "concept/design.html#expressions",
    "title": "Design",
    "section": "",
    "text": "The main user-facing component of Ibis is expressions. The base class of all expressions in Ibis is the [ibis.expr.types.Expr][] class.\nExpressions provide the user facing API, most of which is defined in ibis/expr/api.py.\n\n\nIbis’s type system consists of a set of rules for specifying the types of inputs to ibis.expr.types.Node subclasses. Upon construction of a Node subclass, Ibis performs validation of every input to the node based on the rule that was used to declare the input.\nRules are defined in ibis.expr.rules\n\n\n\n\n\nExpressions are a thin but important abstraction over operations, containing only type information and shape information, i.e., whether they are tables, columns, or scalars.\n\nExamples of expression types include [StringValue][ibis.expr.types.StringValue] and [Table][ibis.expr.types.Table]. \n\n\n\n\n\nNode subclasses make up the core set of operations of Ibis. Each node corresponds to a particular operation.\nMost nodes are defined in the ibis.expr.operations module.\nExamples of nodes include ibis.expr.operations.Add and ibis.expr.operations.Sum.\nNodes (transitively) inherit from a class that allows node authors to define their node’s input arguments directly in the class body.\nAdditionally the output_type member of the class is a rule or method that defines the shape (scalar or column) and element type of the operation.\nAn example of usage is a node that representats a logarithm operation:\n\nimport ibis.expr.rules as rlz\nfrom ibis.expr.operations import Value\n\nclass Log(Value):\n   # A double scalar or column\n   arg = rlz.double\n   # Optional argument, defaults to None\n   base = rlz.optional(rlz.double)\n   # Output expression's datatype will correspond to arg's datatype\n   output_dtype = rlz.dtype_like('arg')\n   # Output expression will be scalar if arg is scalar, column otherwise\n   output_shape = rlz.shape_like('arg')\nThis class describes an operation called Log that takes one required argument: a double scalar or column, and one optional argument: a double scalar or column named base that defaults to nothing if not provided. The base argument is None by default so that the expression will behave as the underlying database does.\nSimilar objects are instantiated when you use Ibis APIs:\nimport ibis\nt = ibis.table([('a', 'float')], name='t')\nlog_1p = (1 + t.a).log()  # an Add and a Log are instantiated here\n\n\n\nSeparating expressions from their underlying operations makes it easy to generically describe and validate the inputs to particular nodes. In the log example, it doesn’t matter what operation (node) the double-valued arguments are coming from, they must only satisfy the requirement denoted by the rule.\nSeparation of the ibis.expr.types.Node and ibis.expr.types.Expr classes also allows the API to be tied to the physical type of the expression rather than the particular operation, making it easy to define the API in terms of types rather than specific operations.\nFurthermore, operations often have an output type that depends on the input type. An example of this is the greatest function, which takes the maximum of all of its arguments. Another example is CASE statements, whose THEN expressions determine the output type of the expression.\nThis allows Ibis to provide only the APIs that make sense for a particular type, even when an operation yields a different output type depending on its input. Concretely, this means that you cannot perform operations that don’t make sense, like computing the average of a string column."
  },
  {
    "objectID": "concept/design.html#compilation",
    "href": "concept/design.html#compilation",
    "title": "Design",
    "section": "",
    "text": "The next major component of Ibis is the compilers.\nThe first few versions of Ibis directly generated strings, but the compiler infrastructure was generalized to support compilation of SQLAlchemy based expressions.\nThe compiler works by translating the different pieces of SQL expression into a string or SQLAlchemy expression.\nThe main pieces of a SELECT statement are:\n!. The set of column expressions (select_set) !. WHERE clauses (where) !. GROUP BY clauses (group_by) !. HAVING clauses (having) !. LIMIT clauses (limit) !. ORDER BY clauses (order_by) !. DISTINCT clauses (distinct)\nEach of these pieces is translated into a SQL string and finally assembled by the instance of the ibis.sql.compiler.ExprTranslator subclass specific to the backend being compiled. For example, the ibis.impala.compiler.ImpalaExprTranslator is one of the subclasses that will perform this translation.\n!!! note “Ibis can target other systems besides SQL”\nWhile Ibis was designed with an explicit goal of first-class SQL support,\nIbis can target other systems such as pandas."
  },
  {
    "objectID": "concept/design.html#execution",
    "href": "concept/design.html#execution",
    "title": "Design",
    "section": "",
    "text": "Presumably we want to do something with our compiled expressions. This is where execution comes in.\nThis is least complex part of Ibis, mostly only requiring Ibis to correctly handle whatever the database hands back.\nBy and large, the execution of compiled SQL is handled by the database to which SQL is sent from Ibis.\nHowever, once the data arrives from the database we need to convert that data to a pandas DataFrame.\nThe Query class, with its ibis.sql.client.Query._fetch method, provides a way for Ibis ibis.sql.client.SQLClient objects to do any additional processing necessary after the database returns results to the client."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]